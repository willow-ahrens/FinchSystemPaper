\section{The Finch Compiler}

\subsection{Finch Normal Form}

Our semantics in Figure
\ref{fig:semantics_core,fig:semantics_lifecycle,fig:semantics_looplets} is only
well-defined on some programs. We define a particular class of programs on which
our semantics are well-defined, and refer to it as \textbf{Finch Normal Form}.
The properties of such a program are as follows:
\begin{enumerate}
    \item[Access with Indices] Though Finch allows general expressions in an
    access (i.e. `A[i + j]` or `A[I[i]]`), the normal form restricts to allow only indices in 
    accesses (i.e. `A[i]`), rather than more general expressions.
    \item[Evaluable Dimensions] Loop dimensions and declaration dimensions must
    be evaluable at the time we compile them, so we restrict the normal form to
    programs whose loop dimensions and declaration dimensions are extents with
    limits defined in the scope of the corresponding loop or declaration
    statement.
    \item[Concordant] Finch is column-major by default, and the normal form
    requires the order of indices in an access to match the order in which loops
    are nested around it.  For example,
    \mintinline{julia}{for j = _; for i = _; s[] += A[i, j] end end}
    is concordant but
    \mintinline{julia}{for i = _; for j = _; s[] += A[i, j] end end} is not.
    \item[Lifecycle Constraints] Tensors in read mode may appear on the right
    hand side only. Tensors in update mode may appear on the left hand side
    only. To make it easier to statically analyze lifecycle constraints, we
    restrict tensors to only change modes in the same scope in which they were
    defined.
\end{enumerate}

The subsequent sections will explain how programs that violate each of these
constraints can be rewritten to programs that satisfy them, and thus how we can
support such a wide variety of programs. For example, we can write nonconcordant
programs like  \mintinline{julia}{for i = _; for j = _; s[] += A[i, j] end end}
by adding a loop to randomly access \mintinline{julia}{A} or adjusting the
storage order of \mintinline{julia}{A} by adding a lazy transposition wrapper.

\subsection{Wrapperization}

    Many fancy operations on indices can be resolved by introducing equivalent
    \textbf{wrapper arrays} which modify the behavior of the tensors they wrap,
    or by introducing \textbf{mask arrays} which replace index expressions like
    \mintinline{julia}{i == j} with their equivalent masks (in this case, a
    diagonal mask tensor).

    All wrapper arrays are eventually unwrapped by the compiler as we lower
    them, some earlier than others. For example, the $swizzle$ array wraps a
    tensor and permutes the indices of an access when it is unwrapped during the
    wrapperization pass. On the other hand, the $offset$ array wraps a tensor
    and shifts all of the ranges declared in Figure \ref{fig:semantics_looplets} by one.
    The implementation burden for a wrapper tensor is to implement a suitable
    program modification during the wrapperization procedure and then unwrap the wrapper, or to 
    implement the looplet functions of \ref{fig:semantics_looplets} with some minor modifications
    to shift dimensions, for example. Wrapper arrays are summarized in Table \ref{tab:wrappers}

    Mask arrays have a more straightforward implementation using static looplets
    that are constructed during the unfurl step. They are summarized in table \ref{tab:masks}. Mask tensors
    allow us to lift computations with masks to the level of the loop, without modifying the loop directly.

    For example, \begin{minted}{julia}
    for i=_, j=_
        if i <= j
            s[] += A[i, j]
        end
    end
    \end{minted}

    would likely compile to something like

    \begin{minted}{julia}
    for i = 1:n
        for j = 1:i
            s[] += A[i, j]
        end
    end
    \end{minted}

    \begin{table}[h]
        \scriptsize
        \centering
        \begin{tabular}{|>{\raggedright\arraybackslash}m{0.25\linewidth}|>{\raggedright\arraybackslash}m{0.75\linewidth}|}
        \hline
        \textbf{Transformation Example} & \textbf{Description} \\
        \hline
        A[i + a] $\rightarrow$ offset(A, 1)[i] & Creates an OffsetArray such that \texttt{offset(tns, delta...)[i...] == tns[i .+ delta...]}. \\
        \hline
        A[i + x] $\rightarrow$ toeplitz(A, 1)[i, x] & Creates a ToeplitzArray, adding a dimension that shifts another dimension of the original tensor. The added dimensions are produced during a call to Unfurl, when a lookup looplet is emitted for the first dimension. \\
        \hline
        A[(a:b)(i)] $\rightarrow$ window(A, a:b)[i] & Creates a WindowedArray, representing a view into another tensor. This wrapper returns a different size of tensor. \\
        \hline
        A[\~i] $\rightarrow$ permissive(A)[i] & Creates a PermissiveArray, allowing for out-of-bounds access or padding. This tensor returns no dimensions as its size. \\
        \hline
        A[p(i)] $\rightarrow$ protocolize(A, p)[i] & Accesses dimension n with protocol protos[n], allowing for advanced iteration protocols. \\
        \hline
        A[p(i)] $\rightarrow$ swizzle(A, perm)[i] & A lazily transposed array, swizzle(A, perm)[idx...] is transformed to A[idx[perm]...] during wrapperization. \\
        \hline
        \end{tabular}
        \caption{Wrapper arrays and some example indexing sugar they enable.}
    \label{table:wrappers}
    \end{table}

    \begin{table}[h]
        \scriptsize
        \centering
        \begin{tabular}{|>{\raggedright\arraybackslash}m{0.3\linewidth}|>{\raggedright\arraybackslash}m{0.4\linewidth}|}
        \hline
        \textbf{Transformation Example} & \textbf{Description} \\
        \hline
        $i < j \rightarrow$ UpTriMask()[i, j - 1] & Upper triangular mask, true if $i < j$. \\
        \hline
        $i \geq j \rightarrow$ LoTriMask()[i, j] & Lower triangular mask, true if $i \geq j$. \\
        \hline
        $l \leq i < j \rightarrow$ Bandmask()[i, l, h - 1] & Banded mask, true for elements within a specified band. \\
        \hline
        $i == j \rightarrow$ DiagMask()[i, j] & Diagonal mask, true if $i == j$. \\
        \hline
        $i \neq j \rightarrow$ !(DiagMask()[i, j]) & Inverse diagonal mask, true if $i \neq j$. \\
        \hline
        chunkmask(b) & Chunk mask, for chunked tensor access. True if $b \times (j - 1) < i \leq b \times j$. \\
        \hline
        \end{tabular}
        \caption{Mask tensors and some example cases they are inserted.}
        \label{table:masks}
    \end{table}
    
\subsection{Dimensionalization}
    Looplets typically require the dimension of the loop extent to match the dimensions of the tensor. 
    %
    However, it is cumbersome to write the dimensions
    in loop programs, and most tensor compilers have a means of specifying the dimensions automatically.
    %
    In many pure Einsum languages, determining dimensions
    is not needed because any tensor dimensions that share an index are assumed to be the same~\cite{kjolstad_tensor_2017}.
    %
    Other languages such as Halide perform bounds inference wherein
    known bounds are symbolically propagated to fill in unknown bounds, often from the desired output/input sizes to intermediates via some conservative approximation such as interval analysis or polyhedral methods~\cite{ragan-kelley_halide_2013, grosser2012polly}.
    %
    We refer to the process of discovering suitable
    dimensions as \textbf{dimensionalization}. 
    %

    
    %\willow{Teo, please explain
    %alternative approaches}>

    In Finch, we use a straightforward algorithm to automatically determine suitable dimensions for loops and declaration statements (output tensors).
    %
    Finch determines the dimension of a loop index i from all of the tensors using i in an access, as well as the bounds in the loop itself, and operates similarly for declarations.
    %   
    Our algorithm uses the following principles, assuming dimension types form a lattice:
    \begin{enumerate}
        \item We assign dimensions to indices.
        \item Using an index in an access “hints” that the index should have the corresponding dimension.
        \item Loop dimensions are equal to the “meet” of all hints in the loop body
        and any existing dimensions in the loop bounds. The meet usually asserts
        that dimensions match, but may also e.g. propagate info about parallelization
        \item The \mintinline{julia}{_} symbol represents a dimensionless quantity at the bottom of the dimension lattice.
        \item We assign dimensions to declarations.
        \item Left hand side (updating) tensor access “hint” the size of that tensor
        \item The dimensions of a declaration are the “meet” of all hints from
        the declaration to the first read.
        \item The new dimensions of the declared tensor are used when the tensor is on the right hand side (reading)
        access.
    \end{enumerate}

    For example, consider the following code
    
    \begin{minted}{julia}
    A = fsprand(3, 4, 0.5)
    B = fsprand(4, 5, 0.5)
    C = Tensor(Dense(SparseList(Element(0.0))))
    @finch begin
    C .= 0
    for i = 1:3
        for j = _
            for k = _
                C[i, j] += A[i, k] * B[k, j]
            end
        end
    end
    \end{minted}
    
    In the above code, the second dimension of A must match the first dimension of B. Also, the first dimension of A must match the i loop dimension, 1:3. Finch will also resize declared tensors to match indices used in writes, so C is resized to (1:3, 1:5). If no dimensions are specified elsewhere, then Finch will use the dimension of the declared tensor.
    
    Dimensionalization occurs after wrapper arrays are de-sugared. You can
    therefore exempt a tensor from dimensionalization by wrapping the
    corresponding index in ~ to produce a "PermissiveArray". For example,
    
    \begin{minted}{julia}
    @finch begin
    y .= 0
    for i = 1:3
        y[~i] += x[i]
    end
    \end{minted}
    
    does not set the dimension of y, and y does not participate in dimensionalization.
    
\subsection{Concordization}
    After wrapperization, all arrays are normalized to column-major ordering and
    the unrecognized expressions are left in the access expressions. At that
    point, we run a pass over the code to make the program concordant.
    
    We make expressions concordant by inserting loops with one iteration. For example, the following expression can be made concordant by adding a loop for the out of order access to \texttt{i}.
    \begin{minted}{julia}
        @finch begin
        for i = _
            for j = _
                for k = _
                    s[] += A[i, k, j]
                end
            end
        end
    \end{minted}

    \begin{minted}{julia}
        @finch begin
        for i = _
            for j = _
                for k = _
                    for l = _
                        if i == l #diagonal mask
                            s[] += A[l, k, j]
                        end
                    end
                end
            end
        end
    \end{minted}

    We can concordize unrecognized index expressions using a similar approach:

    \begin{minted}{julia}
        for i = _
            A[I[i]] += 1
        end
    \end{minted}

    \begin{minted}{julia}
        for i = _
            for j = _
                if j == I[i]
                    A[j] += 1
                end
            end
        end
    \end{minted}

\subsection{Life Cycles and Scopes}
\teo{Can we maybe skip this because we cover it in the semantics?}
\subsection{Simplification and Algebraic Transformations}

\subsection{Bounds Analysis}
\subsection{Performance Warnings}
\teo{I also think this maybe belongs in an appendix.}
