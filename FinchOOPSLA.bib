
@inproceedings{backus_fortran_1957,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '57 ({Western})},
	title = {The {FORTRAN} automatic coding system},
	isbn = {978-1-4503-7861-1},
	url = {https://doi.org/10.1145/1455567.1455599},
	doi = {10.1145/1455567.1455599},
	abstract = {The FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM's next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan. The goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.},
	urldate = {2022-08-04},
	booktitle = {Papers presented at the {February} 26-28, 1957, western joint computer conference: {Techniques} for reliability},
	publisher = {Association for Computing Machinery},
	author = {Backus, J. W. and Beeber, R. J. and Best, S. and Goldberg, R. and Haibt, L. M. and Herrick, H. L. and Nelson, R. A. and Sayre, D. and Sheridan, P. B. and Stern, H. and Ziller, I. and Hughes, R. A. and Nutt, R.},
	month = feb,
	year = {1957},
	pages = {188--198},
	file = {Full Text PDF:/Users/willow/Zotero/storage/SUHHBWFU/Backus et al. - 1957 - The FORTRAN automatic coding system.pdf:application/pdf},
}

@article{psarras_landscape_2021,
	title = {The landscape of software for tensor computations},
	url = {http://arxiv.org/abs/2103.13756},
	abstract = {Tensors (also commonly seen as multi-linear operators or as multi-dimensional arrays) are ubiquitous in scientiﬁc computing and in data science, and so are the software eﬀorts for tensor operations. Particularly in recent years, we have observed an explosion in libraries, compilers, packages, and toolboxes; unfortunately these eﬀorts are very much scattered among the diﬀerent scientiﬁc domains, and inevitably suﬀer from replication, suboptimal implementations, and in many cases, limited visibility. As a ﬁrst step towards countering these ineﬃciencies, here we survey and loosely classify software packages related to tensor computations. Our aim is to assemble a comprehensive and up-to-date snapshot of the tensor software landscape, with the intention of helping both users and developers. Aware of the diﬃculties inherent in any multi-discipline survey, we very much welcome the reader’s help in amending and expanding our software list, which currently features 77 projects.},
	language = {en},
	urldate = {2022-04-29},
	journal = {arXiv:2103.13756 [cs]},
	author = {Psarras, Christos and Karlsson, Lars and Li, Jiajia and Bientinesi, Paolo},
	month = may,
	year = {2021},
	note = {arXiv: 2103.13756},
	keywords = {Computer Science - Mathematical Software},
	file = {Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:/Users/willow/Zotero/storage/9FYH3IWU/Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:application/pdf},
}

@article{hu_taichi_2019,
	title = {Taichi: a language for high-performance computation on spatially sparse data structures},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {Taichi},
	url = {https://doi.org/10.1145/3355089.3356506},
	doi = {10.1145/3355089.3356506},
	abstract = {3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multi-level sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose Taichi, a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This decoupling of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With 1{\textless}u{\textgreater}1{\textless}/u{\textgreater}0 th as many lines of code, we achieve 4.55× higher performance on average, compared to hand-optimized reference implementations.},
	number = {6},
	urldate = {2020-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Hu, Yuanming and Li, Tzu-Mao and Anderson, Luke and Ragan-Kelley, Jonathan and Durand, Frédo},
	month = nov,
	year = {2019},
	keywords = {GPU computing, sparse data structures},
	pages = {201:1--201:16},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LI4HK8PK/Hu et al. - 2019 - Taichi a language for high-performance computatio.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Number: 7825
Publisher: Nature Publishing Group},
	pages = {357--362},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZFNZYPK/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/IFYNZTWL/s41586-020-2649-2.html:text/html},
}

@article{langr_evaluation_2016,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	volume = {27},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2015.2401575},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langr, Daniel and Tvrdík, Pavel},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {sparse matrix-vector multiplication, Sparse matrices, Runtime, parallel processing, Indexes, matrix-vector multiplication, sparse matrix, Matrix converters, mathematics computing, test matrices, Standards, vectors, storage format, matrix algebra, Memory management, evaluation criteria, Evaluation criterion, FLOPS, HPC application, memory footprint, nonzero matrix structure, single evaluation criterion, sparse matrix storage formats, SpMV-based application phase},
	pages = {428--440},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRZMJ4TJ/7036061.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/D75SXF9Z/7036061.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/8IRGEGDG/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/I3FNKNCF/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	url = {https://doi.org/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
	number = {OOPSLA},
	urldate = {2021-11-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	keywords = {Compilation, Sparse Array Programming, Sparse Arrays},
	pages = {128:1--128:29},
	file = {Full Text PDF:/Users/willow/Zotero/storage/724NU8UX/Henry et al. - 2021 - Compilation of sparse array programming models.pdf:application/pdf},
}

@article{kjolstad_tensor_2017,
	title = {The {Tensor} {Algebra} {Compiler}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2019-11-20},
	journal = {Proc. ACM Program. Lang.},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {parallelism, code generation, linear algebra, tensors, performance, sparse data structures, iteration graphs, merge lattices, tensor algebra},
	pages = {77:1--77:29},
	file = {ACM Full Text PDF:/Users/willow/Zotero/storage/M3YTNCUI/Kjolstad et al. - 2017 - The Tensor Algebra Compiler.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/EZXEWXYJ/Kjolstad et al. - 2017 - The tensor algebra compiler.pdf:application/pdf;kjolstad-oopsla17-taco-preprint.pdf:/Users/willow/Zotero/storage/XUPL86I6/kjolstad-oopsla17-taco-preprint.pdf:application/pdf},
}

@inproceedings{ragan-kelley_halide_2013,
	address = {New York, NY, USA},
	series = {{PLDI} '13},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Halide},
	url = {http://doi.org/10.1145/2491956.2462176},
	doi = {10.1145/2491956.2462176},
	abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Frédo and Amarasinghe, Saman},
	month = jun,
	year = {2013},
	keywords = {autotuning, parallelism, gpu, optimization, vectorization, locality, compiler, image processing, domain specific language, redundant computation},
	pages = {519--530},
	file = {Full Text PDF:/Users/willow/Zotero/storage/QZF9XYFW/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	volume = {17},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	language = {en},
	number = {3},
	urldate = {2021-11-19},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = mar,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biophysical chemistry;Computational biology and bioinformatics;Technology
Subject\_term\_id: biophysical-chemistry;computational-biology-and-bioinformatics;technology},
	keywords = {Biophysical chemistry, Computational biology and bioinformatics, Technology},
	pages = {261--272},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2Q9W83VV/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/V7KB63J9/s41592-019-0686-2.html:text/html},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	urldate = {2021-11-19},
	booktitle = {12th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 16)},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Q4MBXVX8/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@inproceedings{williams_optimization_2007,
	address = {New York, NY, USA},
	series = {{SC} '07},
	title = {Optimization of sparse matrix-vector multiplication on emerging multicore platforms},
	isbn = {978-1-59593-764-3},
	url = {http://doi.org/10.1145/1362622.1362674},
	doi = {10.1145/1362622.1362674},
	abstract = {We are witnessing a dramatic change in computer architecture due to the multicore paradigm shift, as every electronic device from cell phones to supercomputers confronts parallelism of unprecedented scale. To fully unleash the potential of these systems, the HPC community must develop multicore specific optimization methodologies for important scientific computations. In this work, we examine sparse matrix-vector multiply (SpMV) - one of the most heavily used kernels in scientific computing - across a broad spectrum of multicore designs. Our experimental platform includes the homogeneous AMD dual-core and Intel quad-core designs, the heterogeneous STI Cell, as well as the first scientific study of the highly multithreaded Sun Niagara2. We present several optimization strategies especially effective for the multicore environment, and demonstrate significant performance improvements compared to existing state-of-the-art serial and parallel SpMV implementations. Additionally, we present key insights into the architectural tradeoffs of leading multicore design strategies, in the context of demanding memory-bound numerical algorithms.},
	urldate = {2022-03-15},
	booktitle = {Proceedings of the 2007 {ACM}/{IEEE} conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Williams, Samuel and Oliker, Leonid and Vuduc, Richard and Shalf, John and Yelick, Katherine and Demmel, James},
	month = nov,
	year = {2007},
	keywords = {Sparse matrices, Supercomputers, Kernel, Multicore processing, Parallel processing, Computer architecture, Scientific computing, Cellular phones, Optimization methods, Sun},
	pages = {1--12},
	file = {Full Text PDF:/Users/willow/Zotero/storage/G4VQIMPC/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/P7C29HUU/5348797.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/XVTJHNHK/5348797.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/MLDMCFE7/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf},
}

@inproceedings{ahrens_looplets_2023,
	address = {New York, NY, USA},
	series = {{CGO} 2023},
	title = {Looplets: {A} {Language} for {Structured} {Coiteration}},
	isbn = {9798400701016},
	shorttitle = {Looplets},
	doi = {10.1145/3579990.3580020},
	abstract = {Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Specializing for structure yields significant speedups. But automatically generating efficient code for structured data is challenging, especially when arrays with different structure interact. We show how to abstract over array structures so that the compiler can generate code to coiterate over any combination of them. Our technique enables new array formats (such as 1DVBL for irregular clustered sparsity), new iteration strategies (such as galloping intersections), and new operations over structured data (such as concatenation or convolution).},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Donenfeld, Daniel and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = feb,
	year = {2023},
	keywords = {Sparse, Tensor, Array, Coiteration, Compressed},
	pages = {41--54},
	file = {Full Text PDF:/Users/willow/Zotero/storage/HUBGW7N9/Ahrens et al. - 2023 - Looplets A Language for Structured Coiteration.pdf:application/pdf},
}

@article{psarras_linear_2022,
	title = {The {Linear} {Algebra} {Mapping} {Problem}. {Current} state of linear algebra languages and libraries},
	volume = {48},
	issn = {0098-3500, 1557-7295},
	url = {http://arxiv.org/abs/1911.09421},
	doi = {10.1145/3549935},
	abstract = {We observe a disconnect between the developers and the end users of linear algebra libraries. On the one hand, the numerical linear algebra and the high-performance communities invest significant effort in the development and optimization of highly sophisticated numerical kernels and libraries, aiming at the maximum exploitation of both the properties of the input matrices, and the architectural features of the target computing platform. On the other hand, end users are progressively less likely to go through the error-prone and time consuming process of directly using said libraries by writing their code in C or Fortran; instead, languages and libraries such as Matlab, Julia, Eigen and Armadillo, which offer a higher level of abstraction, are becoming more and more popular. Users are given the opportunity to code matrix computations with a syntax that closely resembles the mathematical description; it is then a compiler or an interpreter that internally maps the input program to lower level kernels, as provided by libraries such as BLAS and LAPACK. Unfortunately, our experience suggests that in terms of performance, this translation is typically vastly suboptimal. In this paper, we first introduce the Linear Algebra Mapping Problem, and then investigate how effectively a benchmark of test problems is solved by popular high-level programming languages. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both standard compiler optimizations such as common subexpression elimination and loop-invariant code motion, as well as linear algebra specific optimizations such as optimal parenthesization of a matrix product and kernel selection for matrices with properties. The aim of this study is to give concrete guidelines for the development of languages and libraries that support linear algebra computations.},
	number = {3},
	urldate = {2023-10-03},
	journal = {ACM Transactions on Mathematical Software},
	author = {Psarras, Christos and Barthels, Henrik and Bientinesi, Paolo},
	month = sep,
	year = {2022},
	note = {arXiv:1911.09421 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Mathematical Software},
	pages = {1--30},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/6APM5QY7/Psarras et al. - 2022 - The Linear Algebra Mapping Problem. Current state .pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/6JF2TJK5/1911.html:text/html},
}

@inproceedings{lo_roofline_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	isbn = {978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	doi = {10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	keywords = {CUDA unified memory, Memory bandwidth, Roofline},
	pages = {129--148},
	file = {Submitted Version:/Users/willow/Zotero/storage/X3YWPR6T/Lo et al. - 2015 - Roofline Model Toolkit A Practical Tool for Archi.pdf:application/pdf},
}

@inproceedings{fegade_cora_2022,
	title = {The {CoRa} {Tensor} {Compiler}: {Compilation} for {Ragged} {Tensors} with {Minimal} {Padding}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems}},
	author = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip and Mowry, Todd},
	editor = {Marculescu, D. and Chi, Y. and Wu, C.},
	year = {2022},
	pages = {721--747},
	file = {Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/8PT7K7GL/Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018,
	address = {Eugene OR USA},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer’s push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebrabased framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-ofthe-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {ACM},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	pages = {1--11},
	file = {Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:/Users/willow/Zotero/storage/ADD2JSKL/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018-1,
	address = {New York, NY, USA},
	series = {{ICPP} '18},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer's push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebra-based framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-of-the-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	keywords = {graph algorithms, sparse matrix multiplication, breadth-first search},
	pages = {1--11},
	file = {Full Text PDF:/Users/willow/Zotero/storage/NGHWT98P/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6GTQB2I8/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{moler_history_2020,
	title = {A history of {MATLAB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3386331},
	doi = {10.1145/3386331},
	abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
	number = {HOPL},
	urldate = {2024-03-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Moler, Cleve and Little, Jack},
	month = jun,
	year = {2020},
	keywords = {linear algebra, MATLAB, matrix computation},
	pages = {81:1--81:67},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2HANKRI8/Moler and Little - 2020 - A history of MATLAB.pdf:application/pdf},
}

@book{abelson_structure_1996,
	title = {Structure and {Interpretation} of {Computer} {Programs}},
	isbn = {978-0-262-51087-5 978-0-262-31091-8},
	url = {https://library.oapen.org/handle/20.500.12657/26092},
	abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
	language = {English},
	urldate = {2024-03-18},
	publisher = {The MIT Press},
	author = {Abelson, Harold and Sussman, Gerald Jay},
	month = jul,
	year = {1996},
	note = {Accepted: 2019-01-17 23:55},
	keywords = {bic Book Industry Communication::U Computing \& information technology::UY Computer science},
	file = {Full Text PDF:/Users/willow/Zotero/storage/KZEF98BF/Abelson and Sussman - 1996 - Structure and Interpretation of Computer Programs.pdf:application/pdf},
}

@book{knuth_art_1997,
	title = {The {Art} of {Computer} {Programming}: {Fundamental} {Algorithms}, {Volume} 1},
	isbn = {978-0-321-63574-7},
	shorttitle = {The {Art} of {Computer} {Programming}},
	abstract = {\&\&gt;The bible of all fundamental algorithms and the work that taught many of today's software developers most of what they know about computer programming.  —Byte, September 1995   I can't begin to tell you how many pleasurable hours of study and recreation they have afforded me! I have pored over them in cars, restaurants, at work, at home... and even at a Little League game when my son wasn't in the line-up. —Charles Long   If you think you're a really good programmer... read [Knuth's] Art of Computer Programming... You should definitely send me a resume if you can read the whole thing. —Bill Gates   It's always a pleasure when a problem is hard enough that you have to get the Knuths off the shelf. I find that merely opening one has a very useful terrorizing effect on computers. —Jonathan Laventhol   This first volume in the series begins with basic programming concepts and techniques, then focuses more particularly on information structures—the representation of information inside a computer, the structural relationships between data elements and how to deal with them efficiently. Elementary applications are given to simulation, numerical methods, symbolic computing, software and system design. Dozens of simple and important algorithms and techniques have been added to those of the previous edition. The section on mathematical preliminaries has been extensively revised to match present trends in research.  Ebook (PDF version) produced by Mathematical Sciences Publishers (MSP),http://msp.org},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Knuth, Donald E.},
	month = jul,
	year = {1997},
	note = {Google-Books-ID: x9AsAwAAQBAJ},
	keywords = {Computers / Programming / General},
}
