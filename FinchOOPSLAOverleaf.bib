
BibTeX
@inproceedings{sparseTIR,
author = {Ye, Zihao and Lai, Ruihang and Shao, Junru and Chen, Tianqi and Ceze, Luis},
title = {SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582047},
doi = {10.1145/3582016.3582047},
abstract = {Sparse tensors are rapidly becoming critical components of modern deep learning workloads. However, developing high-performance sparse operators can be difficult and tedious, and existing vendor libraries cannot satisfy the escalating demands from new operators. Sparse tensor compilers simplify the development of operators, but efficient sparse compilation for deep learning remains challenging because a single sparse format cannot maximize hardware efficiency, and single-shot compilers cannot keep up with latest hardware and system advances. In this paper, we observe that the key to addressing both these challenges is to leverage composable formats and composable transformations. We propose SparseTIR, a sparse tensor compilation abstraction that offers composable formats and composable transformations for deep learning workloads. SparseTIR constructs a search space over these composable components for performance tuning. With these improvements, SparseTIR obtains consistent performance speedups vs vendor libraries on GPUs for single operators: 1.20-2.34x for GNN operators, 1.05-2.98x for sparse attention operators, and 0.56-7.45x for sparse convolution operators. SparseTIR also accelerates end-to-end GNNs by 1.08-1.52x for GraphSAGE training, and 4.20-40.18x for RGCN inference.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {660â€“678},
numpages = {19},
keywords = {Vectorization, Tensor Cores, Tensor Compilers, Sparse Computation, Scheduling, Kernel Fusion, Code Generation and Optimizations},
location = {, Vancouver, BC, Canada, },
series = {ASPLOS 2023}
}



@article{won2023unified,
  title={Unified Convolution Framework: A compiler-based approach to support sparse convolutions},
  author={Won, Jaeyeon and Hong, Changwan and Mendis, Charith and Emer, Joel and Amarasinghe, Saman},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}


@article{sze2017efficient,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Proceedings of the IEEE},
  volume={105},
  number={12},
  pages={2295--2329},
  year={2017},
  publisher={Ieee}
}

@article{chou2022compilation,
  title={Compilation of dynamic sparse tensor algebra},
  author={Chou, Stephen and Amarasinghe, Saman},
  journal={Proceedings of the ACM on Programming Languages},
  volume={6},
  number={OOPSLA2},
  pages={1408--1437},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{senanayake2020sparse,
  title={A sparse iteration space transformation framework for sparse tensor algebra},
  author={Senanayake, Ryan and Hong, Changwan and Wang, Ziheng and Wilson, Amalee and Chou, Stephen and Kamil, Shoaib and Amarasinghe, Saman and Kjolstad, Fredrik},
  journal={Proceedings of the ACM on Programming Languages},
  volume={4},
  number={OOPSLA},
  pages={1--30},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{chou2018format,
  title={Format abstraction for sparse tensor algebra compilers},
  author={Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
  journal={Proceedings of the ACM on Programming Languages},
  volume={2},
  number={OOPSLA},
  pages={1--30},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@article{ghorbani2023compiling,
  title={Compiling structured tensor algebra},
  author={Ghorbani, Mahdi and Huot, Mathieu and Hashemian, Shideh and Shaikhha, Amir},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={OOPSLA2},
  pages={204--233},
  year={2023},
  publisher={ACM New York, NY, USA}
}
