
@inproceedings{im_optimizing_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimizing {Sparse} {Matrix} {Computations} for {Register} {Reuse} in {SPARSITY}},
	isbn = {978-3-540-42232-7 978-3-540-45545-5},
	url = {https://link.springer.com/chapter/10.1007/3-540-45545-0_22},
	doi = {10.1007/3-540-45545-0_22},
	abstract = {Sparse matrix-vector multiplication is an important computational kernel that tends to perform poorly on modern processors, largely because of its high ratio of memory operations to arithmetic operations. Optimizing this algorithm is difficult, both because of the complexity of memory systems and because the performance is highly dependent on the nonzero structure of the matrix. The Sparsity system is designed to address these problem by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. The most difficult aspect of optimizing these algorithms is selecting among a large set of possible transformations and choosing parameters, such as block size. In this paper we discuss the optimization of two operations: a sparse matrix times a dense vector and a sparse matrix times a set of dense vectors. Our experience indicates that for matrices arising in scientific simulations, register level optimizations are critical, and we focus here on the optimizations and parameter selection techniques used in Sparsity for register-level optimizations. We demonstrate speedups of up to 2× for the single vector case and 5× for the multiple vector case.},
	language = {en},
	urldate = {2018-02-06},
	booktitle = {Computational {Science} — {ICCS} 2001},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Im, Eun-Jin and Yelick, Katherine},
	month = may,
	year = {2001},
	pages = {127--136},
	file = {Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:/Users/willow/Zotero/storage/GW3ZQ2X8/Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, MNIST, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRS5H7PF/726791.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/V69R5ZYM/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{eitz_how_2012,
	title = {How do humans sketch objects?},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2185520.2185540},
	doi = {10.1145/2185520.2185540},
	abstract = {Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73\% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56\% accuracy (chance is 0.4\%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.},
	number = {4},
	urldate = {2022-08-31},
	journal = {ACM Transactions on Graphics},
	author = {Eitz, Mathias and Hays, James and Alexa, Marc},
	month = jul,
	year = {2012},
	keywords = {crowd-sourcing, learning, recognition, sketch},
	pages = {44:1--44:10},
	file = {Full Text PDF:/Users/willow/Zotero/storage/VNICQZRI/Eitz et al. - 2012 - How do humans sketch objects.pdf:application/pdf},
}

@misc{ahrens_optimal_2021,
	title = {On {Optimal} {Partitioning} {For} {Sparse} {Matrices} {In} {Variable} {Block} {Row} {Format}},
	url = {http://arxiv.org/abs/2005.12414},
	doi = {10.48550/arXiv.2005.12414},
	abstract = {The Variable Block Row (VBR) format is an influential blocked sparse matrix format designed for matrices with shared sparsity structure between adjacent rows and columns. VBR groups adjacent rows and columns, storing the resulting blocks that contain nonzeros in a dense format. This reduces the memory footprint and enables optimizations such as register blocking and instruction-level parallelism. Existing approaches use heuristics to determine which rows and columns should be grouped together. We show that finding the optimal grouping of rows and columns for VBR is NP-hard under several reasonable cost models. In light of this finding, we propose a 1-dimensional variant of VBR, called 1D-VBR, which achieves better performance than VBR by only grouping rows. We describe detailed cost models for runtime and memory consumption. Then, we describe a linear time dynamic programming solution for optimally grouping the rows for 1D-VBR format. We extend our algorithm to produce a heuristic VBR partitioner which alternates between optimally partitioning rows and columns, assuming the columns or rows to be fixed, respectively. Our alternating heuristic produces VBR matrices with the smallest memory footprint of any partitioner we tested.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Ahrens, Willow and Boman, Erik G.},
	month = may,
	year = {2021},
	note = {arXiv:2005.12414 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/BWHWXSNS/Ahrens and Boman - 2021 - On Optimal Partitioning For Sparse Matrices In Var.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/E63KTTB9/2005.html:text/html},
}

@inproceedings{backus_fortran_1957,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '57 ({Western})},
	title = {The {FORTRAN} automatic coding system},
	isbn = {978-1-4503-7861-1},
	url = {https://doi.org/10.1145/1455567.1455599},
	doi = {10.1145/1455567.1455599},
	abstract = {The FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM's next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan. The goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.},
	urldate = {2022-08-04},
	booktitle = {Papers presented at the {February} 26-28, 1957, western joint computer conference: {Techniques} for reliability},
	publisher = {Association for Computing Machinery},
	author = {Backus, J. W. and Beeber, R. J. and Best, S. and Goldberg, R. and Haibt, L. M. and Herrick, H. L. and Nelson, R. A. and Sayre, D. and Sheridan, P. B. and Stern, H. and Ziller, I. and Hughes, R. A. and Nutt, R.},
	month = feb,
	year = {1957},
	pages = {188--198},
	file = {Full Text PDF:/Users/willow/Zotero/storage/SUHHBWFU/Backus et al. - 1957 - The FORTRAN automatic coding system.pdf:application/pdf},
}

@inproceedings{donenfeld_unified_2022,
	title = {Unified {Compilation} for {Lossless} {Compression} and {Sparse} {Computing}},
	doi = {10.1109/CGO53902.2022.9741282},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to support lossless compression techniques, including variants of run-length encoding and Lempel-Ziv compression. We develop new abstractions to represent losslessly compressed data as a generalized form of sparse tensors, with repetitions of values (which are compressed out in storage) represented by non-scalar, dynamic fill values. We then show how a compiler can use these abstractions to emit efficient code that computes on losslessly compressed data. By unifying lossless compression with sparse tensor algebra, our technique is able to generate code that computes with both losslessly compressed data and sparse data, as well as generate code that computes directly on compressed data without needing to first decompress it.Our evaluation shows our technique generates efficient image and video processing kernels that compute on losslessly compressed data. We find that the generated kernels are up to 16. 3× faster than equivalent dense kernels generated by TACO, a tensor algebra compiler, and up to 16. 1× faster than OpenCV, a widely used image processing library.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Donenfeld, Daniel and Chou, Stephen and Amarasinghe, Saman},
	month = apr,
	year = {2022},
	keywords = {Algebra, Codes, compressed domain processing, Computational efficiency, Image coding, Kernel, Libraries, lossless compression, sparse tensor algebra, Tensors},
	pages = {205--216},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/7FMZUU5R/9741282.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/JLWQKCNS/Donenfeld et al. - 2022 - Unified Compilation for Lossless Compression and S.pdf:application/pdf},
}

@article{psarras_landscape_2021,
	title = {The landscape of software for tensor computations},
	url = {http://arxiv.org/abs/2103.13756},
	abstract = {Tensors (also commonly seen as multi-linear operators or as multi-dimensional arrays) are ubiquitous in scientiﬁc computing and in data science, and so are the software eﬀorts for tensor operations. Particularly in recent years, we have observed an explosion in libraries, compilers, packages, and toolboxes; unfortunately these eﬀorts are very much scattered among the diﬀerent scientiﬁc domains, and inevitably suﬀer from replication, suboptimal implementations, and in many cases, limited visibility. As a ﬁrst step towards countering these ineﬃciencies, here we survey and loosely classify software packages related to tensor computations. Our aim is to assemble a comprehensive and up-to-date snapshot of the tensor software landscape, with the intention of helping both users and developers. Aware of the diﬃculties inherent in any multi-discipline survey, we very much welcome the reader’s help in amending and expanding our software list, which currently features 77 projects.},
	language = {en},
	urldate = {2022-04-29},
	journal = {arXiv:2103.13756 [cs]},
	author = {Psarras, Christos and Karlsson, Lars and Li, Jiajia and Bientinesi, Paolo},
	month = may,
	year = {2021},
	note = {arXiv: 2103.13756},
	keywords = {Computer Science - Mathematical Software},
	file = {Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:/Users/willow/Zotero/storage/9FYH3IWU/Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:application/pdf},
}

@inproceedings{ahrens_autoscheduling_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Autoscheduling for sparse tensor algebra with an asymptotic cost model},
	copyright = {All rights reserved},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523442},
	doi = {10.1145/3519939.3523442},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small Pareto frontier of asymptotically non-dominating kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = jun,
	year = {2022},
	keywords = {Asymptotic Analysis, Automatic Scheduling, Compilers, Conjunctive Query Containment, Query Optimization, Sparse Tensors},
	pages = {269--285},
	file = {Full Text PDF:/Users/willow/Zotero/storage/V6PAKBZT/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/QDCN3ULM/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf},
}

@article{hu_taichi_2019,
	title = {Taichi: a language for high-performance computation on spatially sparse data structures},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {Taichi},
	url = {https://doi.org/10.1145/3355089.3356506},
	doi = {10.1145/3355089.3356506},
	abstract = {3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multi-level sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose Taichi, a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This decoupling of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With 1{\textless}u{\textgreater}1{\textless}/u{\textgreater}0 th as many lines of code, we achieve 4.55× higher performance on average, compared to hand-optimized reference implementations.},
	number = {6},
	urldate = {2020-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Hu, Yuanming and Li, Tzu-Mao and Anderson, Luke and Ragan-Kelley, Jonathan and Durand, Frédo},
	month = nov,
	year = {2019},
	keywords = {GPU computing, sparse data structures},
	pages = {201:1--201:16},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LI4HK8PK/Hu et al. - 2019 - Taichi a language for high-performance computatio.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Number: 7825
Publisher: Nature Publishing Group},
	pages = {357--362},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZFNZYPK/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/IFYNZTWL/s41586-020-2649-2.html:text/html},
}

@article{langr_evaluation_2016,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	volume = {27},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2015.2401575},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langr, Daniel and Tvrdík, Pavel},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {sparse matrix-vector multiplication, Sparse matrices, Runtime, parallel processing, Indexes, matrix-vector multiplication, sparse matrix, Matrix converters, mathematics computing, test matrices, Standards, vectors, storage format, matrix algebra, Memory management, evaluation criteria, Evaluation criterion, FLOPS, HPC application, memory footprint, nonzero matrix structure, single evaluation criterion, sparse matrix storage formats, SpMV-based application phase},
	pages = {428--440},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRZMJ4TJ/7036061.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/D75SXF9Z/7036061.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/8IRGEGDG/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/I3FNKNCF/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	url = {https://doi.org/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
	number = {OOPSLA},
	urldate = {2021-11-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	keywords = {Compilation, Sparse Array Programming, Sparse Arrays},
	pages = {128:1--128:29},
	file = {Full Text PDF:/Users/willow/Zotero/storage/724NU8UX/Henry et al. - 2021 - Compilation of sparse array programming models.pdf:application/pdf},
}

@inproceedings{liu_csr5_2015,
	address = {New York, NY, USA},
	series = {{ICS} '15},
	title = {{CSR5}: {An} {Efficient} {Storage} {Format} for {Cross}-{Platform} {Sparse} {Matrix}-{Vector} {Multiplication}},
	isbn = {978-1-4503-3559-1},
	shorttitle = {{CSR5}},
	url = {https://doi.org/10.1145/2751205.2751209},
	doi = {10.1145/2751205.2751209},
	abstract = {Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations. We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6\%, 28.5\%, 173.0\% and 293.3\% (up to 213.3\%, 153.6\%, 405.1\% and 943.3\%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the 29th {ACM} on {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Weifeng and Vinter, Brian},
	month = jun,
	year = {2015},
	keywords = {cpu, csr, csr5, gpu, sparse matrices, spmv, storage formats, xeon phi},
	pages = {339--350},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6PDCWARD/Liu and Vinter - 2015 - CSR5 An Efficient Storage Format for Cross-Platfo.pdf:application/pdf},
}

@article{kjolstad_tensor_2017,
	title = {The {Tensor} {Algebra} {Compiler}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2019-11-20},
	journal = {Proc. ACM Program. Lang.},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {parallelism, code generation, linear algebra, tensors, performance, sparse data structures, iteration graphs, merge lattices, tensor algebra},
	pages = {77:1--77:29},
	file = {ACM Full Text PDF:/Users/willow/Zotero/storage/M3YTNCUI/Kjolstad et al. - 2017 - The Tensor Algebra Compiler.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/EZXEWXYJ/Kjolstad et al. - 2017 - The tensor algebra compiler.pdf:application/pdf;kjolstad-oopsla17-taco-preprint.pdf:/Users/willow/Zotero/storage/XUPL86I6/kjolstad-oopsla17-taco-preprint.pdf:application/pdf},
}

@inproceedings{vuduc_performance_2002,
	title = {Performance {Optimizations} and {Bounds} for {Sparse} {Matrix}-{Vector} {Multiply}},
	doi = {10.1109/SC.2002.10025},
	abstract = {We consider performance tuning, by code and data structure reorganization, of sparse matrix-vector multiply (SpM×V), one of the most important computational kernels in scientific applications. This paper addresses the fundamental questions of what limits exist on such performance tuning, and how closely tuned code approaches these limits. Specifically, we develop upper and lower bounds on the performance (Mflop/s) of SpM×V when tuned using our previously proposed register blocking optimization. These bounds are based on the non-zero pattern in the matrix and the cost of basic memory operations, such as cache hits and misses. We evaluate our tuned implementations with respect to these bounds using hardware counter data on 4 different platforms and on test set of 44 sparse matrices. We find that we can often get within 20\% of the upper bound, particularly on class of matrices from finite element modeling (FEM) problems; on non-FEM matrices, performance improvements of 2× are still possible. Lastly, we present new heuristic that selects optimal or near-optimal register block sizes (the key tuning parameters) more accurately than our previous heuristic. Using the new heuristic, we show improvements in SpM×V performance (Mflop/s) by as much as 2.5× over an untuned implementation. Collectively, our results suggest that future performance improvements, beyond those that we have already demonstrated for SpM×V, will come from two sources: (1) consideration of higher-level matrix structures (e.g. exploiting symmetry, matrix reordering, multiple register block sizes), and (2) optimizing kernels with more opportunity for data reuse (e.g. sparse matrix-multiple vector multiply, multiplication of AT A by a vector).},
	booktitle = {{SC} '02: {Proceedings} of the 2002 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Vuduc, R. and Demmel, J.W. and Yelick, K.A. and Kamil, S. and Nishtala, R. and Lee, B.},
	month = nov,
	year = {2002},
	note = {ISSN: 1063-9535},
	keywords = {Costs, Counting circuits, Data structures, Hardware, Kernel, Optimization, Registers, Sparse matrices, Testing, Upper bound},
	pages = {26--26},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/NB49QD9N/1592862.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/BLUY9Q7P/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf;Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:/Users/willow/Zotero/storage/EIBMBAFB/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf},
}

@inproceedings{ragan-kelley_halide_2013,
	address = {New York, NY, USA},
	series = {{PLDI} '13},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Halide},
	url = {http://doi.org/10.1145/2491956.2462176},
	doi = {10.1145/2491956.2462176},
	abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Frédo and Amarasinghe, Saman},
	month = jun,
	year = {2013},
	keywords = {autotuning, parallelism, gpu, optimization, vectorization, locality, compiler, image processing, domain specific language, redundant computation},
	pages = {519--530},
	file = {Full Text PDF:/Users/willow/Zotero/storage/QZF9XYFW/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	volume = {17},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	language = {en},
	number = {3},
	urldate = {2021-11-19},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = mar,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biophysical chemistry;Computational biology and bioinformatics;Technology
Subject\_term\_id: biophysical-chemistry;computational-biology-and-bioinformatics;technology},
	keywords = {Biophysical chemistry, Computational biology and bioinformatics, Technology},
	pages = {261--272},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2Q9W83VV/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/V7KB63J9/s41592-019-0686-2.html:text/html},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	urldate = {2021-11-19},
	booktitle = {12th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 16)},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Q4MBXVX8/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@inproceedings{williams_optimization_2007,
	address = {New York, NY, USA},
	series = {{SC} '07},
	title = {Optimization of sparse matrix-vector multiplication on emerging multicore platforms},
	isbn = {978-1-59593-764-3},
	url = {http://doi.org/10.1145/1362622.1362674},
	doi = {10.1145/1362622.1362674},
	abstract = {We are witnessing a dramatic change in computer architecture due to the multicore paradigm shift, as every electronic device from cell phones to supercomputers confronts parallelism of unprecedented scale. To fully unleash the potential of these systems, the HPC community must develop multicore specific optimization methodologies for important scientific computations. In this work, we examine sparse matrix-vector multiply (SpMV) - one of the most heavily used kernels in scientific computing - across a broad spectrum of multicore designs. Our experimental platform includes the homogeneous AMD dual-core and Intel quad-core designs, the heterogeneous STI Cell, as well as the first scientific study of the highly multithreaded Sun Niagara2. We present several optimization strategies especially effective for the multicore environment, and demonstrate significant performance improvements compared to existing state-of-the-art serial and parallel SpMV implementations. Additionally, we present key insights into the architectural tradeoffs of leading multicore design strategies, in the context of demanding memory-bound numerical algorithms.},
	urldate = {2022-03-15},
	booktitle = {Proceedings of the 2007 {ACM}/{IEEE} conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Williams, Samuel and Oliker, Leonid and Vuduc, Richard and Shalf, John and Yelick, Katherine and Demmel, James},
	month = nov,
	year = {2007},
	keywords = {Sparse matrices, Supercomputers, Kernel, Multicore processing, Parallel processing, Computer architecture, Scientific computing, Cellular phones, Optimization methods, Sun},
	pages = {1--12},
	file = {Full Text PDF:/Users/willow/Zotero/storage/G4VQIMPC/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/P7C29HUU/5348797.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/XVTJHNHK/5348797.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/MLDMCFE7/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf},
}

@article{zhou_enabling_2020,
	title = {Enabling {Runtime} {SpMV} {Format} {Selection} through an {Overhead} {Conscious} {Method}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2932931},
	abstract = {Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix; it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give.},
	number = {1},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhou, Weijie and Zhao, Yue and Shen, Xipeng and Chen, Wang},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Buildings, high performance computing, Kernel, Matrix converters, prediction model, Predictive models, program optimization, Runtime, Sparse matrices, sparse matrix format, SpMV, Time series analysis},
	pages = {80--93},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/6S64R2T2/8787872.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/PTUF7P8M/Zhou et al. - 2020 - Enabling Runtime SpMV Format Selection through an .pdf:application/pdf},
}

@inproceedings{kjolstad_tensor_2019,
	title = {Tensor {Algebra} {Compilation} with {Workspaces}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1109/CGO.2019.8661185},
	doi = {10.1109/CGO.2019.8661185},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to introduce temporary tensors called workspaces to avoid inefficient sparse data structures accesses. We develop an intermediate representation (IR) for tensor operations called concrete index notation that specifies when sub-computations occur and where they are stored. We then describe the workspace transformation in this IR, how to programmatically invoke it, and how the IR is compiled to sparse code. Finally, we show how the transformation can be used to optimize sparse tensor kernels, including sparse matrix multiplication, sparse tensor addition, and the matricized tensor times Khatri-Rao product (MTTKRP). Our results show that the workspace transformation brings the performance of these kernels on par with hand-optimized implementations. For example, we improve the performance of MTTKRP with dense output by up to 35\%, and enable generating sparse matrix multiplication and MTTKRP with sparse output, neither of which were supported by prior tensor algebra compilers.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Kjolstad, Fredrik and Ahrens, Willow and Kamil, Shoaib and Amarasinghe, Saman},
	month = feb,
	year = {2019},
	keywords = {Algebra, Arrays, code optimization, compiler IR, concrete index notation, data structures, Indexes, intermediate representation, Kernel, matricized tensor times Khatri-Rao product, matrix decomposition, matrix multiplication, program compilers, sparse data structures, sparse matrices, Sparse matrices, sparse matrix multiplication, sparse tensor addition, sparse tensor algebra, sparse tensor algebra compilers, sparse tensor kernels, temporaries, tensors, workspaces},
	pages = {180--192},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FCYUD2HP/Kjolstad et al. - 2019 - Tensor algebra compilation with workspaces.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/SAZD6MM9/8661185.html:text/html;Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:/Users/willow/Zotero/storage/K5Z25DFE/Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:application/pdf},
}

@inproceedings{ahrens_looplets_2023,
	address = {New York, NY, USA},
	series = {{CGO} 2023},
	title = {Looplets: {A} {Language} for {Structured} {Coiteration}},
	isbn = {9798400701016},
	shorttitle = {Looplets},
	doi = {10.1145/3579990.3580020},
	abstract = {Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Specializing for structure yields significant speedups. But automatically generating efficient code for structured data is challenging, especially when arrays with different structure interact. We show how to abstract over array structures so that the compiler can generate code to coiterate over any combination of them. Our technique enables new array formats (such as 1DVBL for irregular clustered sparsity), new iteration strategies (such as galloping intersections), and new operations over structured data (such as concatenation or convolution).},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Donenfeld, Daniel and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = feb,
	year = {2023},
	keywords = {Sparse, Tensor, Array, Coiteration, Compressed},
	pages = {41--54},
	file = {Full Text PDF:/Users/willow/Zotero/storage/HUBGW7N9/Ahrens et al. - 2023 - Looplets A Language for Structured Coiteration.pdf:application/pdf},
}

@article{psarras_linear_2022,
	title = {The {Linear} {Algebra} {Mapping} {Problem}. {Current} state of linear algebra languages and libraries},
	volume = {48},
	issn = {0098-3500, 1557-7295},
	url = {http://arxiv.org/abs/1911.09421},
	doi = {10.1145/3549935},
	abstract = {We observe a disconnect between the developers and the end users of linear algebra libraries. On the one hand, the numerical linear algebra and the high-performance communities invest significant effort in the development and optimization of highly sophisticated numerical kernels and libraries, aiming at the maximum exploitation of both the properties of the input matrices, and the architectural features of the target computing platform. On the other hand, end users are progressively less likely to go through the error-prone and time consuming process of directly using said libraries by writing their code in C or Fortran; instead, languages and libraries such as Matlab, Julia, Eigen and Armadillo, which offer a higher level of abstraction, are becoming more and more popular. Users are given the opportunity to code matrix computations with a syntax that closely resembles the mathematical description; it is then a compiler or an interpreter that internally maps the input program to lower level kernels, as provided by libraries such as BLAS and LAPACK. Unfortunately, our experience suggests that in terms of performance, this translation is typically vastly suboptimal. In this paper, we first introduce the Linear Algebra Mapping Problem, and then investigate how effectively a benchmark of test problems is solved by popular high-level programming languages. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both standard compiler optimizations such as common subexpression elimination and loop-invariant code motion, as well as linear algebra specific optimizations such as optimal parenthesization of a matrix product and kernel selection for matrices with properties. The aim of this study is to give concrete guidelines for the development of languages and libraries that support linear algebra computations.},
	number = {3},
	urldate = {2023-10-03},
	journal = {ACM Transactions on Mathematical Software},
	author = {Psarras, Christos and Barthels, Henrik and Bientinesi, Paolo},
	month = sep,
	year = {2022},
	note = {arXiv:1911.09421 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Mathematical Software},
	pages = {1--30},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/6APM5QY7/Psarras et al. - 2022 - The Linear Algebra Mapping Problem. Current state .pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/6JF2TJK5/1911.html:text/html},
}

@inproceedings{lo_roofline_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	isbn = {978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	doi = {10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	keywords = {CUDA unified memory, Memory bandwidth, Roofline},
	pages = {129--148},
	file = {Submitted Version:/Users/willow/Zotero/storage/X3YWPR6T/Lo et al. - 2015 - Roofline Model Toolkit A Practical Tool for Archi.pdf:application/pdf},
}

@inproceedings{fegade_cora_2022,
	title = {The {CoRa} {Tensor} {Compiler}: {Compilation} for {Ragged} {Tensors} with {Minimal} {Padding}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems}},
	author = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip and Mowry, Todd},
	editor = {Marculescu, D. and Chi, Y. and Wu, C.},
	year = {2022},
	pages = {721--747},
	file = {Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/8PT7K7GL/Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018,
	address = {Eugene OR USA},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer’s push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebrabased framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-ofthe-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {ACM},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	pages = {1--11},
	file = {Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:/Users/willow/Zotero/storage/ADD2JSKL/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018-1,
	address = {New York, NY, USA},
	series = {{ICPP} '18},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer's push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebra-based framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-of-the-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	keywords = {graph algorithms, sparse matrix multiplication, breadth-first search},
	pages = {1--11},
	file = {Full Text PDF:/Users/willow/Zotero/storage/NGHWT98P/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6GTQB2I8/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{moler_history_2020,
	title = {A history of {MATLAB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3386331},
	doi = {10.1145/3386331},
	abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
	number = {HOPL},
	urldate = {2024-03-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Moler, Cleve and Little, Jack},
	month = jun,
	year = {2020},
	keywords = {linear algebra, MATLAB, matrix computation},
	pages = {81:1--81:67},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2HANKRI8/Moler and Little - 2020 - A history of MATLAB.pdf:application/pdf},
}

@book{abelson_structure_1996,
	title = {Structure and {Interpretation} of {Computer} {Programs}},
	isbn = {978-0-262-51087-5 978-0-262-31091-8},
	url = {https://library.oapen.org/handle/20.500.12657/26092},
	abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
	language = {English},
	urldate = {2024-03-18},
	publisher = {The MIT Press},
	author = {Abelson, Harold and Sussman, Gerald Jay},
	month = jul,
	year = {1996},
	note = {Accepted: 2019-01-17 23:55},
	keywords = {bic Book Industry Communication::U Computing \& information technology::UY Computer science},
	file = {Full Text PDF:/Users/willow/Zotero/storage/KZEF98BF/Abelson and Sussman - 1996 - Structure and Interpretation of Computer Programs.pdf:application/pdf},
}

@book{knuth_art_1997,
	title = {The {Art} of {Computer} {Programming}: {Fundamental} {Algorithms}, {Volume} 1},
	isbn = {978-0-321-63574-7},
	shorttitle = {The {Art} of {Computer} {Programming}},
	abstract = {\&\&gt;The bible of all fundamental algorithms and the work that taught many of today's software developers most of what they know about computer programming.  —Byte, September 1995   I can't begin to tell you how many pleasurable hours of study and recreation they have afforded me! I have pored over them in cars, restaurants, at work, at home... and even at a Little League game when my son wasn't in the line-up. —Charles Long   If you think you're a really good programmer... read [Knuth's] Art of Computer Programming... You should definitely send me a resume if you can read the whole thing. —Bill Gates   It's always a pleasure when a problem is hard enough that you have to get the Knuths off the shelf. I find that merely opening one has a very useful terrorizing effect on computers. —Jonathan Laventhol   This first volume in the series begins with basic programming concepts and techniques, then focuses more particularly on information structures—the representation of information inside a computer, the structural relationships between data elements and how to deal with them efficiently. Elementary applications are given to simulation, numerical methods, symbolic computing, software and system design. Dozens of simple and important algorithms and techniques have been added to those of the previous edition. The section on mathematical preliminaries has been extensively revised to match present trends in research.  Ebook (PDF version) produced by Mathematical Sciences Publishers (MSP),http://msp.org},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Knuth, Donald E.},
	month = jul,
	year = {1997},
	note = {Google-Books-ID: x9AsAwAAQBAJ},
	keywords = {Computers / Programming / General},
}

@inproceedings{dias_sparselnr_2022,
	address = {Virtual Event},
	title = {{SparseLNR}: accelerating sparse tensor computations using loop nest restructuring},
	isbn = {978-1-4503-9281-5},
	shorttitle = {{SparseLNR}},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532386},
	doi = {10.1145/3524059.3532386},
	abstract = {Sparse tensor algebra computations have become important in many real-world applications like machine learning, scientific simulations, and data mining. Hence, automated code generation and performance optimizations for tensor algebra kernels are paramount. Recent advancements such as the Tensor Algebra Compiler (TACO) greatly generalize and automate the code generation for tensor algebra expressions. However, the code generated by TACO for many important tensor computations remains suboptimal due to the absence of a scheduling directive to support transformations such as distribution/fusion.},
	language = {en},
	urldate = {2024-03-20},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Dias, Adhitha and Sundararajah, Kirshanthan and Saumya, Charitha and Kulkarni, Milind},
	month = jun,
	year = {2022},
	pages = {1--14},
	file = {Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:/Users/willow/Zotero/storage/ESTZ5XHN/Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:application/pdf},
}

@article{fisher_hypermedia_1996,
	title = {Hypermedia image processing reference},
	journal = {England: John Wiley \& Sons Ltd},
	author = {Fisher, Robert and Perkins, Simon and Walker, Ashley and Wolfart, Erik},
	year = {1996},
	pages = {118--130},
	file = {Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:/Users/willow/Zotero/storage/HT6FNWC4/Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	number = {6266},
	urldate = {2024-04-01},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	month = dec,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {Omniglot},
	pages = {1332--1338},
}

@book{gonzalez_digital_2006,
	address = {USA},
	title = {Digital {Image} {Processing} (3rd {Edition})},
	isbn = {978-0-13-168728-8},
	publisher = {Prentice-Hall, Inc.},
	author = {Gonzalez, Rafael C. and Woods, Richard E.},
	month = jan,
	year = {2006},
}
