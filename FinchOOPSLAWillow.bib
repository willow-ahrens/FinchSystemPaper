
@book{saad_iterative_2003,
	address = {Philadelphia},
	edition = {2nd},
	title = {Iterative methods for sparse linear systems},
	isbn = {978-0-89871-534-7},
	publisher = {SIAM},
	author = {Saad, Yousef},
	year = {2003},
	keywords = {Differential equations, Partial, Iterative methods (Mathematics), Numerical solutions, Sparse matrices},
	file = {errata_1.pdf:/Users/willow/Zotero/storage/DPHY8WHG/errata_1.pdf:application/pdf;errata_2.pdf:/Users/willow/Zotero/storage/DLEXN7DL/errata_2.pdf:application/pdf;Saad - 2003 - Iterative methods for sparse linear systems.pdf:/Users/willow/Zotero/storage/MSLIU2HN/Saad - 2003 - Iterative methods for sparse linear systems.pdf:application/pdf},
}

@techreport{saad_sparskit_1990,
	title = {{SPARSKIT}: {A} basic tool kit for sparse matrix computations},
	shorttitle = {{SPARSKIT}},
	url = {https://ntrs.nasa.gov/search.jsp?R=19910023551},
	abstract = {Presented here are the main features of a tool package for manipulating and working with sparse matrices. One of the goals of the package is to provide basic tools to facilitate the exchange of software and data between researchers in sparse matrix computations. The starting point is the Harwell/Boeing collection of matrices for which the authors provide a number of tools. Among other things, the package provides programs for converting data structures, printing simple statistics on a matrix, plotting a matrix profile, and performing linear algebra operations with sparse matrices.},
	urldate = {2020-01-26},
	author = {Saad, Youcef},
	month = may,
	year = {1990},
	keywords = {data structures, format, algebra, applications programs, computation, matrices, plotting, printing},
	file = {NASA NTRS Full Text PDF:/Users/willow/Zotero/storage/SRM6DKTQ/Saad - 1990 - SPARSKIT A basic tool kit for sparse matrix compu.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/EQJWVI5Z/search.html:text/html},
}

@inproceedings{smith_splatt:_2015,
	address = {Washington, DC, USA},
	series = {{IPDPS} '15},
	title = {{SPLATT}: {Efficient} and {Parallel} {Sparse} {Tensor}-{Matrix} {Multiplication}},
	isbn = {978-1-4799-8649-1},
	shorttitle = {{SPLATT}},
	url = {https://doi.org/10.1109/IPDPS.2015.27},
	doi = {10.1109/IPDPS.2015.27},
	abstract = {Multi-dimensional arrays, or tensors, are increasingly found in fields such as signal processing and recommender systems. Real-world tensors can be enormous in size and often very sparse. There is a need for efficient, high-performance tools capable of processing the massive sparse tensors of today and the future. This paper introduces SPLATT, a C library with shared-memory parallelism for three-mode tensors. SPLATT contains algorithmic improvements over competing state of the art tools for sparse tensor factorization. SPLATT has a fast, parallel method of multiplying a matricide tensor by a Khatri-Rao product, which is a key kernel in tensor factorization methods. SPLATT uses a novel data structure that exploits the sparsity patterns of tensors. This data structure has a small memory footprint similar to competing methods and allows for the computational improvements featured in our work. We also present a method of finding cache-friendly reordering and utilizing them with a novel form of cache tiling. To our knowledge, this is the first work to investigate reordering and cache tiling in this context. SPLATT averages almost 30x speedup compared to our baseline when using 16 threads and reaches over 80x speedup on NELL-2.},
	urldate = {2019-11-20},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	publisher = {IEEE Computer Society},
	author = {Smith, Shaden and Ravindran, Niranjay and Sidiropoulos, Nicholas D. and Karypis, George},
	year = {2015},
	keywords = {CANDECOMP, CPD, PARAFAC, parallel, Sparse tensors},
	pages = {61--70},
	file = {Smith et al. - 2015 - SPLATT Efficient and Parallel Sparse Tensor-Matri.pdf:/Users/willow/Zotero/storage/L3MBWXEX/Smith et al. - 2015 - SPLATT Efficient and Parallel Sparse Tensor-Matri.pdf:application/pdf;Smith et al. - 2015 - SPLATT Efficient and Parallel Sparse Tensor-Matri.pdf:/Users/willow/Zotero/storage/UWWUHFG3/Smith et al. - 2015 - SPLATT Efficient and Parallel Sparse Tensor-Matri.pdf:application/pdf},
}

@inproceedings{im_optimizing_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimizing {Sparse} {Matrix} {Computations} for {Register} {Reuse} in {SPARSITY}},
	isbn = {978-3-540-42232-7 978-3-540-45545-5},
	url = {https://link.springer.com/chapter/10.1007/3-540-45545-0_22},
	doi = {10.1007/3-540-45545-0_22},
	abstract = {Sparse matrix-vector multiplication is an important computational kernel that tends to perform poorly on modern processors, largely because of its high ratio of memory operations to arithmetic operations. Optimizing this algorithm is difficult, both because of the complexity of memory systems and because the performance is highly dependent on the nonzero structure of the matrix. The Sparsity system is designed to address these problem by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. The most difficult aspect of optimizing these algorithms is selecting among a large set of possible transformations and choosing parameters, such as block size. In this paper we discuss the optimization of two operations: a sparse matrix times a dense vector and a sparse matrix times a set of dense vectors. Our experience indicates that for matrices arising in scientific simulations, register level optimizations are critical, and we focus here on the optimizations and parameter selection techniques used in Sparsity for register-level optimizations. We demonstrate speedups of up to 2× for the single vector case and 5× for the multiple vector case.},
	language = {en},
	urldate = {2018-02-06},
	booktitle = {Computational {Science} — {ICCS} 2001},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Im, Eun-Jin and Yelick, Katherine},
	month = may,
	year = {2001},
	pages = {127--136},
	file = {Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:/Users/willow/Zotero/storage/GW3ZQ2X8/Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:application/pdf},
}

@article{dongarra_design_2017,
	series = {International {Conference} on {Computational} {Science}, {ICCS} 2017, 12-14 {June} 2017, {Zurich}, {Switzerland}},
	title = {The {Design} and {Performance} of {Batched} {BLAS} on {Modern} {High}-{Performance} {Computing} {Systems}},
	volume = {108},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050917307056},
	doi = {10.1016/j.procs.2017.05.138},
	abstract = {A current trend in high-performance computing is to decompose a large linear algebra problem into batches containing thousands of smaller problems, that can be solved independently, before collating the results. To standardize the interface to these routines, the community is developing an extension to the BLAS standard (the batched BLAS), enabling users to perform thousands of small BLAS operations in parallel whilst making efficient use of their hardware. We discuss the benefits and drawbacks of the current batched BLAS proposals and perform a number of experiments, focusing on a general matrix-matrix multiplication (GEMM), to explore their affect on the performance. In particular we analyze the effect of novel data layouts which, for example, interleave the matrices in memory to aid vectorization and prefetching of data. Utilizing these modifications our code outperforms both MKL1 CuBLAS2 by up to 6 times on the self-hosted Intel KNL (codenamed Knights Landing) and Kepler GPU architectures, for large numbers of double precision GEMM operations using matrices of size 2 × 2 to 20 × 20.},
	urldate = {2018-08-21},
	journal = {Procedia Computer Science},
	author = {Dongarra, J. and Hammarling, S. and Higham, N. and Relton, S. D. and Valero-Lara, P. and Zounon, M.},
	month = jan,
	year = {2017},
	keywords = {Batched BLAS, BLAS, High-performance computing, Memory management, Parallel processing, Scientific computing},
	pages = {495--504},
	file = {ScienceDirect Full Text PDF:/Users/willow/Zotero/storage/7BIYV7ZR/Dongarra et al. - 2017 - The Design and Performance of Batched BLAS on Mode.pdf:application/pdf;ScienceDirect Snapshot:/Users/willow/Zotero/storage/CWFKWFNH/S1877050917307056.html:text/html},
}

@article{van_zee_blis:_2015,
	title = {{BLIS}: {A} {Framework} for {Rapidly} {Instantiating} {BLAS} {Functionality}},
	volume = {41},
	issn = {0098-3500},
	shorttitle = {{BLIS}},
	url = {http://doi.acm.org/10.1145/2764454},
	doi = {10.1145/2764454},
	abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
	number = {3},
	urldate = {2018-08-02},
	journal = {ACM Trans. Math. Softw.},
	author = {Van Zee, Field G. and van de Geijn, Robert A.},
	month = jun,
	year = {2015},
	keywords = {BLAS, high-performance, libraries, Linear algebra, matrix},
	pages = {14:1--14:33},
	file = {ACM Full Text PDF:/Users/willow/Zotero/storage/4BUTN5Y8/Van Zee and van de Geijn - 2015 - BLIS A Framework for Rapidly Instantiating BLAS F.pdf:application/pdf},
}

@article{solomonik_sparse_2015,
	title = {Sparse {Tensor} {Algebra} as a {Parallel} {Programming} {Model}},
	url = {http://arxiv.org/abs/1512.00066},
	abstract = {Dense and sparse tensors allow the representation of most bulk data structures in computational science applications. We show that sparse tensor algebra can also be used to express many of the transformations on these datasets, especially those which are parallelizable. Tensor computations are a natural generalization of matrix and graph computations. We extend the usual basic operations of tensor summation and contraction to arbitrary functions, and further operations such as reductions and mapping. The expression of these transformations in a high-level sparse linear algebra domain specific language allows our framework to understand their properties at runtime to select the preferred communication-avoiding algorithm. To demonstrate the efficacy of our approach, we show how key graph algorithms as well as common numerical kernels can be succinctly expressed using our interface and provide performance results of a general library implementation.},
	urldate = {2020-06-11},
	journal = {arXiv:1512.00066 [cs]},
	author = {Solomonik, Edgar and Hoefler, Torsten},
	month = nov,
	year = {2015},
	note = {arXiv: 1512.00066},
	keywords = {Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/CSKKHSY4/Solomonik and Hoefler - 2015 - Sparse Tensor Algebra as a Parallel Programming Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/NNMHZAWR/1512.html:text/html},
}

@article{chou_compilation_2022,
	title = {Compilation of dynamic sparse tensor algebra},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3563338},
	doi = {10.1145/3563338},
	abstract = {Many applications, from social network graph analytics to control flow analysis, compute on sparse data that evolves over the course of program execution. Such data can be represented as dynamic sparse tensors and efficiently stored in formats (data layouts) that utilize pointer-based data structures like block linked lists, binary search trees, B-trees, and C-trees among others. These specialized formats support fast in-place modification and are thus better suited than traditional, array-based data structures like CSR for storing dynamic sparse tensors. However, different dynamic sparse tensor formats have distinct benefits and drawbacks, and performing different computations on tensors that are stored in different formats can require vastly dissimilar code that are not straightforward to correctly implement and optimize. This paper shows how a compiler can generate efficient code to compute tensor algebra operations on dynamic sparse tensors that may be stored in a wide range of disparate formats. We propose a language for precisely specifying recursive, pointer-based data structures, and we show how this language can express many different dynamic data structures, including all the ones named above as well as many more. We then describe how, given high-level specifications of such dynamic data structures, a compiler can emit code to efficiently access and compute on dynamic sparse tensors that are stored in the aforementioned data structures. We evaluate our technique and find it generates efficient dynamic sparse tensor algebra kernels that have performance comparable to, if not better than, state-of-the-art libraries and frameworks such as PAM, Aspen, STINGER, and Terrace. At the same time, our technique supports a wider range of tensor algebra operations---such as those that simultaneously compute with static and dynamic sparse tensors---than Aspen, STINGER, and Terrace, while also achieving significantly better performance than PAM for those same operations.},
	number = {OOPSLA2},
	urldate = {2023-03-20},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Chou, Stephen and Amarasinghe, Saman},
	month = oct,
	year = {2022},
	keywords = {dynamic sparse tensors, node schema language, pointer-based data structures, sparse tensor algebra, sparse tensor algebra compilation, sparse tensor formats},
	pages = {175:1408--175:1437},
	file = {Full Text PDF:/Users/willow/Zotero/storage/P4HH5Y9P/Chou and Amarasinghe - 2022 - Compilation of dynamic sparse tensor algebra.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/XKYWZTUW/Chou and Amarasinghe - 2022 - Compilation of dynamic sparse tensor algebra.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/8XBAIHZQ/Chou and Amarasinghe - 2022 - Compilation of dynamic sparse tensor algebra.pdf:application/pdf},
}

@misc{kovach_correct_2022,
	title = {Correct {Compilation} of {Semiring} {Contractions}},
	url = {http://arxiv.org/abs/2207.13291},
	doi = {10.48550/arXiv.2207.13291},
	abstract = {We introduce a formal operational semantics that describes the fused execution of variable contraction problems, which compute indexed arithmetic over a semiring and generalize sparse and dense tensor algebra, relational algebra, and graph algorithms. We prove that the model is correct with respect to a functional semantics. We also develop a compiler for variable contraction expressions and show that its performance is equivalent to a state-of-the art sparse tensor algebra compiler, while providing greater generality and correctness guarantees.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Kovach, Scott and Kjolstad, Fredrik},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13291 [cs]},
	keywords = {Computer Science - Programming Languages, F.3.1, F.3.2},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/92ZNYZA9/Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/X9EPYKM8/2207.html:text/html;Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:/Users/willow/Zotero/storage/M36DVD8B/Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:application/pdf},
}

@article{schleich_optimizing_2022,
	title = {Optimizing {Tensor} {Programs} on {Flexible} {Storage}},
	url = {http://arxiv.org/abs/2210.06267},
	doi = {10.48550/arXiv.2210.06267},
	abstract = {Tensor programs often need to process large tensors (vectors, matrices, or higher order tensors) that require a specialized storage format for their memory layout. Several such layouts have been proposed in the literature, such as the Coordinate Format, the Compressed Sparse Row format, and many others, that were especially designed to optimally store tensors with specific sparsity properties. However, existing tensor processing systems require specialized extensions in order to take advantage of every new storage format. In this paper we describe a system that allows users to define flexible storage formats in a declarative tensor query language, similar to the language used by the tensor program. The programmer only needs to write storage mappings, which describe, in a declarative way, how the tensors are laid out in main memory. Then, we describe a cost-based optimizer that optimizes the tensor program for the specific memory layout. We demonstrate empirically significant performance improvements compared to state-of-the-art tensor processing systems.},
	urldate = {2023-01-25},
	author = {Schleich, Maximilian and Shaikhha, Amir and Suciu, Dan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06267 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/2FW8LIU6/Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/VJNPTS3W/2210.html:text/html;Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:/Users/willow/Zotero/storage/NPQFGVEG/Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:application/pdf},
}

@incollection{grossman_5_1989,
	title = {5. {FIDIL}: {A} {Language} for {Scientific} {Programming}},
	isbn = {978-0-89871-239-1 978-1-61197-103-3},
	shorttitle = {5. {FIDIL}},
	url = {http://epubs.siam.org/doi/10.1137/1.9781611971033.ch5},
	abstract = {FIDIL is a new programming language for scientific computation. In this paper, we give a. brief overview of the language, largely consisting of several extended examples from computational fluid dynamics.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Symbolic {Computation}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Hilfinger, Paul N. and Colella, Philip},
	editor = {Grossman, Robert},
	month = jan,
	year = {1989},
	doi = {10.1137/1.9781611971033.ch5},
	pages = {97--138},
	file = {Hilfinger and Colella - 1989 - 5. FIDIL A Language for Scientific Programming.pdf:/Users/willow/Zotero/storage/4ESGBJZJ/Hilfinger and Colella - 1989 - 5. FIDIL A Language for Scientific Programming.pdf:application/pdf},
}

@article{baden_lattice_1993,
	title = {Lattice parallelism: a parallel programming model for manipulating non-uniform structured scientific data structures},
	volume = {28},
	issn = {0362-1340},
	shorttitle = {Lattice parallelism},
	url = {https://doi.org/10.1145/156668.156679},
	doi = {10.1145/156668.156679},
	number = {1},
	urldate = {2022-10-26},
	journal = {ACM SIGPLAN Notices},
	author = {Baden, Scott B. and Kohn, Scott R.},
	month = jan,
	year = {1993},
	pages = {24--27},
	file = {Full Text PDF:/Users/willow/Zotero/storage/EBM3HU8C/Baden and Kohn - 1993 - Lattice parallelism a parallel programming model .pdf:application/pdf},
}

@misc{hsu_sparse_2022,
	title = {The {Sparse} {Abstract} {Machine}},
	url = {http://arxiv.org/abs/2208.14610},
	doi = {10.48550/arXiv.2208.14610},
	abstract = {We propose the Sparse Abstract Machine (SAM), an intermediate representation and abstract machine model for targeting sparse tensor algebra to reconfigurable and fixed-function spatial dataflow accelerators. SAM defines a streaming abstraction with sparse primitives that encompass a large space of scheduled tensor algebra expressions. SAM dataflow graphs naturally separate tensor formats from algorithms and is expressive enough to incorporate many sparse-iteration and hardware-specific optimizations. We show an automatic compilation technique from a high-level language to SAM and a set of hardware primitives which implement it. We evaluate the generality and extensibility of our sparse abstract machine, explore the performance space of sparse tensor algebra optimizations using SAM, and provide an example implementation of our SAM architecture.},
	urldate = {2022-09-07},
	publisher = {arXiv},
	author = {Hsu, Olivia and Strange, Maxwell and Won, Jaeyeon and Sharma, Ritvik and Olukotun, Kunle and Emer, Joel and Horowitz, Mark and Kjolstad, Fredrik},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14610 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/PRGGWMYR/Hsu et al. - 2022 - The Sparse Abstract Machine.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/JTEDVSVC/2208.html:text/html},
}

@inproceedings{thies_manipulating_2009,
	address = {New York, NY, USA},
	series = {{MM} '09},
	title = {Manipulating lossless video in the compressed domain},
	isbn = {978-1-60558-608-3},
	url = {https://doi.org/10.1145/1631272.1631319},
	doi = {10.1145/1631272.1631319},
	abstract = {A compressed-domain transformation is one that operates directly on the compressed format, rather than requiring conversion to an uncompressed format prior to processing. Performing operations in the compressed domain offers large speedups, as it reduces the volume of data processed and avoids the overhead of re-compression. While previous researchers have focused on compressed-domain techniques for lossy data formats, there are few techniques that apply to lossless formats. In this paper, we present a general technique for transforming lossless data as compressed with the sliding-window Lempel Ziv algorithm (LZ77). We focus on applications in video editing, where our technique supports color adjustment, video compositing, and other operations directly on the Apple Animation format (a variant of LZ77). We implemented a subset of our technique as an automatic program transformation. Using the StreamIt language, users write a program to operate on uncompressed data, and our compiler transforms the program to operate on compressed data. Experiments show that the technique offers speedups roughly proportional to the compression factor. For our benchmark suite of 12 videos in Apple Animation format, speedups range from 1.1x to 471x, with a median of 15x.},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 17th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Thies, William and Hall, Steven and Amarasinghe, Saman},
	month = oct,
	year = {2009},
	keywords = {compressed domain, Lempel-Ziv, lossless compression, LZ77, stream programming, streamit, synchronous dataflow, video editing},
	pages = {331--340},
	file = {Full Text PDF:/Users/willow/Zotero/storage/RBR3QRBI/Thies et al. - 2009 - Manipulating lossless video in the compressed doma.pdf:application/pdf},
}

@article{franchetti_spiral_2018,
	title = {{SPIRAL}: {Extreme} {Performance} {Portability}},
	volume = {106},
	issn = {0018-9219, 1558-2256},
	shorttitle = {{SPIRAL}},
	url = {https://ieeexplore.ieee.org/document/8510983/},
	doi = {10.1109/JPROC.2018.2873289},
	language = {en},
	number = {11},
	urldate = {2022-09-01},
	journal = {Proceedings of the IEEE},
	author = {Franchetti, Franz and Low, Tze Meng and Popovici, Doru Thom and Veras, Richard M. and Spampinato, Daniele G. and Johnson, Jeremy R. and Puschel, Markus and Hoe, James C. and Moura, Jose M. F.},
	month = nov,
	year = {2018},
	pages = {1935--1968},
	file = {Franchetti et al. - 2018 - SPIRAL Extreme Performance Portability.pdf:/Users/willow/Zotero/storage/47XFJ3UU/Franchetti et al. - 2018 - SPIRAL Extreme Performance Portability.pdf:application/pdf},
}

@inproceedings{pizzuti_generating_2021,
	address = {New York, NY, USA},
	series = {{FHPNC} 2021},
	title = {Generating high performance code for irregular data structures using dependent types},
	isbn = {978-1-4503-8614-2},
	url = {https://doi.org/10.1145/3471873.3472977},
	doi = {10.1145/3471873.3472977},
	abstract = {Parallel architectures offer high performance but are challenging to program. Data parallel functional languages offer a solution by providing a high-level programming model to work with accelerators such as GPUs. Existing languages are designed to work with dense arrays, limiting their usefulness in expressing irregular data structures, such as graphs and sparse matrices important in many application domains. This paper addresses this limitation by extending a data-parallel language with limited dependent types, including position dependent arrays and dependent pairs to model irregular data structures. The approach is demonstrated through three case studies: dense to sparse matrix conversion, sparse matrix-vector multiplication, and parallel breadth-first search. Experimental results show that this approach outperforms state-of-the-art implementations on GPUs. Compared to Nvidia’s cuSparse, our automatically generated code achieves an average speedup of 1.2× for dense to sparse matrix conversion and 1.3× for sparse matrix-vector multiplication.},
	urldate = {2022-09-01},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN} {International} {Workshop} on {Functional} {High}-{Performance} and {Numerical} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Pizzuti, Federico and Steuwer, Michel and Dubach, Christophe},
	month = aug,
	year = {2021},
	keywords = {Dependent Types, Irregular Data Structures},
	pages = {37--49},
	file = {Full Text PDF:/Users/willow/Zotero/storage/9WPP87JG/Pizzuti et al. - 2021 - Generating high performance code for irregular dat.pdf:application/pdf},
}

@inproceedings{ikarashi_exocompilation_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Exocompilation for productive programming of hardware accelerators},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523446},
	doi = {10.1145/3519939.3523446},
	abstract = {High-performance kernel libraries are critical to exploiting accelerators and specialized instructions in many applications. Because compilers are difficult to extend to support diverse and rapidly-evolving hardware targets, and automatic optimization is often insufficient to guarantee state-of-the-art performance, these libraries are commonly still coded and optimized by hand, at great expense, in low-level C and assembly. To better support development of high-performance libraries for specialized hardware, we propose a new programming language, Exo, based on the principle of exocompilation: externalizing target-specific code generation support and optimization policies to user-level code. Exo allows custom hardware instructions, specialized memories, and accelerator configuration state to be defined in user libraries. It builds on the idea of user scheduling to externalize hardware mapping and optimization decisions. Schedules are defined as composable rewrites within the language, and we develop a set of effect analyses which guarantee program equivalence and memory safety through these transformations. We show that Exo enables rapid development of state-of-the-art matrix-matrix multiply and convolutional neural network kernels, for both an embedded neural accelerator and x86 with AVX-512 extensions, in a few dozen lines of code each.},
	urldate = {2022-09-01},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ikarashi, Yuka and Bernstein, Gilbert Louis and Reinking, Alex and Genc, Hasan and Ragan-Kelley, Jonathan},
	month = jun,
	year = {2022},
	keywords = {hardware accelerators, instruction abstraction, program optimization, scheduling, user-extensible backend \& scheduling, user-schedulable languages},
	pages = {703--718},
	file = {Full Text PDF:/Users/willow/Zotero/storage/96X6S4EG/Ikarashi et al. - 2022 - Exocompilation for productive programming of hardw.pdf:application/pdf},
}

@article{liu_verified_2022,
	title = {Verified tensor-program optimization via high-level scheduling rewrites},
	volume = {6},
	url = {https://doi.org/10.1145/3498717},
	doi = {10.1145/3498717},
	abstract = {We present a lightweight Coq framework for optimizing tensor kernels written in a pure, functional array language. Optimizations rely on user scheduling using series of verified, semantics-preserving rewrites. Unusually for compilation targeting imperative code with arrays and nested loops, all rewrites are source-to-source within a purely functional language. Our language comprises a set of core constructs for expressing high-level computation detail and a set of what we call reshape operators, which can be derived from core constructs but trigger low-level decisions about storage patterns and ordering. We demonstrate that not only is this system capable of deriving the optimizations of existing state-of-the-art languages like Halide and generating comparably performant code, it is also able to schedule a family of useful program transformations beyond what is reachable in Halide.},
	number = {POPL},
	urldate = {2022-09-01},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Liu, Amanda and Bernstein, Gilbert Louis and Chlipala, Adam and Ragan-Kelley, Jonathan},
	month = jan,
	year = {2022},
	keywords = {array programming, formal verification, optimization, proof assistants},
	pages = {55:1--55:28},
	file = {Full Text PDF:/Users/willow/Zotero/storage/NHAZS36S/Liu et al. - 2022 - Verified tensor-program optimization via high-leve.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/AKFB743Y/Liu et al. - 2022 - Verified tensor-program optimization via high-leve.pdf:application/pdf},
}

@inproceedings{kepner_mathematical_2016,
	title = {Mathematical foundations of the {GraphBLAS}},
	doi = {10.1109/HPEC.2016.7761646},
	abstract = {The GraphBLAS standard (GraphBlas.org) is being developed to bring the potential of matrix-based graph algorithms to the broadest possible audience. Mathematically, the GraphBLAS defines a core set of matrix-based graph operations that can be used to implement a wide class of graph algorithms in a wide range of programming environments. This paper provides an introduction to the mathematics of the GraphBLAS. Graphs represent connections between vertices with edges. Matrices can represent a wide range of graphs using adjacency matrices or incidence matrices. Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data. Fortunately, the two are easily connected by matrix multiplication. A key feature of matrix mathematics is that a very small number of matrix operations can be used to manipulate a very wide range of graphs. This composability of a small number of operations is the foundation of the GraphBLAS. A standard such as the GraphBLAS can only be effective if it has low performance overhead. Performance measurements of prototype GraphBLAS implementations indicate that the overhead is low.},
	booktitle = {2016 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Kepner, Jeremy and Aaltonen, Peter and Bader, David and Buluç, Aydin and Franchetti, Franz and Gilbert, John and Hutchison, Dylan and Kumar, Manoj and Lumsdaine, Andrew and Meyerhenke, Henning and McMillan, Scott and Yang, Carl and Owens, John D. and Zalewski, Marcin and Mattson, Timothy and Moreira, Jose},
	month = sep,
	year = {2016},
	keywords = {Additives, Finite element analysis, Matrices, Sparse matrices, Standards},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/8BLBSN6G/7761646.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/R63FRNWW/Kepner et al. - 2016 - Mathematical foundations of the GraphBLAS.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Machine learning, Neural networks, Feature extraction, Character recognition, Hidden Markov models, Multi-layer neural network, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, MNIST},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRS5H7PF/726791.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/V69R5ZYM/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	url = {https://www.science.org/doi/full/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	number = {6266},
	urldate = {2022-08-31},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	month = dec,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {Omniglot},
	pages = {1332--1338},
	file = {Full Text PDF:/Users/willow/Zotero/storage/IU83TXIK/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:application/pdf},
}

@article{eitz_how_2012,
	title = {How do humans sketch objects?},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2185520.2185540},
	doi = {10.1145/2185520.2185540},
	abstract = {Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73\% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56\% accuracy (chance is 0.4\%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.},
	number = {4},
	urldate = {2022-08-31},
	journal = {ACM Transactions on Graphics},
	author = {Eitz, Mathias and Hays, James and Alexa, Marc},
	month = jul,
	year = {2012},
	keywords = {crowd-sourcing, learning, recognition, sketch},
	pages = {44:1--44:10},
	file = {Full Text PDF:/Users/willow/Zotero/storage/VNICQZRI/Eitz et al. - 2012 - How do humans sketch objects.pdf:application/pdf},
}

@misc{ahrens_optimal_2021,
	title = {On {Optimal} {Partitioning} {For} {Sparse} {Matrices} {In} {Variable} {Block} {Row} {Format}},
	url = {http://arxiv.org/abs/2005.12414},
	doi = {10.48550/arXiv.2005.12414},
	abstract = {The Variable Block Row (VBR) format is an influential blocked sparse matrix format designed for matrices with shared sparsity structure between adjacent rows and columns. VBR groups adjacent rows and columns, storing the resulting blocks that contain nonzeros in a dense format. This reduces the memory footprint and enables optimizations such as register blocking and instruction-level parallelism. Existing approaches use heuristics to determine which rows and columns should be grouped together. We show that finding the optimal grouping of rows and columns for VBR is NP-hard under several reasonable cost models. In light of this finding, we propose a 1-dimensional variant of VBR, called 1D-VBR, which achieves better performance than VBR by only grouping rows. We describe detailed cost models for runtime and memory consumption. Then, we describe a linear time dynamic programming solution for optimally grouping the rows for 1D-VBR format. We extend our algorithm to produce a heuristic VBR partitioner which alternates between optimally partitioning rows and columns, assuming the columns or rows to be fixed, respectively. Our alternating heuristic produces VBR matrices with the smallest memory footprint of any partitioner we tested.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Ahrens, Willow and Boman, Erik G.},
	month = may,
	year = {2021},
	note = {arXiv:2005.12414 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {Ahrens and Boman - 2020 - On Optimal Partitioning For Sparse Matrices In Var.pdf:/Users/willow/Zotero/storage/RGQGCZKD/Ahrens and Boman - 2020 - On Optimal Partitioning For Sparse Matrices In Var.pdf:application/pdf;arXiv Fulltext PDF:/Users/willow/Zotero/storage/BWHWXSNS/Ahrens and Boman - 2021 - On Optimal Partitioning For Sparse Matrices In Var.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/E63KTTB9/2005.html:text/html},
}

@inproceedings{backus_fortran_1957,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '57 ({Western})},
	title = {The {FORTRAN} automatic coding system},
	isbn = {978-1-4503-7861-1},
	url = {https://doi.org/10.1145/1455567.1455599},
	doi = {10.1145/1455567.1455599},
	abstract = {The FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM's next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan. The goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.},
	urldate = {2022-08-04},
	booktitle = {Papers presented at the {February} 26-28, 1957, western joint computer conference: {Techniques} for reliability},
	publisher = {Association for Computing Machinery},
	author = {Backus, J. W. and Beeber, R. J. and Best, S. and Goldberg, R. and Haibt, L. M. and Herrick, H. L. and Nelson, R. A. and Sayre, D. and Sheridan, P. B. and Stern, H. and Ziller, I. and Hughes, R. A. and Nutt, R.},
	month = feb,
	year = {1957},
	pages = {188--198},
	file = {Full Text PDF:/Users/willow/Zotero/storage/SUHHBWFU/Backus et al. - 1957 - The FORTRAN automatic coding system.pdf:application/pdf},
}

@misc{ye_sparsetir_2022,
	title = {{SparseTIR}: {Composable} {Abstractions} for {Sparse} {Compilation} in {Deep} {Learning}},
	shorttitle = {{SparseTIR}},
	url = {http://arxiv.org/abs/2207.04606},
	doi = {10.48550/arXiv.2207.04606},
	abstract = {Sparse tensors are rapidly becoming critical components of modern deep learning workloads. However, developing high-performance sparse operators can be difficult and tedious, and existing vendor libraries cannot satisfy the escalating demands from new operators. Sparse tensor compilers simplify the development of operators, but efficient sparse compilation for deep learning remains challenging because a single sparse format cannot maximize hardware efficiency, and single-shot compilers cannot keep up with latest hardware and system advances. We show that the key to addressing both challenges is two forms of composability. In this paper, we propose SparseTIR, a sparse tensor compilation abstraction that offers composable formats and composable transformations for deep learning workloads. SparseTIR constructs a search space over these composable components for performance tuning. With these improvements, SparseTIR obtains consistent performance speedups vs vendor libraries on GPUs for single operators: 1.1-3.3x for GNN operators and 1.1-4.4x for sparse transformer operators. SparseTIR also accelerates end-to-end GNNs by 1.1-2.2x for GraphSAGE training and 0.9-26x for RGCN inference.},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Ye, Zihao and Lai, Ruihang and Shao, Junru and Chen, Tianqi and Ceze, Luis},
	month = jul,
	year = {2022},
	note = {arXiv:2207.04606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/7FRG42DC/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf;arXiv Fulltext PDF:/Users/willow/Zotero/storage/LUXDW9YS/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/GP857R84/2207.html:text/html;Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:/Users/willow/Zotero/storage/UE9AVP2B/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf},
}

@inproceedings{donenfeld_unified_2022,
	title = {Unified {Compilation} for {Lossless} {Compression} and {Sparse} {Computing}},
	doi = {10.1109/CGO53902.2022.9741282},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to support lossless compression techniques, including variants of run-length encoding and Lempel-Ziv compression. We develop new abstractions to represent losslessly compressed data as a generalized form of sparse tensors, with repetitions of values (which are compressed out in storage) represented by non-scalar, dynamic fill values. We then show how a compiler can use these abstractions to emit efficient code that computes on losslessly compressed data. By unifying lossless compression with sparse tensor algebra, our technique is able to generate code that computes with both losslessly compressed data and sparse data, as well as generate code that computes directly on compressed data without needing to first decompress it.Our evaluation shows our technique generates efficient image and video processing kernels that compute on losslessly compressed data. We find that the generated kernels are up to 16. 3× faster than equivalent dense kernels generated by TACO, a tensor algebra compiler, and up to 16. 1× faster than OpenCV, a widely used image processing library.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Donenfeld, Daniel and Chou, Stephen and Amarasinghe, Saman},
	month = apr,
	year = {2022},
	keywords = {Libraries, Tensors, Computational efficiency, Kernel, Algebra, sparse tensor algebra, lossless compression, Codes, compressed domain processing, Image coding},
	pages = {205--216},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/7FMZUU5R/9741282.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/JLWQKCNS/Donenfeld et al. - 2022 - Unified Compilation for Lossless Compression and S.pdf:application/pdf},
}

@article{hagedorn_achieving_2020,
	title = {Achieving high-performance the functional way: a functional pearl on expressing high-performance optimizations as rewrite strategies},
	volume = {4},
	issn = {2475-1421},
	shorttitle = {Achieving high-performance the functional way},
	url = {https://dl.acm.org/doi/10.1145/3408974},
	doi = {10.1145/3408974},
	abstract = {In this functional pearl, we show how to employ functional programming techniques to solve this challenge with elegance. We present two functional languages that work together - each addressing a separate concern. RISE is a functional language for expressing computations using well known functional data-parallel patterns. ELEVATE is a functional language for describing optimization strategies. A high-level RISE program is transformed into a low-level form using optimization strategies written in ELEVATE. From the rewritten low-level program high-performance parallel code is automatically generated. In contrast to existing high-performance domain-specific systems with scheduling APIs, in our approach programmers are not restricted to a set of built-in operations and optimizations but freely define their own computational patterns in RISE and optimization strategies in ELEVATE in a composable and reusable way. We show how our holistic functional approach achieves competitive performance with the state-of-the-art imperative systems Halide and TVM. CCS Concepts: • Software and its engineering → Functional languages; Compilers; • Theory of computation → Rewrite systems.},
	language = {en},
	number = {ICFP},
	urldate = {2022-04-29},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Hagedorn, Bastian and Lenfers, Johannes and Kœhler, Thomas and Qin, Xueying and Gorlatch, Sergei and Steuwer, Michel},
	month = aug,
	year = {2020},
	pages = {1--29},
	file = {Hagedorn et al. - 2020 - Achieving high-performance the functional way a f.pdf:/Users/willow/Zotero/storage/E673JW72/Hagedorn et al. - 2020 - Achieving high-performance the functional way a f.pdf:application/pdf},
}

@article{psarras_landscape_2021,
	title = {The landscape of software for tensor computations},
	url = {http://arxiv.org/abs/2103.13756},
	abstract = {Tensors (also commonly seen as multi-linear operators or as multi-dimensional arrays) are ubiquitous in scientiﬁc computing and in data science, and so are the software eﬀorts for tensor operations. Particularly in recent years, we have observed an explosion in libraries, compilers, packages, and toolboxes; unfortunately these eﬀorts are very much scattered among the diﬀerent scientiﬁc domains, and inevitably suﬀer from replication, suboptimal implementations, and in many cases, limited visibility. As a ﬁrst step towards countering these ineﬃciencies, here we survey and loosely classify software packages related to tensor computations. Our aim is to assemble a comprehensive and up-to-date snapshot of the tensor software landscape, with the intention of helping both users and developers. Aware of the diﬃculties inherent in any multi-discipline survey, we very much welcome the reader’s help in amending and expanding our software list, which currently features 77 projects.},
	language = {en},
	urldate = {2022-04-29},
	journal = {arXiv:2103.13756 [cs]},
	author = {Psarras, Christos and Karlsson, Lars and Li, Jiajia and Bientinesi, Paolo},
	month = may,
	year = {2021},
	note = {arXiv: 2103.13756},
	keywords = {Computer Science - Mathematical Software},
	file = {Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:/Users/willow/Zotero/storage/9FYH3IWU/Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:application/pdf},
}

@inproceedings{ahrens_autoscheduling_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Autoscheduling for sparse tensor algebra with an asymptotic cost model},
	copyright = {All rights reserved},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523442},
	doi = {10.1145/3519939.3523442},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small Pareto frontier of asymptotically non-dominating kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = jun,
	year = {2022},
	keywords = {Asymptotic Analysis, Automatic Scheduling, Compilers, Conjunctive Query Containment, Query Optimization, Sparse Tensors},
	pages = {269--285},
	file = {Full Text PDF:/Users/willow/Zotero/storage/V6PAKBZT/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/QDCN3ULM/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf},
}

@phdthesis{kjolstad_sparse_2020,
	type = {Thesis},
	title = {Sparse tensor algebra compilation},
	copyright = {MIT theses may be protected by copyright. Please reuse MIT thesis content according to the MIT Libraries Permissions Policy, which is available through the URL provided.},
	url = {https://dspace.mit.edu/handle/1721.1/128314},
	abstract = {This dissertation shows how to compile any sparse tensor algebra expression to CPU and GPU code that matches the performance of hand-optimized implementations. A tensor algebra expression is sparse if at least one of its tensor operands is sparse, and a tensor is sparse if most of its values are zero. If a tensor is sparse, then we can store its nonzero values in a compressed data structure, and omit the zeros. Indeed, as the matrices and tensors in many important applications are extremely sparse, compressed data structures provide the only practical means to store them. A sparse tensor algebra expression may contain any number of operations, which must be compiled to fused loops that compute the entire expression simultaneously. It is not viable to support only binary expressions, because their composition may result in worse asymptotic complexity than the fused implementation.},
	language = {eng},
	urldate = {2022-07-21},
	school = {Massachusetts Institute of Technology},
	author = {Kjølstad, Fredrik Berg},
	year = {2020},
	note = {Accepted: 2020-11-03T20:30:04Z
ISBN: 9781201259824},
	file = {Full Text PDF:/Users/willow/Zotero/storage/K5EHAPI7/Kjølstad - 2020 - Sparse tensor algebra compilation.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/YHLBFJYQ/128314.html:text/html},
}

@article{neumann_efficiently_2011,
	title = {Efficiently compiling efficient query plans for modern hardware},
	volume = {4},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2002938.2002940},
	doi = {10.14778/2002938.2002940},
	abstract = {As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and exible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mispredictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans. In this work we present a novel compilation strategy that translates a query into compact and efficient machine code using the LLVM compiler framework. By aiming at good code and data locality and predictable branch layout the resulting code frequently rivals the performance of hand-written C++ code. We integrated these techniques into the HyPer main memory database system and show that this results in excellent query performance while requiring only modest compilation time.},
	number = {9},
	urldate = {2022-06-27},
	journal = {Proceedings of the VLDB Endowment},
	author = {Neumann, Thomas},
	month = jun,
	year = {2011},
	pages = {539--550},
	file = {Full Text PDF:/Users/willow/Zotero/storage/W9JDWCAF/Neumann - 2011 - Efficiently compiling efficient query plans for mo.pdf:application/pdf},
}

@article{tang_torchsparse_2022,
	title = {{TorchSparse}: {Efficient} {Point} {Cloud} {Inference} {Engine}},
	shorttitle = {{TorchSparse}},
	url = {http://arxiv.org/abs/2204.10319},
	abstract = {Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efﬁciently on the general-purpose hardware. Furthermore, existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It applies adaptive matrix multiplication grouping to trade computation for better regularity, achieving 1.4-1.5× speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7×. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6× and 1.5× measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.},
	language = {en},
	urldate = {2022-04-28},
	journal = {arXiv:2204.10319 [cs]},
	author = {Tang, Haotian and Liu, Zhijian and Li, Xiuyu and Lin, Yujun and Han, Song},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.10319},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Performance},
	file = {Tang et al. - 2022 - TorchSparse Efficient Point Cloud Inference Engin.pdf:/Users/willow/Zotero/storage/R5ST95AE/Tang et al. - 2022 - TorchSparse Efficient Point Cloud Inference Engin.pdf:application/pdf},
}

@inproceedings{shi_column_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Column {Partition} and {Permutation} for {Run} {Length} {Encoding} in {Columnar} {Databases}},
	isbn = {978-1-4503-6735-6},
	url = {http://doi.org/10.1145/3318464.3384413},
	doi = {10.1145/3318464.3384413},
	abstract = {Effective compression is essential when databases are used in Big Data applications. For in-memory columnar databases, compression can help to load the columns faster and speed up query evaluation. In this paper, we consider compressing columns using the Run Length Encoding (RLE). This algorithm encodes each region with identical value using a single run. The question we study in this paper is 'how to rearrange table columns for better compression?' We observe that not every column of a table benefits from column compression in an ideal column arrangement. Because finding the optimal column arrangement is NP-hard, we propose an incremental heuristic that identifies the set of columns to be compressed and the order of rows that offer a better compression ratio. Our preliminary experiments confirm that our algorithm improves the compression rate by up to 25\% on test data, compared with compressing all columns of a table.},
	urldate = {2020-12-09},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Jia},
	month = jun,
	year = {2020},
	keywords = {columnar database, data compression},
	pages = {2873--2874},
	file = {Full Text PDF:/Users/willow/Zotero/storage/R4VQX8Q6/Shi - 2020 - Column Partition and Permutation for Run Length En.pdf:application/pdf},
}

@article{hu_taichi_2019,
	title = {Taichi: a language for high-performance computation on spatially sparse data structures},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {Taichi},
	url = {https://doi.org/10.1145/3355089.3356506},
	doi = {10.1145/3355089.3356506},
	abstract = {3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multi-level sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose Taichi, a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This decoupling of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With 1{\textless}u{\textgreater}1{\textless}/u{\textgreater}0 th as many lines of code, we achieve 4.55× higher performance on average, compared to hand-optimized reference implementations.},
	number = {6},
	urldate = {2020-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Hu, Yuanming and Li, Tzu-Mao and Anderson, Luke and Ragan-Kelley, Jonathan and Durand, Frédo},
	month = nov,
	year = {2019},
	keywords = {GPU computing, sparse data structures},
	pages = {201:1--201:16},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LI4HK8PK/Hu et al. - 2019 - Taichi a language for high-performance computatio.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Number: 7825
Publisher: Nature Publishing Group},
	pages = {357--362},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZFNZYPK/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/IFYNZTWL/s41586-020-2649-2.html:text/html},
}

@article{langr_evaluation_2016,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	volume = {27},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2015.2401575},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langr, Daniel and Tvrdík, Pavel},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {sparse matrix-vector multiplication, Sparse matrices, Runtime, parallel processing, Indexes, matrix-vector multiplication, sparse matrix, Matrix converters, mathematics computing, test matrices, Standards, vectors, storage format, matrix algebra, Memory management, evaluation criteria, Evaluation criterion, FLOPS, HPC application, memory footprint, nonzero matrix structure, single evaluation criterion, sparse matrix storage formats, SpMV-based application phase},
	pages = {428--440},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRZMJ4TJ/7036061.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/D75SXF9Z/7036061.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/8IRGEGDG/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/I3FNKNCF/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf},
}

@article{senanayake_sparse_2020,
	title = {A sparse iteration space transformation framework for sparse tensor algebra},
	volume = {4},
	url = {https://doi.org/10.1145/3428226},
	doi = {10.1145/3428226},
	abstract = {We address the problem of optimizing sparse tensor algebra in a compiler and show how to define standard loop transformations---split, collapse, and reorder---on sparse iteration spaces. The key idea is to track the transformation functions that map the original iteration space to derived iteration spaces. These functions are needed by the code generator to emit code that maps coordinates between iteration spaces at runtime, since the coordinates in the sparse data structures remain in the original iteration space. We further demonstrate that derived iteration spaces can tile both the universe of coordinates and the subset of nonzero coordinates: the former is analogous to tiling dense iteration spaces, while the latter tiles sparse iteration spaces into statically load-balanced blocks of nonzeros. Tiling the space of nonzeros lets the generated code efficiently exploit heterogeneous compute resources such as threads, vector units, and GPUs. We implement these concepts by extending the sparse iteration theory implementation in the TACO system. The associated scheduling API can be used by performance engineers or it can be the target of an automatic scheduling system. We outline one heuristic autoscheduling system, but other systems are possible. Using the scheduling API, we show how to optimize mixed sparse-dense tensor algebra expressions on CPUs and GPUs. Our results show that the sparse transformations are sufficient to generate code with competitive performance to hand-optimized implementations from the literature, while generalizing to all of the tensor algebra.},
	number = {OOPSLA},
	urldate = {2021-11-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Senanayake, Ryan and Hong, Changwan and Wang, Ziheng and Wilson, Amalee and Chou, Stephen and Kamil, Shoaib and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = nov,
	year = {2020},
	keywords = {Optimizing Transformations, Sparse Iteration Spaces, Sparse Tensor Algebra},
	pages = {158:1--158:30},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6WCKXU8K/Senanayake et al. - 2020 - A sparse iteration space transformation framework .pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/6JTA6NI8/Senanayake et al. - 2020 - A sparse iteration space transformation framework .pdf:application/pdf},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	url = {https://doi.org/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
	number = {OOPSLA},
	urldate = {2021-11-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	keywords = {Compilation, Sparse Array Programming, Sparse Arrays},
	pages = {128:1--128:29},
	file = {Full Text PDF:/Users/willow/Zotero/storage/724NU8UX/Henry et al. - 2021 - Compilation of sparse array programming models.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/KJRC8JFG/Henry et al. - 2021 - Compilation of sparse array programming models.pdf:application/pdf},
}

@phdthesis{kotlyar_relational_1999,
	type = {{PhD} {Thesis}},
	title = {Relational {Algebraic} {Techniques} for the {Synthesis} of {Sparse} {Matrix} {Programs}},
	school = {Cornell},
	author = {Kotlyar, Vladimir},
	year = {1999},
	note = {00000},
}

@techreport{kotlyar_compiling_1997,
	title = {Compiling parallel sparse code for user-defined data structures},
	institution = {Cornell},
	author = {Kotlyar, Vladimir and Pingali, Keshav and Stodghill, Paul},
	year = {1997},
	note = {00000},
}

@inproceedings{liu_csr5_2015,
	address = {New York, NY, USA},
	series = {{ICS} '15},
	title = {{CSR5}: {An} {Efficient} {Storage} {Format} for {Cross}-{Platform} {Sparse} {Matrix}-{Vector} {Multiplication}},
	isbn = {978-1-4503-3559-1},
	shorttitle = {{CSR5}},
	url = {https://doi.org/10.1145/2751205.2751209},
	doi = {10.1145/2751205.2751209},
	abstract = {Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations. We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6\%, 28.5\%, 173.0\% and 293.3\% (up to 213.3\%, 153.6\%, 405.1\% and 943.3\%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the 29th {ACM} on {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Weifeng and Vinter, Brian},
	month = jun,
	year = {2015},
	keywords = {sparse matrices, storage formats, gpu, spmv, cpu, csr, csr5, xeon phi},
	pages = {339--350},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6PDCWARD/Liu and Vinter - 2015 - CSR5 An Efficient Storage Format for Cross-Platfo.pdf:application/pdf},
}

@article{vuduc_oski:_2005,
	title = {{OSKI}: {A} library of automatically tuned sparse matrix kernels},
	volume = {16},
	issn = {1742-6596},
	shorttitle = {{OSKI}},
	url = {https://doi.org/10.1088/1742-6596/16/1/071},
	doi = {10.1088/1742-6596/16/1/071},
	abstract = {The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines.},
	language = {en},
	urldate = {2021-09-10},
	journal = {Journal of Physics: Conference Series},
	author = {Vuduc, Richard and Demmel, James W. and Yelick, Katherine A.},
	month = jan,
	year = {2005},
	note = {Publisher: IOP Publishing},
	pages = {521--530},
	file = {IOP Full Text PDF:/Users/willow/Zotero/storage/2PMSY3TR/Vuduc et al. - 2005 - OSKI A library of automatically tuned sparse matr.pdf:application/pdf;Vuduc et al. - 2005 - OSKI A library of automatically tuned sparse matr.pdf:/Users/willow/Zotero/storage/3B6HP6WP/Vuduc et al. - 2005 - OSKI A library of automatically tuned sparse matr.pdf:application/pdf},
}

@article{kjolstad_tensor_2017,
	title = {The {Tensor} {Algebra} {Compiler}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2019-11-20},
	journal = {Proc. ACM Program. Lang.},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {parallelism, code generation, linear algebra, tensors, performance, sparse data structures, iteration graphs, merge lattices, tensor algebra},
	pages = {77:1--77:29},
	file = {ACM Full Text PDF:/Users/willow/Zotero/storage/M3YTNCUI/Kjolstad et al. - 2017 - The Tensor Algebra Compiler.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/EZXEWXYJ/Kjolstad et al. - 2017 - The tensor algebra compiler.pdf:application/pdf;kjolstad-oopsla17-taco-preprint.pdf:/Users/willow/Zotero/storage/XUPL86I6/kjolstad-oopsla17-taco-preprint.pdf:application/pdf},
}

@inproceedings{mcauley_hidden_2013,
	address = {New York, NY, USA},
	series = {{RecSys} '13},
	title = {Hidden factors and hidden topics: understanding rating dimensions with review text},
	isbn = {978-1-4503-2409-0},
	shorttitle = {Hidden factors and hidden topics},
	url = {http://doi.org/10.1145/2507157.2507163},
	doi = {10.1145/2507157.2507163},
	abstract = {In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wizards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we obtain highly interpretable textual labels for latent rating dimensions, which helps us to `justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the information present in review text; this is especially true for new products and users, who may have too few ratings to model their latent factors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 7th {ACM} conference on {Recommender} systems},
	publisher = {Association for Computing Machinery},
	author = {McAuley, Julian and Leskovec, Jure},
	month = oct,
	year = {2013},
	keywords = {recommender systems, topic models},
	pages = {165--172},
	file = {Full Text PDF:/Users/willow/Zotero/storage/HCJPCSHG/McAuley and Leskovec - 2013 - Hidden factors and hidden topics understanding ra.pdf:application/pdf;McAuley and Leskovec - 2013 - Hidden factors and hidden topics understanding ra.pdf:/Users/willow/Zotero/storage/7DNUTEAF/McAuley and Leskovec - 2013 - Hidden factors and hidden topics understanding ra.pdf:application/pdf},
}

@inproceedings{vuduc_performance_2002,
	title = {Performance {Optimizations} and {Bounds} for {Sparse} {Matrix}-{Vector} {Multiply}},
	doi = {10.1109/SC.2002.10025},
	abstract = {We consider performance tuning, by code and data structure reorganization, of sparse matrix-vector multiply (SpM×V), one of the most important computational kernels in scientific applications. This paper addresses the fundamental questions of what limits exist on such performance tuning, and how closely tuned code approaches these limits. Specifically, we develop upper and lower bounds on the performance (Mflop/s) of SpM×V when tuned using our previously proposed register blocking optimization. These bounds are based on the non-zero pattern in the matrix and the cost of basic memory operations, such as cache hits and misses. We evaluate our tuned implementations with respect to these bounds using hardware counter data on 4 different platforms and on test set of 44 sparse matrices. We find that we can often get within 20\% of the upper bound, particularly on class of matrices from finite element modeling (FEM) problems; on non-FEM matrices, performance improvements of 2× are still possible. Lastly, we present new heuristic that selects optimal or near-optimal register block sizes (the key tuning parameters) more accurately than our previous heuristic. Using the new heuristic, we show improvements in SpM×V performance (Mflop/s) by as much as 2.5× over an untuned implementation. Collectively, our results suggest that future performance improvements, beyond those that we have already demonstrated for SpM×V, will come from two sources: (1) consideration of higher-level matrix structures (e.g. exploiting symmetry, matrix reordering, multiple register block sizes), and (2) optimizing kernels with more opportunity for data reuse (e.g. sparse matrix-multiple vector multiply, multiplication of AT A by a vector).},
	booktitle = {{SC} '02: {Proceedings} of the 2002 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Vuduc, R. and Demmel, J.W. and Yelick, K.A. and Kamil, S. and Nishtala, R. and Lee, B.},
	month = nov,
	year = {2002},
	note = {ISSN: 1063-9535},
	keywords = {Data structures, Hardware, Sparse matrices, Testing, Optimization, Upper bound, Kernel, Costs, Registers, Counting circuits},
	pages = {26--26},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/NB49QD9N/1592862.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/BLUY9Q7P/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf;Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:/Users/willow/Zotero/storage/EIBMBAFB/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf},
}

@inproceedings{ragan-kelley_halide_2013,
	address = {New York, NY, USA},
	series = {{PLDI} '13},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Halide},
	url = {http://doi.org/10.1145/2491956.2462176},
	doi = {10.1145/2491956.2462176},
	abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Frédo and Amarasinghe, Saman},
	month = jun,
	year = {2013},
	keywords = {autotuning, parallelism, gpu, optimization, vectorization, locality, compiler, image processing, domain specific language, redundant computation},
	pages = {519--530},
	file = {Full Text PDF:/Users/willow/Zotero/storage/QZF9XYFW/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@inproceedings{bik_automatic_1994,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On automatic data structure selection and code generation for sparse computations},
	isbn = {978-3-540-48308-3},
	doi = {10.1007/3-540-57659-2_4},
	abstract = {Traditionally restructuring compilers were only able to apply program transformations in order to exploit certain characteristics of the target architecture. Adaptation of data structures was limited to e.g. linearization or transposing of arrays. However, as more complex data structures are required to exploit characteristics of the data operated on, current compiler support appears to be inappropriate. In this paper we present the implementation issues of a restructuring compiler that automatically converts programs operating on dense matrices into sparse code, i.e. after a suited data structure has been selected for every dense matrix that in fact is sparse, the original code is adapted to operate on these data structures. This simplifies the task of the programmer and, in general, enables the compiler to apply more optimizations.},
	language = {en},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer},
	author = {Bik, Aart J. C. and Wijshoff, Harry A. G.},
	editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	year = {1994},
	keywords = {Restructuring Compilers, Sparse Computations, Sparse Matrices},
	pages = {57--75},
	file = {Full Text PDF:/Users/willow/Zotero/storage/N9BR5TBS/Bik and Wijshoff - 1994 - On automatic data structure selection and code gen.pdf:application/pdf},
}

@article{strout_sparse_2018,
	title = {The {Sparse} {Polyhedral} {Framework}: {Composing} {Compiler}-{Generated} {Inspector}-{Executor} {Code}},
	volume = {106},
	issn = {1558-2256},
	shorttitle = {The {Sparse} {Polyhedral} {Framework}},
	doi = {10.1109/JPROC.2018.2857721},
	abstract = {Irregular applications such as big graph analysis, material simulations, molecular dynamics simulations, and finite element analysis have performance problems due to their use of sparse data structures. Inspector-executor strategies improve sparse computation performance through parallelization and data locality optimizations. An inspector reschedules and reorders data at runtime, and an executor is a transformed version of the original computation that uses the newly reorganized schedules and data structures. Inspector-executor transformations are commonly written in a domain-specific or even application-specific fashion. Significant progress has been made in incorporating such inspector-executor transformations into existing compiler transformation frameworks, thus enabling their use with compile-time transformations. However, composing inspector-executor transformations in a general way has only been done in the context of the Sparse Polyhedral Framework (SPF). Though SPF enables the general composition of such transformations, the resulting inspector and executor performance suffers due to missed specialization opportunities. This paper reviews the history and current state of the art for inspector-executor strategies and reviews how the SPF enables the composition of inspector-executor transformations. Further, it describes a research vision to combine this generality in SPF with specialization to achieve composable and high performance inspectors and executors, producing a powerful compiler framework for sparse matrix computations.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Strout, Michelle Mills and Hall, Mary and Olschanowsky, Catherine},
	month = nov,
	year = {2018},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {compile-time transformations, compiler transformation frameworks, compiler-generated inspector-executor code, data structures, Dynamic compiler, High performance computing, high performance inspectors, inspector-executor strategies, inspector-executor transformations, Intermediate representations, irregular computations, Optimization, program compilers, program optimization and parallelization, Runtime, sparse data structures, sparse matrices, Sparse matrices, sparse polyhedral framework},
	pages = {1921--1934},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/WWPWE65Q/8436444.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/UUC788QU/8436444.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/B6IE99G7/8436444.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/9LIRZNXN/Strout et al. - 2018 - The Sparse Polyhedral Framework Composing Compile.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/A4APW5ZL/Strout et al. - 2018 - The Sparse Polyhedral Framework Composing Compile.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/JG2FNQLC/Strout et al. - 2018 - The Sparse Polyhedral Framework Composing Compile.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/CZMKV3KM/Strout et al. - 2018 - The Sparse Polyhedral Framework Composing Compile.pdf:application/pdf},
}

@inproceedings{bik_compilation_1993,
	address = {New York, NY, USA},
	series = {{ICS} '93},
	title = {Compilation techniques for sparse matrix computations},
	isbn = {978-0-89791-600-4},
	url = {http://doi.org/10.1145/165939.166023},
	doi = {10.1145/165939.166023},
	abstract = {The problem of compiler optimization of sparse codes is well known and no satisfactory solutions have been found yet. One of the major obstacles is formed by the fact that sparse programs deal explicitly with the particular data structures selected for storing sparse matrices. This explicit data structure handling obscures the functionality of a code to such a degree that the optimization of the code is prohibited, e.g. by the introduction of indirect addressing. The method presented in this paper postpones data structure selection until the compile phase, thereby allowing the compiler to combine code optimization with explicit data structure selection. Not only enables this method the compiler to generate efficient code for sparse computations, also the task of the programmer is greatly reduced in complexity.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 7th international conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Bik, Aart J. C. and Wijshoff, Harry A. G.},
	month = aug,
	year = {1993},
	keywords = {sparse matrices, optimization, program transformations, restructuring compilers, sparse computations, compilation techniques},
	pages = {416--424},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZCICPAR/Bik and Wijshoff - 1993 - Compilation techniques for sparse matrix computati.pdf:application/pdf},
}

@inproceedings{kotlyar_relational_1997,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A relational approach to the compilation of sparse matrix programs},
	isbn = {978-3-540-69549-3},
	doi = {10.1007/BFb0002751},
	abstract = {We present a relational algebra based framework for compiling efficient sparse matrix code from dense DO-ANY loops and a specification of the representation of the sparse matrix. We present experimental data that demonstrates that the code generated by our compiler achieves performance competitive with that of hand-written codes for important computational kernels.},
	language = {en},
	booktitle = {Euro-{Par}'97 {Parallel} {Processing}},
	publisher = {Springer},
	author = {Kotlyar, Vladimir and Pingali, Keshav and Stodghill, Paul},
	editor = {Lengauer, Christian and Griebl, Martin and Gorlatch, Sergei},
	year = {1997},
	keywords = {Loop Nest, Relational Query, Sparse Code, Sparse Matrix, Storage Format},
	pages = {318--327},
	file = {Springer Full Text PDF:/Users/willow/Zotero/storage/3JWGLXRU/Kotlyar et al. - 1997 - A relational approach to the compilation of sparse.pdf:application/pdf;Springer Full Text PDF:/Users/willow/Zotero/storage/QAKRP5KM/Kotlyar et al. - 1997 - A relational approach to the compilation of sparse.pdf:application/pdf},
}

@inproceedings{pugh_sipr_1999,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SIPR}: {A} {New} {Framework} for {Generating} {Efficient} {Code} for {Sparse} {Matrix} {Computations}},
	isbn = {978-3-540-48319-9},
	shorttitle = {{SIPR}},
	doi = {10.1007/3-540-48319-5_14},
	abstract = {Developing computational codes that compute with sparse matrices is a difficult and error-prone process. Automatic generation of sparse code from the corresponding dense version would simplify the programmer’s task, provided that a compiler-generated code is fast enough to be used instead of a hand-written code. We propose a new Sparse Intermediate Program Representation (SIPR) that separates the issue of maintaining complicated data structures from the actual matrix computations to be performed. Cost analysis of SIPR allows for the prediction of the program efficiency, and provides a solid basis for choosing efficient sparse implementations among many possible ones. The SIPR framework allows the use of techniques that are frequently used in the hand-written codes but previously were not considered for compiler-generated codes due to their complexity. We have developed tools that allow the automatic generation of efficient C++ implementations from SIPR, and describe experimental results on the performance of those implementations.},
	language = {en},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer},
	author = {Pugh, William and Shpeisman, Tatiana},
	editor = {Chatterjee, Siddhartha and Prins, Jan F. and Carter, Larry and Ferrante, Jeanne and Li, Zhiyuan and Sehr, David and Yew, Pen-Chung},
	year = {1999},
	keywords = {Array Reference, Element Store, Partial Pivoting, Sparse Code, Sparse Matrix},
	pages = {213--229},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FFJV8593/Pugh and Shpeisman - 1999 - SIPR A New Framework for Generating Efficient Cod.pdf:application/pdf;Springer Full Text PDF:/Users/willow/Zotero/storage/NJGMHHZE/Pugh and Shpeisman - 1999 - SIPR A New Framework for Generating Efficient Cod.pdf:application/pdf},
}

@article{chou_format_2018,
	title = {Format abstraction for sparse tensor algebra compilers},
	volume = {2},
	url = {https://doi.org/10.1145/3276493},
	doi = {10.1145/3276493},
	abstract = {This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code to compute any tensor algebra expression on any combination of the aforementioned formats. To demonstrate our technique, we have implemented it in the taco tensor algebra compiler. Our modular code generator design makes it simple to add support for new tensor formats, and the performance of the generated code is competitive with hand-optimized implementations. Furthermore, by extending taco to support a wider range of formats specialized for different application and data characteristics, we can improve end-user application performance. For example, if input data is provided in the COO format, our technique allows computing a single matrix-vector multiplication directly with the data in COO, which is up to 3.6× faster than by first converting the data to CSR.},
	number = {OOPSLA},
	urldate = {2021-11-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = oct,
	year = {2018},
	keywords = {modular code generation, sparse tensor algebra compilation, tensor formats},
	pages = {123:1--123:30},
	file = {Full Text PDF:/Users/willow/Zotero/storage/C37QWRCB/Chou et al. - 2018 - Format abstraction for sparse tensor algebra compi.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/9NF3UNWF/Chou et al. - 2018 - Format abstraction for sparse tensor algebra compi.pdf:application/pdf},
}

@inproceedings{baghdadi_tiramisu_2019,
	address = {Washington, DC, USA},
	series = {{CGO} 2019},
	title = {Tiramisu: a polyhedral compiler for expressing fast and portable code},
	isbn = {978-1-72811-436-1},
	shorttitle = {Tiramisu},
	abstract = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
	urldate = {2021-11-17},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {IEEE Press},
	author = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
	month = feb,
	year = {2019},
	keywords = {Code Generation, Code Optimization, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance, Computer Science - Programming Languages, Deep Learning, Distributed Systems, GPU, Polyhedral Model, Tensors},
	pages = {193--205},
	file = {arXiv.org Snapshot:/Users/willow/Zotero/storage/N55KDPP5/1804.html:text/html;Full Text PDF:/Users/willow/Zotero/storage/XR8KQDD2/Baghdadi et al. - 2019 - Tiramisu a polyhedral compiler for expressing fas.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	volume = {17},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	language = {en},
	number = {3},
	urldate = {2021-11-19},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = mar,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biophysical chemistry;Computational biology and bioinformatics;Technology
Subject\_term\_id: biophysical-chemistry;computational-biology-and-bioinformatics;technology},
	keywords = {Biophysical chemistry, Computational biology and bioinformatics, Technology},
	pages = {261--272},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2Q9W83VV/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/V7KB63J9/s41592-019-0686-2.html:text/html},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	urldate = {2021-11-19},
	booktitle = {12th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 16)},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Q4MBXVX8/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@inproceedings{williams_optimization_2007,
	address = {New York, NY, USA},
	series = {{SC} '07},
	title = {Optimization of sparse matrix-vector multiplication on emerging multicore platforms},
	isbn = {978-1-59593-764-3},
	url = {http://doi.org/10.1145/1362622.1362674},
	doi = {10.1145/1362622.1362674},
	abstract = {We are witnessing a dramatic change in computer architecture due to the multicore paradigm shift, as every electronic device from cell phones to supercomputers confronts parallelism of unprecedented scale. To fully unleash the potential of these systems, the HPC community must develop multicore specific optimization methodologies for important scientific computations. In this work, we examine sparse matrix-vector multiply (SpMV) - one of the most heavily used kernels in scientific computing - across a broad spectrum of multicore designs. Our experimental platform includes the homogeneous AMD dual-core and Intel quad-core designs, the heterogeneous STI Cell, as well as the first scientific study of the highly multithreaded Sun Niagara2. We present several optimization strategies especially effective for the multicore environment, and demonstrate significant performance improvements compared to existing state-of-the-art serial and parallel SpMV implementations. Additionally, we present key insights into the architectural tradeoffs of leading multicore design strategies, in the context of demanding memory-bound numerical algorithms.},
	urldate = {2022-03-15},
	booktitle = {Proceedings of the 2007 {ACM}/{IEEE} conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Williams, Samuel and Oliker, Leonid and Vuduc, Richard and Shalf, John and Yelick, Katherine and Demmel, James},
	month = nov,
	year = {2007},
	keywords = {Sparse matrices, Supercomputers, Kernel, Multicore processing, Parallel processing, Computer architecture, Scientific computing, Cellular phones, Optimization methods, Sun},
	pages = {1--12},
	file = {Full Text PDF:/Users/willow/Zotero/storage/G4VQIMPC/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/P7C29HUU/5348797.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/XVTJHNHK/5348797.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/MLDMCFE7/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf},
}

@inproceedings{solomonik_cyclops_2013,
	title = {Cyclops {Tensor} {Framework}: {Reducing} {Communication} and {Eliminating} {Load} {Imbalance} in {Massively} {Parallel} {Contractions}},
	shorttitle = {Cyclops {Tensor} {Framework}},
	doi = {10.1109/IPDPS.2013.112},
	abstract = {Cyclops (cyclic-operations) Tensor Framework (CTF) 1 is a distributed library for tensor contractions. CTF aims to scale high-dimensional tensor contractions such as those required in the Coupled Cluster (CC) electronic structure method to massively-parallel supercomputers. The framework preserves tensor structure by subdividing tensors cyclically, producing a regular parallel decomposition. An internal virtualization layer provides completely general mapping support while maintaining ideal load balance. The mapping framework decides on the best mapping for each tensor contraction at run-time via explicit calculations of memory usage and communication volume. CTF employs a general redistribution kernel, which transposes tensors of any dimension between arbitrary distributed layouts, yet touches each piece of data only once. Sequential symmetric contractions are reduced to matrix multiplication calls via tensor index transpositions and partial unpacking. The user-level interface elegantly expresses arbitrary-dimensional generalized tensor contractions in the form of a domain specific language. We demonstrate performance of CC with single and double excitations on 8192 nodes of Blue Gene/Q and show that CTF outperforms NWChem on Cray XE6 supercomputers for benchmarked systems.},
	booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
	author = {Solomonik, Edgar and Matthews, Devin and Hammond, Jeff and Demmel, James},
	month = may,
	year = {2013},
	note = {ISSN: 1530-2075},
	keywords = {Chemistry, Clustering algorithms, communication-avoiding algorithms, Coupled Cluster, Cyclops, Equations, Indexes, Manganese, Program processors, Tensile stress, tensor contractions},
	pages = {813--824},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/F5CY8QZN/6569864.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/X25BVLFH/Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:application/pdf;Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:/Users/willow/Zotero/storage/9LFL6LHL/Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:application/pdf},
}

@article{zhou_enabling_2020,
	title = {Enabling {Runtime} {SpMV} {Format} {Selection} through an {Overhead} {Conscious} {Method}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2932931},
	abstract = {Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix; it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give.},
	number = {1},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhou, Weijie and Zhao, Yue and Shen, Xipeng and Chen, Wang},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Predictive models, Sparse matrices, Runtime, Kernel, Matrix converters, SpMV, high performance computing, Buildings, program optimization, prediction model, sparse matrix format, Time series analysis},
	pages = {80--93},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/6S64R2T2/8787872.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/PTUF7P8M/Zhou et al. - 2020 - Enabling Runtime SpMV Format Selection through an .pdf:application/pdf},
}

@inproceedings{kjolstad_tensor_2019,
	title = {Tensor {Algebra} {Compilation} with {Workspaces}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1109/CGO.2019.8661185},
	doi = {10.1109/CGO.2019.8661185},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to introduce temporary tensors called workspaces to avoid inefficient sparse data structures accesses. We develop an intermediate representation (IR) for tensor operations called concrete index notation that specifies when sub-computations occur and where they are stored. We then describe the workspace transformation in this IR, how to programmatically invoke it, and how the IR is compiled to sparse code. Finally, we show how the transformation can be used to optimize sparse tensor kernels, including sparse matrix multiplication, sparse tensor addition, and the matricized tensor times Khatri-Rao product (MTTKRP). Our results show that the workspace transformation brings the performance of these kernels on par with hand-optimized implementations. For example, we improve the performance of MTTKRP with dense output by up to 35\%, and enable generating sparse matrix multiplication and MTTKRP with sparse output, neither of which were supported by prior tensor algebra compilers.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Kjolstad, Fredrik and Ahrens, Willow and Kamil, Shoaib and Amarasinghe, Saman},
	month = feb,
	year = {2019},
	keywords = {Sparse matrices, Kernel, Indexes, Arrays, program compilers, sparse matrices, data structures, matrix decomposition, matrix multiplication, tensors, intermediate representation, Algebra, sparse matrix multiplication, sparse tensor algebra, sparse data structures, code optimization, compiler IR, concrete index notation, matricized tensor times Khatri-Rao product, sparse tensor addition, sparse tensor algebra compilers, sparse tensor kernels, temporaries, workspaces},
	pages = {180--192},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FCYUD2HP/Kjolstad et al. - 2019 - Tensor algebra compilation with workspaces.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/SAZD6MM9/8661185.html:text/html;Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:/Users/willow/Zotero/storage/K5Z25DFE/Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:application/pdf},
}

@article{bik_compiler_2022,
	title = {Compiler {Support} for {Sparse} {Tensor} {Computations} in {MLIR}},
	url = {http://arxiv.org/abs/2202.04305},
	abstract = {Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This paper discusses integrating this idea into MLIR.},
	urldate = {2022-03-17},
	journal = {arXiv:2202.04305 [cs]},
	author = {Bik, Aart J. C. and Koanantakool, Penporn and Shpeisman, Tatiana and Vasilache, Nicolas and Zheng, Bixia and Kjolstad, Fredrik},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.04305},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/MSR9ELDI/Bik et al. - 2022 - Compiler Support for Sparse Tensor Computations in.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/UXD7G5HA/2202.html:text/html},
}

@inproceedings{hutchison_laradb_2017,
	address = {New York, NY, USA},
	series = {{BeyondMR}'17},
	title = {{LaraDB}: {A} {Minimalist} {Kernel} for {Linear} and {Relational} {Algebra} {Computation}},
	isbn = {978-1-4503-5019-8},
	shorttitle = {{LaraDB}},
	url = {http://doi.org/10.1145/3070607.3070608},
	doi = {10.1145/3070607.3070608},
	abstract = {Analytics tasks manipulate structured data with variants of relational algebra (RA) and quantitative data with variants of linear algebra (LA). The two computational models have overlapping expressiveness, motivating a common programming model that affords unified reasoning and algorithm design. At the logical level we propose LARA, a lean algebra of three operators, that expresses RA and LA as well as relevant optimization rules. We show a series of proofs that position LARA at just the right level of expressiveness for a middleware algebra: more explicit than MapReduce but more general than RA or LA. At the physical level we find that the LARA operators afford efficient implementations using a single primitive that is available in a variety of backend engines: range scans over partitioned sorted maps. To evaluate these ideas, we implemented the LARA operators as range iterators in Apache Accumulo, a popular implementation of Google's BigTable. First we show how LARA expresses a sensor quality control task, and we measure the performance impact of optimizations LARA admits on this task. Second we show that the LARADB implementation outperforms Accumulo's native MapReduce integration on a core task involving join and aggregation in the form of matrix multiply, especially at smaller scales that are typically a poor fit for scale-out approaches. We find that LARADB offers a conceptually lean framework for optimizing mixed-abstraction analytics tasks, without giving up fast record-level updates and scans.},
	urldate = {2022-03-15},
	booktitle = {Proceedings of the 4th {ACM} {SIGMOD} {Workshop} on {Algorithms} and {Systems} for {MapReduce} and {Beyond}},
	publisher = {Association for Computing Machinery},
	author = {Hutchison, Dylan and Howe, Bill and Suciu, Dan},
	month = may,
	year = {2017},
	pages = {1--10},
	file = {Full Text PDF:/Users/willow/Zotero/storage/R7Y9BG2C/Hutchison et al. - 2017 - LaraDB A Minimalist Kernel for Linear and Relatio.pdf:application/pdf},
}

@article{liu_intermediate_2023,
	title = {An {Intermediate} {Language} for {General} {Sparse} {Format} {Customization}},
	issn = {1556-6064},
	doi = {10.1109/LCA.2023.3262610},
	abstract = {The inevitable trend of hardware specialization drives an increasing use of custom data formats in processing sparse workloads, which are typically memory-bound. These formats facilitate the automated generation of target-aware data layouts to improve memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Moreover, since these frameworks adopt an attribute-based approach for format abstraction, they cannot easily be extended to support general format customization. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. More concretely, we express a sparse format as a map from dense coordinates to a layout tree using a small set of well-defined query and mutation primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with hybrid formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, and a simulated processing-in-memory (PIM) device.},
	journal = {IEEE Computer Architecture Letters},
	author = {Liu, Jie and Zhao, Zhongyuan and Ding, Zijian and Brock, Benjamin and Rong, Hongbo and Zhang, Zhiru},
	year = {2023},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Codes, compilers, Compilers, Hardware, heterogeneous (hybrid) systems, Indexes, Kernel, Layout, Metadata, sparse linear algebra, specialized application languages, Tensors},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/UA4W8VQ7/10083210.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/AX7BMSJS/Liu et al. - 2023 - An Intermediate Language for General Sparse Format.pdf:application/pdf},
}

@inproceedings{ahrens_looplets_2023,
	address = {New York, NY, USA},
	series = {{CGO} 2023},
	title = {Looplets: {A} {Language} for {Structured} {Coiteration}},
	isbn = {9798400701016},
	shorttitle = {Looplets},
	doi = {10.1145/3579990.3580020},
	abstract = {Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Specializing for structure yields significant speedups. But automatically generating efficient code for structured data is challenging, especially when arrays with different structure interact. We show how to abstract over array structures so that the compiler can generate code to coiterate over any combination of them. Our technique enables new array formats (such as 1DVBL for irregular clustered sparsity), new iteration strategies (such as galloping intersections), and new operations over structured data (such as concatenation or convolution).},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Donenfeld, Daniel and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = feb,
	year = {2023},
	keywords = {Sparse, Tensor, Array, Coiteration, Compressed},
	pages = {41--54},
	file = {Full Text PDF:/Users/willow/Zotero/storage/HUBGW7N9/Ahrens et al. - 2023 - Looplets A Language for Structured Coiteration.pdf:application/pdf},
}

@article{shaikhha_functional_2022,
	title = {Functional collection programming with semi-ring dictionaries},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3527333},
	doi = {10.1145/3527333},
	abstract = {This paper introduces semi-ring dictionaries, a powerful class of compositional and purely functional collections that subsume other collection types such as sets, multisets, arrays, vectors, and matrices. We developed SDQL, a statically typed language that can express relational algebra with aggregations, linear algebra, and functional collections over data such as relations and matrices using semi-ring dictionaries. Furthermore, thanks to the algebraic structure behind these dictionaries, SDQL unifies a wide range of optimizations commonly used in databases (DB) and linear algebra (LA). As a result, SDQL enables efficient processing of hybrid DB and LA workloads, by putting together optimizations that are otherwise confined to either DB systems or LA frameworks. We show experimentally that a handful of DB and LA workloads can take advantage of the SDQL language and optimizations. SDQL can be competitive with or outperforms a host of systems that are state of the art in their own domain: in-memory DB systems Typer and Tectorwise for (flat, not nested) relational data; SciPy for LA workloads; sparse tensor compiler taco; the Trance nested relational engine; and the in-database machine learning engines LMFAO and Morpheus for hybrid DB/LA workloads over relational data.},
	number = {OOPSLA1},
	urldate = {2023-07-10},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Shaikhha, Amir and Huot, Mathieu and Smith, Jaclyn and Olteanu, Dan},
	month = apr,
	year = {2022},
	keywords = {Nested Relational Algebra, Semi-Ring Dictionary, Sparse Linear Algebra},
	pages = {89:1--89:33},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Z2IY8UTN/Shaikhha et al. - 2022 - Functional collection programming with semi-ring d.pdf:application/pdf},
}

@article{psarras_linear_2022,
	title = {The {Linear} {Algebra} {Mapping} {Problem}. {Current} state of linear algebra languages and libraries},
	volume = {48},
	issn = {0098-3500, 1557-7295},
	url = {http://arxiv.org/abs/1911.09421},
	doi = {10.1145/3549935},
	abstract = {We observe a disconnect between the developers and the end users of linear algebra libraries. On the one hand, the numerical linear algebra and the high-performance communities invest significant effort in the development and optimization of highly sophisticated numerical kernels and libraries, aiming at the maximum exploitation of both the properties of the input matrices, and the architectural features of the target computing platform. On the other hand, end users are progressively less likely to go through the error-prone and time consuming process of directly using said libraries by writing their code in C or Fortran; instead, languages and libraries such as Matlab, Julia, Eigen and Armadillo, which offer a higher level of abstraction, are becoming more and more popular. Users are given the opportunity to code matrix computations with a syntax that closely resembles the mathematical description; it is then a compiler or an interpreter that internally maps the input program to lower level kernels, as provided by libraries such as BLAS and LAPACK. Unfortunately, our experience suggests that in terms of performance, this translation is typically vastly suboptimal. In this paper, we first introduce the Linear Algebra Mapping Problem, and then investigate how effectively a benchmark of test problems is solved by popular high-level programming languages. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both standard compiler optimizations such as common subexpression elimination and loop-invariant code motion, as well as linear algebra specific optimizations such as optimal parenthesization of a matrix product and kernel selection for matrices with properties. The aim of this study is to give concrete guidelines for the development of languages and libraries that support linear algebra computations.},
	number = {3},
	urldate = {2023-10-03},
	journal = {ACM Transactions on Mathematical Software},
	author = {Psarras, Christos and Barthels, Henrik and Bientinesi, Paolo},
	month = sep,
	year = {2022},
	note = {arXiv:1911.09421 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Programming Languages},
	pages = {1--30},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/6APM5QY7/Psarras et al. - 2022 - The Linear Algebra Mapping Problem. Current state .pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/6JF2TJK5/1911.html:text/html},
}

@inproceedings{lo_roofline_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	isbn = {978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	doi = {10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	keywords = {CUDA unified memory, Memory bandwidth, Roofline},
	pages = {129--148},
	file = {Submitted Version:/Users/willow/Zotero/storage/X3YWPR6T/Lo et al. - 2015 - Roofline Model Toolkit A Practical Tool for Archi.pdf:application/pdf},
}

@inproceedings{fegade_cora_2022,
	title = {The {CoRa} {Tensor} {Compiler}: {Compilation} for {Ragged} {Tensors} with {Minimal} {Padding}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems}},
	author = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip and Mowry, Todd},
	editor = {Marculescu, D. and Chi, Y. and Wu, C.},
	year = {2022},
	keywords = {Computer Science - Machine Learning},
	pages = {721--747},
	file = {Fegade et al. - 2021 - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/6CRTC4AC/Fegade et al. - 2021 - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf;Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/8PT7K7GL/Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf},
}

@article{mcmichen_representing_nodate,
	title = {Representing {Data} {Collections} in an {SSA} {Form}},
	abstract = {Compiler research and development has treated computation as the primary driver of performance improvements in C/C++ programs, leaving memory optimizations as a secondary consideration. Developers are currently handed the arduous task of describing both the semantics and layout of their data in memory, either manually or via libraries, prematurely lowering high-level data collections to a low-level view of memory for the compiler. Thus, the compiler can only glean conservative information about the memory in a program, e.g., alias analysis, and is further hampered by heavy memory optimizations. This paper proposes the Memory Object Intermediate Representation (MEMOIR), a language-agnostic SSA form for sequential and associative data collections, objects, and the fields contained therein. At the core of MEMOIR is a decoupling of the memory used to store data from that used to logically organize data. Through its SSA form, MEMOIR compilers can perform element-level analysis on data collections, enabling static analysis on the state of a collection or object at any given program point. To illustrate the power of this analysis, we perform dead element elimination, resulting in a 26.6\% speedup on mcf from SPECINT 2017. With the degree of freedom to mutate memory layout, our MEMOIR compiler performs field elision and dead field elimination, reducing peak memory usage of mcf by 20.8\%.},
	language = {en},
	author = {McMichen, Tommy and Greiner, Nathan and Zhong, Peter and Sossai, Federico and Patel, Atmn and Campanoni, Simone},
	file = {McMichen et al. - Representing Data Collections in an SSA Form.pdf:/Users/willow/Zotero/storage/GP9YC4IB/McMichen et al. - Representing Data Collections in an SSA Form.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018,
	address = {New York, NY, USA},
	series = {{ICPP} '18},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer's push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebra-based framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-of-the-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	keywords = {breadth-first search, graph algorithms, sparse matrix multiplication},
	pages = {1--11},
	file = {Full Text PDF:/Users/willow/Zotero/storage/NGHWT98P/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf;Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:/Users/willow/Zotero/storage/ADD2JSKL/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6GTQB2I8/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{moler_history_2020,
	title = {A history of {MATLAB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3386331},
	doi = {10.1145/3386331},
	abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
	number = {HOPL},
	urldate = {2024-03-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Moler, Cleve and Little, Jack},
	month = jun,
	year = {2020},
	keywords = {linear algebra, MATLAB, matrix computation},
	pages = {81:1--81:67},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2HANKRI8/Moler and Little - 2020 - A history of MATLAB.pdf:application/pdf},
}

@book{abelson_structure_1996,
	title = {Structure and {Interpretation} of {Computer} {Programs}},
	isbn = {978-0-262-51087-5 978-0-262-31091-8},
	url = {https://library.oapen.org/handle/20.500.12657/26092},
	abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
	language = {English},
	urldate = {2024-03-18},
	publisher = {The MIT Press},
	author = {Abelson, Harold and Sussman, Gerald Jay},
	month = jul,
	year = {1996},
	note = {Accepted: 2019-01-17 23:55},
	keywords = {bic Book Industry Communication::U Computing \& information technology::UY Computer science},
	file = {Full Text PDF:/Users/willow/Zotero/storage/KZEF98BF/Abelson and Sussman - 1996 - Structure and Interpretation of Computer Programs.pdf:application/pdf},
}

@book{knuth_art_1997,
	title = {The {Art} of {Computer} {Programming}: {Fundamental} {Algorithms}, {Volume} 1},
	isbn = {978-0-321-63574-7},
	shorttitle = {The {Art} of {Computer} {Programming}},
	abstract = {\&\&gt;The bible of all fundamental algorithms and the work that taught many of today's software developers most of what they know about computer programming.  —Byte, September 1995   I can't begin to tell you how many pleasurable hours of study and recreation they have afforded me! I have pored over them in cars, restaurants, at work, at home... and even at a Little League game when my son wasn't in the line-up. —Charles Long   If you think you're a really good programmer... read [Knuth's] Art of Computer Programming... You should definitely send me a resume if you can read the whole thing. —Bill Gates   It's always a pleasure when a problem is hard enough that you have to get the Knuths off the shelf. I find that merely opening one has a very useful terrorizing effect on computers. —Jonathan Laventhol   This first volume in the series begins with basic programming concepts and techniques, then focuses more particularly on information structures—the representation of information inside a computer, the structural relationships between data elements and how to deal with them efficiently. Elementary applications are given to simulation, numerical methods, symbolic computing, software and system design. Dozens of simple and important algorithms and techniques have been added to those of the previous edition. The section on mathematical preliminaries has been extensively revised to match present trends in research.  Ebook (PDF version) produced by Mathematical Sciences Publishers (MSP),http://msp.org},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Knuth, Donald E.},
	month = jul,
	year = {1997},
	note = {Google-Books-ID: x9AsAwAAQBAJ},
	keywords = {Computers / Programming / General},
}

@inproceedings{dias_sparselnr_2022,
	address = {Virtual Event},
	title = {{SparseLNR}: accelerating sparse tensor computations using loop nest restructuring},
	isbn = {978-1-4503-9281-5},
	shorttitle = {{SparseLNR}},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532386},
	doi = {10.1145/3524059.3532386},
	abstract = {Sparse tensor algebra computations have become important in many real-world applications like machine learning, scientific simulations, and data mining. Hence, automated code generation and performance optimizations for tensor algebra kernels are paramount. Recent advancements such as the Tensor Algebra Compiler (TACO) greatly generalize and automate the code generation for tensor algebra expressions. However, the code generated by TACO for many important tensor computations remains suboptimal due to the absence of a scheduling directive to support transformations such as distribution/fusion.},
	language = {en},
	urldate = {2024-03-20},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Dias, Adhitha and Sundararajah, Kirshanthan and Saumya, Charitha and Kulkarni, Milind},
	month = jun,
	year = {2022},
	keywords = {kernel distribution, loop fusion, loop transformations, sparse tensor algebra},
	pages = {1--14},
	file = {Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:/Users/willow/Zotero/storage/ESTZ5XHN/Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:application/pdf},
}

@article{fisher_hypermedia_1996,
	title = {Hypermedia image processing reference},
	journal = {England: John Wiley \& Sons Ltd},
	author = {Fisher, Robert and Perkins, Simon and Walker, Ashley and Wolfart, Erik},
	year = {1996},
	pages = {118--130},
	file = {Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:/Users/willow/Zotero/storage/HT6FNWC4/Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:application/pdf},
}

@book{gonzalez_digital_2006,
	address = {USA},
	title = {Digital {Image} {Processing} (3rd {Edition})},
	isbn = {978-0-13-168728-8},
	publisher = {Prentice-Hall, Inc.},
	author = {Gonzalez, Rafael C. and Woods, Richard E.},
	month = jan,
	year = {2006},
}

@misc{sundram_compiling_2023,
	title = {Compiling {Recurrences} over {Dense} and {Sparse} {Arrays}},
	url = {http://arxiv.org/abs/2309.04660},
	doi = {10.48550/arXiv.2309.04660},
	abstract = {Recurrence equations lie at the heart of many computational paradigms including dynamic programming, graph analysis, and linear solvers. These equations are often expensive to compute and much work has gone into optimizing them for different situations. The set of recurrence implementations is a large design space across the set of all recurrences (e.g., the Viterbi and Floyd-Warshall algorithms), the choice of data structures (e.g., dense and sparse matrices), and the set of different loop orders. Optimized library implementations do not exist for most points in this design space, and developers must therefore often manually implement and optimize recurrences. We present a general framework for compiling recurrence equations into native code corresponding to any valid point in this general design space. In this framework, users specify a system of recurrences, the type of data structures for storing the input and outputs, and a set of scheduling primitives for optimization. A greedy algorithm then takes this specification and lowers it into a native program that respects the dependencies inherent to the recurrence equation. We describe the compiler transformations necessary to lower this high-level specification into native parallel code for either sparse and dense data structures and provide an algorithm for determining whether the recurrence system is solvable with the provided scheduling primitives. We evaluate the performance and correctness of the generated code on various computational tasks from domains including dense and sparse matrix solvers, dynamic programming, graph problems, and sparse tensor algebra. We demonstrate that generated code has competitive performance to handwritten implementations in libraries.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Sundram, Shiv and Tariq, Muhammad Usman and Kjolstad, Fredrik},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04660 [cs]},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/SWQTJTR9/Sundram et al. - 2023 - Compiling Recurrences over Dense and Sparse Arrays.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/F6T9JKCX/2309.html:text/html},
}

@article{noauthor_developer_2024,
	title = {Developer {Reference} for {Intel}® {oneAPI} {Math} {Kernel} {Library} for {Fortran}},
	url = {https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-fortran/2024-0/overview.html},
	abstract = {The Intel® oneAPI Math Kernel Library (oneMKL) improves performance with math routines for software applications that solve large computational problems. oneMKL provides BLAS and LAPACK linear algebra routines, fast Fourier transforms, vectorized math functions, random number generation functions, and other functionality.},
	language = {en},
	month = apr,
	year = {2024},
	file = {Developer Reference for Intel® oneAPI Math Kernel .pdf:/Users/willow/Zotero/storage/R6N2BIWN/Developer Reference for Intel® oneAPI Math Kernel .pdf:application/pdf},
}

@article{dean_mapreduce_2008,
	title = {{MapReduce}: simplified data processing on large clusters},
	volume = {51},
	issn = {0001-0782},
	shorttitle = {{MapReduce}},
	url = {https://dl.acm.org/doi/10.1145/1327452.1327492},
	doi = {10.1145/1327452.1327492},
	abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
	number = {1},
	urldate = {2024-04-03},
	journal = {Communications of the ACM},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	month = jan,
	year = {2008},
	pages = {107--113},
	file = {Full Text PDF:/Users/willow/Zotero/storage/BFY2JL33/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:application/pdf},
}

@misc{nayak_teaal_2023,
	title = {{TeAAL}: {A} {Declarative} {Framework} for {Modeling} {Sparse} {Tensor} {Accelerators}},
	shorttitle = {{TeAAL}},
	url = {http://arxiv.org/abs/2304.07931},
	abstract = {Over the past few years, the explosion in sparse tensor algebra workloads has led to a corresponding rise in domain-specific accelerators to service them. Due to the irregularity present in sparse tensors, these accelerators employ a wide variety of novel solutions to achieve good performance. At the same time, prior work on design-flexible sparse accelerator modeling does not express this full range of design features, making it difficult to understand the impact of each design choice and compare or extend the state-ofthe-art. To address this, we propose TeAAL: a language and simulator generator for the concise and precise specification and evaluation of sparse tensor algebra accelerators. We use TeAAL to represent and evaluate four disparate state-of-the-art accelerators—ExTensor, Gamma, OuterSPACE, and SIGMA—and verify that it reproduces their performance with high accuracy. Finally, we demonstrate the potential of TeAAL as a tool for designing new accelerators by showing how it can be used to speed up vertex-centric programming accelerators—achieving 1.9× on BFS and 1.2× on SSSP over GraphDynS.},
	language = {en},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Nayak, Nandeeka and Odemuyiwa, Toluwanimi O. and Ugare, Shubham and Fletcher, Christopher W. and Pellauer, Michael and Emer, Joel S.},
	month = oct,
	year = {2023},
	note = {arXiv:2304.07931 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	file = {Nayak et al. - 2023 - TeAAL A Declarative Framework for Modeling Sparse.pdf:/Users/willow/Zotero/storage/X333VC2H/Nayak et al. - 2023 - TeAAL A Declarative Framework for Modeling Sparse.pdf:application/pdf},
}

@inproceedings{yadav_spdistal_2022,
	address = {Dallas, Texas},
	series = {{SC} '22},
	title = {{SpDISTAL}: compiling distributed sparse tensor computations},
	shorttitle = {{SpDISTAL}},
	abstract = {We introduce SpDISTAL, a compiler for sparse tensor algebra that targets distributed systems. SpDISTAL combines separate descriptions of tensor algebra expressions, sparse data structures, data distribution, and computation distribution. Thus, it enables distributed execution of sparse tensor algebra expressions with a wide variety of sparse data structures and data distributions. SpDISTAL is implemented as a C++ library that targets a distributed task-based runtime system and can generate code for nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates distributed code that achieves performance competitive with hand-written distributed functions for specific sparse tensor algebra expressions and that outperforms general interpretation-based systems by one to two orders of magnitude.},
	urldate = {2024-04-03},
	booktitle = {Proceedings of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Press},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = nov,
	year = {2022},
	keywords = {computer science, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages, parallel programming, programming},
	pages = {1--15},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LAU7SENV/Yadav et al. - 2022 - SpDISTAL compiling distributed sparse tensor comp.pdf:application/pdf;Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:/Users/willow/Zotero/storage/RIG6L6EJ/Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:application/pdf;Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:/Users/willow/Zotero/storage/HG5I22L3/Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:application/pdf},
}

@inproceedings{zhang_gamma_2021,
	address = {Virtual USA},
	title = {Gamma: leveraging {Gustavson}’s algorithm to accelerate sparse matrix multiplication},
	isbn = {978-1-4503-8317-2},
	shorttitle = {Gamma},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446702},
	doi = {10.1145/3445814.3446702},
	abstract = {Sparse matrix-sparse matrix multiplication (spMspM) is at the heart of a wide range of scientific and machine learning applications. spMspM is inefficient on general-purpose architectures, making accelerators attractive. However, prior spMspM accelerators use inner- or outer-product dataflows that suffer poor input or output reuse, leading to high traffic and poor performance. These prior accelerators have not explored Gustavson’s algorithm, an alternative spMspM dataflow that does not suffer from these problems but features irregular memory access patterns that prior accelerators do not support.},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Zhang, Guowei and Attaluri, Nithya and Emer, Joel S. and Sanchez, Daniel},
	month = apr,
	year = {2021},
	pages = {687--701},
	file = {Zhang et al. - 2021 - Gamma leveraging Gustavson’s algorithm to acceler.pdf:/Users/willow/Zotero/storage/ZCBY7MID/Zhang et al. - 2021 - Gamma leveraging Gustavson’s algorithm to acceler.pdf:application/pdf},
}

@inproceedings{ye_sparsetir_2023,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2023},
	title = {{SparseTIR}: {Composable} {Abstractions} for {Sparse} {Compilation} in {Deep} {Learning}},
	isbn = {978-1-4503-9918-0},
	url = {https://doi.org/10.1145/3582016.3582047},
	doi = {10.1145/3582016.3582047},
	abstract = {Sparse tensors are rapidly becoming critical components of modern deep learning workloads. However, developing high-performance sparse operators can be difficult and tedious, and existing vendor libraries cannot satisfy the escalating demands from new operators. Sparse tensor compilers simplify the development of operators, but efficient sparse compilation for deep learning remains challenging because a single sparse format cannot maximize hardware efficiency, and single-shot compilers cannot keep up with latest hardware and system advances. In this paper, we observe that the key to addressing both these challenges is to leverage composable formats and composable transformations. We propose SparseTIR, a sparse tensor compilation abstraction that offers composable formats and composable transformations for deep learning workloads. SparseTIR constructs a search space over these composable components for performance tuning. With these improvements, SparseTIR obtains consistent performance speedups vs vendor libraries on GPUs for single operators: 1.20-2.34x for GNN operators, 1.05-2.98x for sparse attention operators, and 0.56-7.45x for sparse convolution operators. SparseTIR also accelerates end-to-end GNNs by 1.08-1.52x for GraphSAGE training, and 4.20-40.18x for RGCN inference.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 3},
	publisher = {Association for Computing Machinery},
	author = {Ye, Zihao and Lai, Ruihang and Shao, Junru and Chen, Tianqi and Ceze, Luis},
	year = {2023},
	note = {event-place: , Vancouver, BC, Canada,},
	keywords = {Code Generation and Optimizations, Kernel Fusion, Scheduling, Sparse Computation, Tensor Compilers, Tensor Cores, Vectorization},
	pages = {660--678},
}

@article{authors_continuous_2024,
	title = {The {Continuous} {Tensor} {Abstraction}: {Where} {Indices} are {Real}},
	journal = {OOPSLA, submitted Round 2},
	author = {Authors, Anonymous},
	year = {2024},
}

@article{won_unified_2023,
	title = {Unified {Convolution} {Framework}: {A} compiler-based approach to support sparse convolutions},
	volume = {5},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Won, Jaeyeon and Hong, Changwan and Mendis, Charith and Emer, Joel and Amarasinghe, Saman},
	year = {2023},
}

@article{sze_efficient_2017,
	title = {Efficient processing of deep neural networks: {A} tutorial and survey},
	volume = {105},
	number = {12},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
	year = {2017},
	note = {Publisher: Ieee},
	pages = {2295--2329},
}

@misc{guennebaud_eigen_2010,
	title = {Eigen v3},
	url = {http://eigen.tuxfamily.org},
	author = {Guennebaud, Gaël and Jacob, Benoît and {others}},
	year = {2010},
}

@inproceedings{verdoolaege_isl_2010,
	title = {isl: {An} integer set library for the polyhedral model},
	booktitle = {International {Congress} on {Mathematical} {Software}},
	publisher = {Springer},
	author = {Verdoolaege, Sven},
	year = {2010},
	pages = {299--302},
}

@inproceedings{yuki_alphaz_2012,
	title = {Alphaz: {A} system for design space exploration in the polyhedral model},
	booktitle = {International {Workshop} on {Languages} and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer},
	author = {Yuki, Tomofumi and Gupta, Gautam and Kim, DaeGon and Pathan, Tanveer and Rajopadhye, Sanjay},
	year = {2012},
	pages = {17--31},
}

@inproceedings{bondhugula_pluto_2008,
	title = {Pluto: {A} practical and fully automatic polyhedral program optimization system},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2008 {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI} 08), {Tucson}, {AZ} ({June} 2008)},
	publisher = {Citeseer},
	author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J and Sadayappan, P},
	year = {2008},
}

@article{chen_framework_2008,
	title = {A framework for composing high-level loop transformations},
	journal = {Technical Report 08–897, USC Computer Science Technical Report},
	author = {Chen, Chun and Chame, Jacqueline and Hall, Mary},
	year = {2008},
}

@article{chen_tvm_2018,
	title = {{TVM}: end-to-end optimization stack for deep learning},
	volume = {11},
	number = {2018},
	journal = {arXiv preprint arXiv:1802.04799},
	author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Shen, Haichen and Yan, Eddie Q and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	year = {2018},
	note = {Publisher: CoRR},
	pages = {20},
}

@inproceedings{tang_torchsparse_2023,
	title = {Torchsparse++: {Efficient} training and inference framework for sparse convolution on gpus},
	booktitle = {Proceedings of the 56th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Tang, Haotian and Yang, Shang and Liu, Zhijian and Hong, Ke and Yu, Zhongming and Li, Xiuyu and Dai, Guohao and Wang, Yu and Han, Song},
	year = {2023},
	pages = {225--239},
}

@inproceedings{huang_ge-spmm_2020,
	title = {Ge-spmm: {General}-purpose sparse matrix-matrix multiplication on gpus for graph neural networks},
	booktitle = {{SC20}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Huang, Guyue and Dai, Guohao and Wang, Yu and Yang, Huazhong},
	year = {2020},
	pages = {1--12},
}

@inproceedings{ashari_fast_2014,
	title = {Fast sparse matrix-vector multiplication on {GPUs} for graph applications},
	booktitle = {{SC}'14: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Ashari, Arash and Sedaghati, Naser and Eisenlohr, John and Parthasarath, Srinivasan and Sadayappan, P},
	year = {2014},
	pages = {781--792},
}

@inproceedings{kang_gbase_2011,
	title = {{GBASE}: a scalable and general graph management system},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Kang, U and Tong, Hanghang and Sun, Jimeng and Lin, Ching-Yung and Faloutsos, Christos},
	year = {2011},
	pages = {1091--1099},
}

@article{rumengan_pyarmadillo_2021,
	title = {{PyArmadillo}: a streamlined linear algebra library for {Python}},
	volume = {6},
	url = {https://doi.org/10.21105/joss.03051},
	doi = {10.21105/joss.03051},
	number = {66},
	journal = {Journal of Open Source Software},
	author = {Rumengan, Jason and Zhuo, Terry Yue and Sanderson, Conrad},
	year = {2021},
	note = {Publisher: The Open Journal},
	pages = {3051},
}

@book{anderson_lapack_1999,
	title = {{LAPACK} {Users}’ {Guide}},
	isbn = {978-0-89871-960-4},
	url = {http://dx.doi.org/10.1137/1.9780898719604},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, L. S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
	month = jan,
	year = {1999},
	doi = {10.1137/1.9780898719604},
}

@article{baumann_array_2021,
	title = {Array databases: {Concepts}, standards, implementations},
	volume = {8},
	journal = {Journal of Big Data},
	author = {Baumann, Peter and Misev, Dimitar and Merticariu, Vlad and Huu, Bang Pham},
	year = {2021},
	note = {Publisher: Springer},
	pages = {1--61},
}

@inproceedings{franchetti_operator_2009,
	title = {Operator language: {A} program generation framework for fast kernels},
	booktitle = {{IFIP} {Working} {Conference} on {Domain}-{Specific} {Languages}},
	publisher = {Springer},
	author = {Franchetti, Franz and de Mesmay, Frédéric and McFarlin, Daniel and Püschel, Markus},
	year = {2009},
	pages = {385--409},
}

@article{luo_scalable_2018,
	title = {Scalable linear algebra on a relational database system},
	volume = {47},
	number = {1},
	journal = {ACM SIGMOD Record},
	author = {Luo, Shangyu and Gao, Zekai J and Gubanov, Michael and Perez, Luis L and Jermaine, Christopher},
	year = {2018},
	note = {Publisher: ACM New York, NY, USA},
	pages = {24--31},
}

@article{stonebraker_scidb_2013,
	title = {{SciDB}: {A} database management system for applications with complex analytics},
	volume = {15},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Stonebraker, Michael and Brown, Paul and Zhang, Donghui and Becla, Jacek},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {54--62},
}

@article{tsatalos_gmap_1996,
	title = {The {GMAP}: {A} versatile tool for physical data independence},
	volume = {5},
	journal = {The VLDB Journal},
	author = {Tsatalos, Odysseas G and Solomon, Marvin H and Ioannidis, Yannis E},
	year = {1996},
	note = {Publisher: Springer},
	pages = {101--118},
}

@article{kersten_everything_2018,
	title = {Everything you always wanted to know about compiled and vectorized queries but were afraid to ask},
	volume = {11},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
	year = {2018},
	note = {Publisher: VLDB Endowment},
	pages = {2209--2222},
}

@article{boncz_vectorwise_2012,
	title = {Vectorwise: {Beyond} column stores},
	volume = {35},
	number = {1},
	journal = {IEEE Data Engineering Bulletin},
	author = {Boncz, PA and Zukowski, M},
	year = {2012},
	pages = {21--27},
}

@book{date_guide_1989,
	title = {A {Guide} to the {SQL} {Standard}},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Date, Chris J},
	year = {1989},
}

@article{zhao_polyhedral_2022,
	title = {Polyhedral specification and code generation of sparse tensor contraction with co-iteration},
	volume = {20},
	number = {1},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Zhao, Tuowen and Popoola, Tobi and Hall, Mary and Olschanowsky, Catherine and Strout, Michelle},
	year = {2022},
	note = {Publisher: ACM New York, NY},
	keywords = {Computer Science - Performance, Computer Science - Programming Languages},
	pages = {1--26},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/QRCUWGKJ/Zhao et al. - 2022 - Polyhedral Specification and Code Generation of Sp.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/JMJG4A5X/2208.html:text/html},
}

@inproceedings{dao_monarch_2022,
	title = {Monarch: {Expressive} structured matrices for efficient and accurate training},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and Ré, Christopher},
	year = {2022},
	pages = {4690--4721},
}

@book{anderson_lapack_1999-1,
	title = {{LAPACK} users' guide},
	publisher = {SIAM},
	author = {Anderson, Edward and Bai, Zhaojun and Bischof, Christian and Blackford, L Susan and Demmel, James and Dongarra, Jack and Du Croz, Jeremy and Greenbaum, Anne and Hammarling, Sven and McKenney, Alan and {others}},
	year = {1999},
}

@article{grosser_pollyperforming_2012,
	title = {Polly—performing polyhedral optimizations on a low-level intermediate representation},
	volume = {22},
	number = {04},
	journal = {Parallel Processing Letters},
	author = {Grosser, Tobias and Groesslinger, Armin and Lengauer, Christian},
	year = {2012},
	note = {Publisher: World Scientific},
	pages = {1250010},
}

@inproceedings{mattson_lagraph_2019,
	title = {{LAGraph}: {A} community effort to collect graph algorithms built on top of the {GraphBLAS}},
	booktitle = {2019 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Mattson, Tim and Davis, Timothy A and Kumar, Manoj and Buluc, Aydin and McMillan, Scott and Moreira, José and Yang, Carl},
	year = {2019},
	pages = {276--284},
}

@inproceedings{buluc_design_2017,
	title = {Design of the {GraphBLAS} {API} for {C}},
	booktitle = {2017 {IEEE} international parallel and distributed processing symposium workshops ({IPDPSW})},
	publisher = {IEEE},
	author = {Buluç, Aydin and Mattson, Tim and McMillan, Scott and Moreira, José and Yang, Carl},
	year = {2017},
	pages = {643--652},
}

@article{bradski_opencv_2000,
	title = {{OpenCV}},
	volume = {3},
	number = {2},
	journal = {Dr. Dobb’s journal of software tools},
	author = {Bradski, Gary and Kaehler, Adrian and {others}},
	year = {2000},
}

@inproceedings{spampinato_basic_2016,
	title = {A basic linear algebra compiler for structured matrices},
	booktitle = {Proceedings of the 2016 {International} {Symposium} on {Code} {Generation} and {Optimization}},
	author = {Spampinato, Daniele G and Püschel, Markus},
	year = {2016},
	pages = {117--127},
}

@article{burns_dedalus_2020,
	title = {Dedalus: {A} flexible framework for numerical simulations with spectral methods},
	volume = {2},
	doi = {10.1103/PhysRevResearch.2.023068},
	number = {2},
	journal = {Physical Review Research},
	author = {Burns, Keaton J. and Vasil, Geoffrey M. and Oishi, Jeffrey S. and Lecoanet, Daniel and Brown, Benjamin P.},
	month = apr,
	year = {2020},
	note = {\_eprint: 1905.10388},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Computational Physics, Physics - Fluid Dynamics},
	pages = {023068},
}

@article{bernstein_ebb_2016,
	title = {Ebb: {A} {DSL} for physical simulation on {CPUs} and {GPUs}},
	volume = {35},
	number = {2},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Bernstein, Gilbert Louis and Shah, Chinmayee and Lemire, Crystal and Devito, Zachary and Fisher, Matthew and Levis, Philip and Hanrahan, Pat},
	year = {2016},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--12},
}

@techreport{balay_petsc_2020,
	title = {{PETSc} {Users} {Manual} ({Rev}. 3.13)},
	institution = {Argonne National Lab.(ANL), Argonne, IL (United States)},
	author = {Balay, S and Abhyankar, S and Adams, Mark F and Brown, J and Brune, P and Buschelman, K and Dalcin, L and Dener, A and Eijkhout, V and Gropp, W and {others}},
	year = {2020},
}

@book{oleary_scientific_2009,
	title = {Scientific computing with case studies},
	publisher = {SIAM},
	author = {O'Leary, Dianne P},
	year = {2009},
}

@article{abhyankar_petsc_nodate,
	title = {{PETSc} {DMNetwork}: {A} {Scalable} {Network} {PDE}-{Based} {Multiphysics} {Simulator}},
	author = {ABHYANKAR, SHRIRANG and BETRIE, GETNET and MALDONADO, DANIEL A and MCINNES, LOIS C and SMITH, BARRY and ZHANG, HONG},
}

@article{golomb_run-length_1966,
	title = {Run-length encodings (corresp.)},
	volume = {12},
	number = {3},
	journal = {IEEE transactions on information theory},
	author = {Golomb, Solomon},
	year = {1966},
	note = {Publisher: Citeseer},
	pages = {399--401},
}

@article{bell_lessons_2007,
	title = {Lessons from the {Netflix} prize challenge},
	volume = {9},
	number = {2},
	journal = {Acm Sigkdd Explorations Newsletter},
	author = {Bell, Robert M and Koren, Yehuda},
	year = {2007},
	note = {Publisher: ACM New York, NY, USA},
	pages = {75--79},
}

@inproceedings{beamer_direction-optimizing_2012,
	title = {Direction-optimizing breadth-first search},
	booktitle = {{SC}'12: {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Beamer, Scott and Asanovic, Krste and Patterson, David},
	year = {2012},
	pages = {1--10},
}

@article{wang_free_2023,
	title = {Free join: {Unifying} worst-case optimal and traditional joins},
	volume = {1},
	number = {2},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Wang, Yisu Remy and Willsey, Max and Suciu, Dan},
	year = {2023},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--23},
}

@article{ghorbani_compiling_2023,
	title = {Compiling {Structured} {Tensor} {Algebra}},
	volume = {7},
	url = {https://dl.acm.org/doi/10.1145/3622804},
	doi = {10.1145/3622804},
	abstract = {Tensor algebra is essential for data-intensive workloads in various computational domains. Computational scientists face a trade-off between the specialization degree provided by dense tensor algebra and the algorithmic efficiency that leverages the structure provided by sparse tensors. This paper presents StructTensor, a framework that symbolically computes structure at compilation time. This is enabled by Structured Tensor Unified Representation (STUR), an intermediate language that can capture tensor computations as well as their sparsity and redundancy structures. Through a mathematical view of lossless tensor computations, we show that our symbolic structure computation and the related optimizations are sound. Finally, for different tensor computation workloads and structures, we experimentally show how capturing the symbolic structure can result in outperforming state-of-the-art frameworks for both dense and sparse tensor algebra.},
	number = {OOPSLA2},
	urldate = {2024-04-24},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Ghorbani, Mahdi and Huot, Mathieu and Hashemian, Shideh and Shaikhha, Amir},
	month = oct,
	year = {2023},
	keywords = {Code generation, Computer Science - Mathematical Software, Computer Science - Programming Languages, Computer Science - Symbolic Computation, Program analysis, Program synthesis, Structured tensors, Tensor algebra},
	pages = {229:204--229:233},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/3PPV2QHK/Ghorbani et al. - 2022 - Compiling Structured Tensor Algebra.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/VJY42QCK/2211.html:text/html;Full Text PDF:/Users/willow/Zotero/storage/39FPVXDD/Ghorbani et al. - 2023 - Compiling Structured Tensor Algebra.pdf:application/pdf},
}
