
@techreport{saad_sparskit_1990,
	title = {{SPARSKIT}: {A} basic tool kit for sparse matrix computations},
	shorttitle = {{SPARSKIT}},
	url = {https://ntrs.nasa.gov/search.jsp?R=19910023551},
	abstract = {Presented here are the main features of a tool package for manipulating and working with sparse matrices. One of the goals of the package is to provide basic tools to facilitate the exchange of software and data between researchers in sparse matrix computations. The starting point is the Harwell/Boeing collection of matrices for which the authors provide a number of tools. Among other things, the package provides programs for converting data structures, printing simple statistics on a matrix, plotting a matrix profile, and performing linear algebra operations with sparse matrices.},
	urldate = {2020-01-26},
	author = {Saad, Youcef},
	month = may,
	year = {1990},
	keywords = {algebra, applications programs, computation, data structures, format, matrices, plotting, printing},
	file = {NASA NTRS Full Text PDF:/Users/willow/Zotero/storage/SRM6DKTQ/Saad - 1990 - SPARSKIT A basic tool kit for sparse matrix compu.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/EQJWVI5Z/search.html:text/html},
}

@inproceedings{im_optimizing_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimizing {Sparse} {Matrix} {Computations} for {Register} {Reuse} in {SPARSITY}},
	isbn = {978-3-540-42232-7 978-3-540-45545-5},
	url = {https://link.springer.com/chapter/10.1007/3-540-45545-0_22},
	doi = {10.1007/3-540-45545-0_22},
	abstract = {Sparse matrix-vector multiplication is an important computational kernel that tends to perform poorly on modern processors, largely because of its high ratio of memory operations to arithmetic operations. Optimizing this algorithm is difficult, both because of the complexity of memory systems and because the performance is highly dependent on the nonzero structure of the matrix. The Sparsity system is designed to address these problem by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. The most difficult aspect of optimizing these algorithms is selecting among a large set of possible transformations and choosing parameters, such as block size. In this paper we discuss the optimization of two operations: a sparse matrix times a dense vector and a sparse matrix times a set of dense vectors. Our experience indicates that for matrices arising in scientific simulations, register level optimizations are critical, and we focus here on the optimizations and parameter selection techniques used in Sparsity for register-level optimizations. We demonstrate speedups of up to 2× for the single vector case and 5× for the multiple vector case.},
	language = {en},
	urldate = {2018-02-06},
	booktitle = {Computational {Science} — {ICCS} 2001},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Im, Eun-Jin and Yelick, Katherine},
	month = may,
	year = {2001},
	pages = {127--136},
	file = {Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:/Users/willow/Zotero/storage/GW3ZQ2X8/Im and Yelick - 2001 - Optimizing Sparse Matrix Computations for Register.pdf:application/pdf},
}

@article{solomonik_sparse_2015,
	title = {Sparse {Tensor} {Algebra} as a {Parallel} {Programming} {Model}},
	url = {http://arxiv.org/abs/1512.00066},
	abstract = {Dense and sparse tensors allow the representation of most bulk data structures in computational science applications. We show that sparse tensor algebra can also be used to express many of the transformations on these datasets, especially those which are parallelizable. Tensor computations are a natural generalization of matrix and graph computations. We extend the usual basic operations of tensor summation and contraction to arbitrary functions, and further operations such as reductions and mapping. The expression of these transformations in a high-level sparse linear algebra domain specific language allows our framework to understand their properties at runtime to select the preferred communication-avoiding algorithm. To demonstrate the efficacy of our approach, we show how key graph algorithms as well as common numerical kernels can be succinctly expressed using our interface and provide performance results of a general library implementation.},
	urldate = {2020-06-11},
	journal = {arXiv:1512.00066 [cs]},
	author = {Solomonik, Edgar and Hoefler, Torsten},
	month = nov,
	year = {2015},
	note = {arXiv: 1512.00066},
	keywords = {Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/CSKKHSY4/Solomonik and Hoefler - 2015 - Sparse Tensor Algebra as a Parallel Programming Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/NNMHZAWR/1512.html:text/html},
}

@misc{kovach_correct_2022,
	title = {Correct {Compilation} of {Semiring} {Contractions}},
	url = {http://arxiv.org/abs/2207.13291},
	doi = {10.48550/arXiv.2207.13291},
	abstract = {We introduce a formal operational semantics that describes the fused execution of variable contraction problems, which compute indexed arithmetic over a semiring and generalize sparse and dense tensor algebra, relational algebra, and graph algorithms. We prove that the model is correct with respect to a functional semantics. We also develop a compiler for variable contraction expressions and show that its performance is equivalent to a state-of-the art sparse tensor algebra compiler, while providing greater generality and correctness guarantees.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Kovach, Scott and Kjolstad, Fredrik},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13291 [cs]},
	keywords = {Computer Science - Programming Languages, F.3.1, F.3.2},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/92ZNYZA9/Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/X9EPYKM8/2207.html:text/html;Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:/Users/willow/Zotero/storage/M36DVD8B/Kovach and Kjolstad - 2022 - Correct Compilation of Semiring Contractions.pdf:application/pdf},
}

@misc{schleich_optimizing_2022,
	title = {Optimizing {Tensor} {Programs} on {Flexible} {Storage}},
	url = {http://arxiv.org/abs/2210.06267},
	doi = {10.48550/arXiv.2210.06267},
	abstract = {Tensor programs often need to process large tensors (vectors, matrices, or higher order tensors) that require a specialized storage format for their memory layout. Several such layouts have been proposed in the literature, such as the Coordinate Format, the Compressed Sparse Row format, and many others, that were especially designed to optimally store tensors with specific sparsity properties. However, existing tensor processing systems require specialized extensions in order to take advantage of every new storage format. In this paper we describe a system that allows users to define flexible storage formats in a declarative tensor query language, similar to the language used by the tensor program. The programmer only needs to write storage mappings, which describe, in a declarative way, how the tensors are laid out in main memory. Then, we describe a cost-based optimizer that optimizes the tensor program for the specific memory layout. We demonstrate empirically significant performance improvements compared to state-of-the-art tensor processing systems.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Schleich, Maximilian and Shaikhha, Amir and Suciu, Dan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06267 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/2FW8LIU6/Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/VJNPTS3W/2210.html:text/html;Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:/Users/willow/Zotero/storage/NPQFGVEG/Schleich et al. - 2022 - Optimizing Tensor Programs on Flexible Storage.pdf:application/pdf},
}

@incollection{grossman_5_1989,
	title = {5. {FIDIL}: {A} {Language} for {Scientific} {Programming}},
	isbn = {978-0-89871-239-1 978-1-61197-103-3},
	shorttitle = {5. {FIDIL}},
	url = {http://epubs.siam.org/doi/10.1137/1.9781611971033.ch5},
	abstract = {FIDIL is a new programming language for scientific computation. In this paper, we give a. brief overview of the language, largely consisting of several extended examples from computational fluid dynamics.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Symbolic {Computation}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Hilfinger, Paul N. and Colella, Philip},
	editor = {Grossman, Robert},
	month = jan,
	year = {1989},
	doi = {10.1137/1.9781611971033.ch5},
	pages = {97--138},
	file = {Hilfinger and Colella - 1989 - 5. FIDIL A Language for Scientific Programming.pdf:/Users/willow/Zotero/storage/4ESGBJZJ/Hilfinger and Colella - 1989 - 5. FIDIL A Language for Scientific Programming.pdf:application/pdf},
}

@article{baden_lattice_1993,
	title = {Lattice parallelism: a parallel programming model for manipulating non-uniform structured scientific data structures},
	volume = {28},
	issn = {0362-1340},
	shorttitle = {Lattice parallelism},
	url = {https://doi.org/10.1145/156668.156679},
	doi = {10.1145/156668.156679},
	number = {1},
	urldate = {2022-10-26},
	journal = {ACM SIGPLAN Notices},
	author = {Baden, Scott B. and Kohn, Scott R.},
	month = jan,
	year = {1993},
	pages = {24--27},
	file = {Full Text PDF:/Users/willow/Zotero/storage/EBM3HU8C/Baden and Kohn - 1993 - Lattice parallelism a parallel programming model .pdf:application/pdf},
}

@misc{hsu_sparse_2022,
	title = {The {Sparse} {Abstract} {Machine}},
	url = {http://arxiv.org/abs/2208.14610},
	doi = {10.48550/arXiv.2208.14610},
	abstract = {We propose the Sparse Abstract Machine (SAM), an intermediate representation and abstract machine model for targeting sparse tensor algebra to reconfigurable and fixed-function spatial dataflow accelerators. SAM defines a streaming abstraction with sparse primitives that encompass a large space of scheduled tensor algebra expressions. SAM dataflow graphs naturally separate tensor formats from algorithms and is expressive enough to incorporate many sparse-iteration and hardware-specific optimizations. We show an automatic compilation technique from a high-level language to SAM and a set of hardware primitives which implement it. We evaluate the generality and extensibility of our sparse abstract machine, explore the performance space of sparse tensor algebra optimizations using SAM, and provide an example implementation of our SAM architecture.},
	urldate = {2022-09-07},
	publisher = {arXiv},
	author = {Hsu, Olivia and Strange, Maxwell and Won, Jaeyeon and Sharma, Ritvik and Olukotun, Kunle and Emer, Joel and Horowitz, Mark and Kjolstad, Fredrik},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14610 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/PRGGWMYR/Hsu et al. - 2022 - The Sparse Abstract Machine.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/JTEDVSVC/2208.html:text/html},
}

@inproceedings{thies_manipulating_2009,
	address = {New York, NY, USA},
	series = {{MM} '09},
	title = {Manipulating lossless video in the compressed domain},
	isbn = {978-1-60558-608-3},
	url = {https://doi.org/10.1145/1631272.1631319},
	doi = {10.1145/1631272.1631319},
	abstract = {A compressed-domain transformation is one that operates directly on the compressed format, rather than requiring conversion to an uncompressed format prior to processing. Performing operations in the compressed domain offers large speedups, as it reduces the volume of data processed and avoids the overhead of re-compression. While previous researchers have focused on compressed-domain techniques for lossy data formats, there are few techniques that apply to lossless formats. In this paper, we present a general technique for transforming lossless data as compressed with the sliding-window Lempel Ziv algorithm (LZ77). We focus on applications in video editing, where our technique supports color adjustment, video compositing, and other operations directly on the Apple Animation format (a variant of LZ77). We implemented a subset of our technique as an automatic program transformation. Using the StreamIt language, users write a program to operate on uncompressed data, and our compiler transforms the program to operate on compressed data. Experiments show that the technique offers speedups roughly proportional to the compression factor. For our benchmark suite of 12 videos in Apple Animation format, speedups range from 1.1x to 471x, with a median of 15x.},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 17th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Thies, William and Hall, Steven and Amarasinghe, Saman},
	month = oct,
	year = {2009},
	keywords = {compressed domain, Lempel-Ziv, lossless compression, LZ77, stream programming, streamit, synchronous dataflow, video editing},
	pages = {331--340},
	file = {Full Text PDF:/Users/willow/Zotero/storage/RBR3QRBI/Thies et al. - 2009 - Manipulating lossless video in the compressed doma.pdf:application/pdf},
}

@inproceedings{pugh_sipr_1999,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SIPR}: {A} {New} {Framework} for {Generating} {Efficient} {Code} for {Sparse} {Matrix} {Computations}},
	isbn = {978-3-540-48319-9},
	shorttitle = {{SIPR}},
	doi = {10.1007/3-540-48319-5_14},
	abstract = {Developing computational codes that compute with sparse matrices is a difficult and error-prone process. Automatic generation of sparse code from the corresponding dense version would simplify the programmer’s task, provided that a compiler-generated code is fast enough to be used instead of a hand-written code. We propose a new Sparse Intermediate Program Representation (SIPR) that separates the issue of maintaining complicated data structures from the actual matrix computations to be performed. Cost analysis of SIPR allows for the prediction of the program efficiency, and provides a solid basis for choosing efficient sparse implementations among many possible ones. The SIPR framework allows the use of techniques that are frequently used in the hand-written codes but previously were not considered for compiler-generated codes due to their complexity. We have developed tools that allow the automatic generation of efficient C++ implementations from SIPR, and describe experimental results on the performance of those implementations.},
	language = {en},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer},
	author = {Pugh, William and Shpeisman, Tatiana},
	editor = {Chatterjee, Siddhartha and Prins, Jan F. and Carter, Larry and Ferrante, Jeanne and Li, Zhiyuan and Sehr, David and Yew, Pen-Chung},
	year = {1999},
	keywords = {Array Reference, Element Store, Partial Pivoting, Sparse Code, Sparse Matrix},
	pages = {213--229},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FFJV8593/Pugh and Shpeisman - 1999 - SIPR A New Framework for Generating Efficient Cod.pdf:application/pdf},
}

@misc{yadav_spdistal_2022,
	title = {{SpDISTAL}: {Compiling} {Distributed} {Sparse} {Tensor} {Computations}},
	shorttitle = {{SpDISTAL}},
	url = {http://arxiv.org/abs/2207.13901},
	abstract = {We introduce SpDISTAL, a compiler for sparse tensor algebra that targets distributed systems. SpDISTAL combines separate descriptions of tensor algebra expressions, sparse data structures, data distribution, and computation distribution. Thus, it enables distributed execution of sparse tensor algebra expressions with a wide variety of sparse data structures and data distributions. SpDISTAL is implemented as a C++ library that targets a distributed task-based runtime system and can generate code for nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates distributed code that achieves performance competitive with hand-written distributed functions for speciﬁc sparse tensor algebra expressions and that outperforms general interpretation-based systems by one to two orders of magnitude.},
	language = {en},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13901 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
	file = {Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:/Users/willow/Zotero/storage/RIG6L6EJ/Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:application/pdf;Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:/Users/willow/Zotero/storage/HG5I22L3/Yadav et al. - 2022 - SpDISTAL Compiling Distributed Sparse Tensor Comp.pdf:application/pdf},
}

@inproceedings{pizzuti_generating_2021,
	address = {New York, NY, USA},
	series = {{FHPNC} 2021},
	title = {Generating high performance code for irregular data structures using dependent types},
	isbn = {978-1-4503-8614-2},
	url = {https://doi.org/10.1145/3471873.3472977},
	doi = {10.1145/3471873.3472977},
	abstract = {Parallel architectures offer high performance but are challenging to program. Data parallel functional languages offer a solution by providing a high-level programming model to work with accelerators such as GPUs. Existing languages are designed to work with dense arrays, limiting their usefulness in expressing irregular data structures, such as graphs and sparse matrices important in many application domains. This paper addresses this limitation by extending a data-parallel language with limited dependent types, including position dependent arrays and dependent pairs to model irregular data structures. The approach is demonstrated through three case studies: dense to sparse matrix conversion, sparse matrix-vector multiplication, and parallel breadth-first search. Experimental results show that this approach outperforms state-of-the-art implementations on GPUs. Compared to Nvidia’s cuSparse, our automatically generated code achieves an average speedup of 1.2× for dense to sparse matrix conversion and 1.3× for sparse matrix-vector multiplication.},
	urldate = {2022-09-01},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN} {International} {Workshop} on {Functional} {High}-{Performance} and {Numerical} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Pizzuti, Federico and Steuwer, Michel and Dubach, Christophe},
	month = aug,
	year = {2021},
	keywords = {Dependent Types, Irregular Data Structures},
	pages = {37--49},
	file = {Full Text PDF:/Users/willow/Zotero/storage/9WPP87JG/Pizzuti et al. - 2021 - Generating high performance code for irregular dat.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, MNIST, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRS5H7PF/726791.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/V69R5ZYM/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{eitz_how_2012,
	title = {How do humans sketch objects?},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2185520.2185540},
	doi = {10.1145/2185520.2185540},
	abstract = {Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73\% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56\% accuracy (chance is 0.4\%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.},
	number = {4},
	urldate = {2022-08-31},
	journal = {ACM Transactions on Graphics},
	author = {Eitz, Mathias and Hays, James and Alexa, Marc},
	month = jul,
	year = {2012},
	keywords = {crowd-sourcing, learning, recognition, sketch},
	pages = {44:1--44:10},
	file = {Full Text PDF:/Users/willow/Zotero/storage/VNICQZRI/Eitz et al. - 2012 - How do humans sketch objects.pdf:application/pdf},
}

@misc{ahrens_optimal_2021,
	title = {On {Optimal} {Partitioning} {For} {Sparse} {Matrices} {In} {Variable} {Block} {Row} {Format}},
	url = {http://arxiv.org/abs/2005.12414},
	doi = {10.48550/arXiv.2005.12414},
	abstract = {The Variable Block Row (VBR) format is an influential blocked sparse matrix format designed for matrices with shared sparsity structure between adjacent rows and columns. VBR groups adjacent rows and columns, storing the resulting blocks that contain nonzeros in a dense format. This reduces the memory footprint and enables optimizations such as register blocking and instruction-level parallelism. Existing approaches use heuristics to determine which rows and columns should be grouped together. We show that finding the optimal grouping of rows and columns for VBR is NP-hard under several reasonable cost models. In light of this finding, we propose a 1-dimensional variant of VBR, called 1D-VBR, which achieves better performance than VBR by only grouping rows. We describe detailed cost models for runtime and memory consumption. Then, we describe a linear time dynamic programming solution for optimally grouping the rows for 1D-VBR format. We extend our algorithm to produce a heuristic VBR partitioner which alternates between optimally partitioning rows and columns, assuming the columns or rows to be fixed, respectively. Our alternating heuristic produces VBR matrices with the smallest memory footprint of any partitioner we tested.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Ahrens, Willow and Boman, Erik G.},
	month = may,
	year = {2021},
	note = {arXiv:2005.12414 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/BWHWXSNS/Ahrens and Boman - 2021 - On Optimal Partitioning For Sparse Matrices In Var.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/E63KTTB9/2005.html:text/html},
}

@inproceedings{backus_fortran_1957,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '57 ({Western})},
	title = {The {FORTRAN} automatic coding system},
	isbn = {978-1-4503-7861-1},
	url = {https://doi.org/10.1145/1455567.1455599},
	doi = {10.1145/1455567.1455599},
	abstract = {The FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM's next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan. The goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.},
	urldate = {2022-08-04},
	booktitle = {Papers presented at the {February} 26-28, 1957, western joint computer conference: {Techniques} for reliability},
	publisher = {Association for Computing Machinery},
	author = {Backus, J. W. and Beeber, R. J. and Best, S. and Goldberg, R. and Haibt, L. M. and Herrick, H. L. and Nelson, R. A. and Sayre, D. and Sheridan, P. B. and Stern, H. and Ziller, I. and Hughes, R. A. and Nutt, R.},
	month = feb,
	year = {1957},
	pages = {188--198},
	file = {Full Text PDF:/Users/willow/Zotero/storage/SUHHBWFU/Backus et al. - 1957 - The FORTRAN automatic coding system.pdf:application/pdf},
}

@misc{ye_sparsetir_2022,
	title = {{SparseTIR}: {Composable} {Abstractions} for {Sparse} {Compilation} in {Deep} {Learning}},
	shorttitle = {{SparseTIR}},
	url = {http://arxiv.org/abs/2207.04606},
	doi = {10.48550/arXiv.2207.04606},
	abstract = {Sparse tensors are rapidly becoming critical components of modern deep learning workloads. However, developing high-performance sparse operators can be difficult and tedious, and existing vendor libraries cannot satisfy the escalating demands from new operators. Sparse tensor compilers simplify the development of operators, but efficient sparse compilation for deep learning remains challenging because a single sparse format cannot maximize hardware efficiency, and single-shot compilers cannot keep up with latest hardware and system advances. We show that the key to addressing both challenges is two forms of composability. In this paper, we propose SparseTIR, a sparse tensor compilation abstraction that offers composable formats and composable transformations for deep learning workloads. SparseTIR constructs a search space over these composable components for performance tuning. With these improvements, SparseTIR obtains consistent performance speedups vs vendor libraries on GPUs for single operators: 1.1-3.3x for GNN operators and 1.1-4.4x for sparse transformer operators. SparseTIR also accelerates end-to-end GNNs by 1.1-2.2x for GraphSAGE training and 0.9-26x for RGCN inference.},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Ye, Zihao and Lai, Ruihang and Shao, Junru and Chen, Tianqi and Ceze, Luis},
	month = jul,
	year = {2022},
	note = {arXiv:2207.04606 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/7FRG42DC/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf;arXiv Fulltext PDF:/Users/willow/Zotero/storage/LUXDW9YS/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/GP857R84/2207.html:text/html;Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:/Users/willow/Zotero/storage/UE9AVP2B/Ye et al. - 2022 - SparseTIR Composable Abstractions for Sparse Comp.pdf:application/pdf},
}

@inproceedings{donenfeld_unified_2022,
	title = {Unified {Compilation} for {Lossless} {Compression} and {Sparse} {Computing}},
	doi = {10.1109/CGO53902.2022.9741282},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to support lossless compression techniques, including variants of run-length encoding and Lempel-Ziv compression. We develop new abstractions to represent losslessly compressed data as a generalized form of sparse tensors, with repetitions of values (which are compressed out in storage) represented by non-scalar, dynamic fill values. We then show how a compiler can use these abstractions to emit efficient code that computes on losslessly compressed data. By unifying lossless compression with sparse tensor algebra, our technique is able to generate code that computes with both losslessly compressed data and sparse data, as well as generate code that computes directly on compressed data without needing to first decompress it.Our evaluation shows our technique generates efficient image and video processing kernels that compute on losslessly compressed data. We find that the generated kernels are up to 16. 3× faster than equivalent dense kernels generated by TACO, a tensor algebra compiler, and up to 16. 1× faster than OpenCV, a widely used image processing library.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Donenfeld, Daniel and Chou, Stephen and Amarasinghe, Saman},
	month = apr,
	year = {2022},
	keywords = {Algebra, Codes, compressed domain processing, Computational efficiency, Image coding, Kernel, Libraries, lossless compression, sparse tensor algebra, Tensors},
	pages = {205--216},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/7FMZUU5R/9741282.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/JLWQKCNS/Donenfeld et al. - 2022 - Unified Compilation for Lossless Compression and S.pdf:application/pdf},
}

@article{hagedorn_achieving_2020,
	title = {Achieving high-performance the functional way: a functional pearl on expressing high-performance optimizations as rewrite strategies},
	volume = {4},
	issn = {2475-1421},
	shorttitle = {Achieving high-performance the functional way},
	url = {https://dl.acm.org/doi/10.1145/3408974},
	doi = {10.1145/3408974},
	abstract = {In this functional pearl, we show how to employ functional programming techniques to solve this challenge with elegance. We present two functional languages that work together - each addressing a separate concern. RISE is a functional language for expressing computations using well known functional data-parallel patterns. ELEVATE is a functional language for describing optimization strategies. A high-level RISE program is transformed into a low-level form using optimization strategies written in ELEVATE. From the rewritten low-level program high-performance parallel code is automatically generated. In contrast to existing high-performance domain-specific systems with scheduling APIs, in our approach programmers are not restricted to a set of built-in operations and optimizations but freely define their own computational patterns in RISE and optimization strategies in ELEVATE in a composable and reusable way. We show how our holistic functional approach achieves competitive performance with the state-of-the-art imperative systems Halide and TVM. CCS Concepts: • Software and its engineering → Functional languages; Compilers; • Theory of computation → Rewrite systems.},
	language = {en},
	number = {ICFP},
	urldate = {2022-04-29},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Hagedorn, Bastian and Lenfers, Johannes and Kœhler, Thomas and Qin, Xueying and Gorlatch, Sergei and Steuwer, Michel},
	month = aug,
	year = {2020},
	pages = {1--29},
	file = {Hagedorn et al. - 2020 - Achieving high-performance the functional way a f.pdf:/Users/willow/Zotero/storage/E673JW72/Hagedorn et al. - 2020 - Achieving high-performance the functional way a f.pdf:application/pdf},
}

@article{psarras_landscape_2021,
	title = {The landscape of software for tensor computations},
	url = {http://arxiv.org/abs/2103.13756},
	abstract = {Tensors (also commonly seen as multi-linear operators or as multi-dimensional arrays) are ubiquitous in scientiﬁc computing and in data science, and so are the software eﬀorts for tensor operations. Particularly in recent years, we have observed an explosion in libraries, compilers, packages, and toolboxes; unfortunately these eﬀorts are very much scattered among the diﬀerent scientiﬁc domains, and inevitably suﬀer from replication, suboptimal implementations, and in many cases, limited visibility. As a ﬁrst step towards countering these ineﬃciencies, here we survey and loosely classify software packages related to tensor computations. Our aim is to assemble a comprehensive and up-to-date snapshot of the tensor software landscape, with the intention of helping both users and developers. Aware of the diﬃculties inherent in any multi-discipline survey, we very much welcome the reader’s help in amending and expanding our software list, which currently features 77 projects.},
	language = {en},
	urldate = {2022-04-29},
	journal = {arXiv:2103.13756 [cs]},
	author = {Psarras, Christos and Karlsson, Lars and Li, Jiajia and Bientinesi, Paolo},
	month = may,
	year = {2021},
	note = {arXiv: 2103.13756},
	keywords = {Computer Science - Mathematical Software},
	file = {Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:/Users/willow/Zotero/storage/9FYH3IWU/Psarras et al. - 2021 - The landscape of software for tensor computations.pdf:application/pdf},
}

@inproceedings{ahrens_autoscheduling_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Autoscheduling for sparse tensor algebra with an asymptotic cost model},
	copyright = {All rights reserved},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523442},
	doi = {10.1145/3519939.3523442},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small Pareto frontier of asymptotically non-dominating kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = jun,
	year = {2022},
	keywords = {Asymptotic Analysis, Automatic Scheduling, Compilers, Conjunctive Query Containment, Query Optimization, Sparse Tensors},
	pages = {269--285},
	file = {Full Text PDF:/Users/willow/Zotero/storage/V6PAKBZT/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/QDCN3ULM/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf},
}

@article{hu_taichi_2019,
	title = {Taichi: a language for high-performance computation on spatially sparse data structures},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {Taichi},
	url = {https://doi.org/10.1145/3355089.3356506},
	doi = {10.1145/3355089.3356506},
	abstract = {3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multi-level sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose Taichi, a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This decoupling of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With 1{\textless}u{\textgreater}1{\textless}/u{\textgreater}0 th as many lines of code, we achieve 4.55× higher performance on average, compared to hand-optimized reference implementations.},
	number = {6},
	urldate = {2020-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Hu, Yuanming and Li, Tzu-Mao and Anderson, Luke and Ragan-Kelley, Jonathan and Durand, Frédo},
	month = nov,
	year = {2019},
	keywords = {GPU computing, sparse data structures},
	pages = {201:1--201:16},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LI4HK8PK/Hu et al. - 2019 - Taichi a language for high-performance computatio.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Number: 7825
Publisher: Nature Publishing Group},
	pages = {357--362},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZFNZYPK/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/IFYNZTWL/s41586-020-2649-2.html:text/html},
}

@article{langr_evaluation_2016,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	volume = {27},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2015.2401575},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langr, Daniel and Tvrdík, Pavel},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {sparse matrix-vector multiplication, Sparse matrices, Runtime, parallel processing, Indexes, matrix-vector multiplication, sparse matrix, Matrix converters, mathematics computing, test matrices, Standards, vectors, storage format, matrix algebra, Memory management, evaluation criteria, Evaluation criterion, FLOPS, HPC application, memory footprint, nonzero matrix structure, single evaluation criterion, sparse matrix storage formats, SpMV-based application phase},
	pages = {428--440},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRZMJ4TJ/7036061.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/D75SXF9Z/7036061.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/8IRGEGDG/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/I3FNKNCF/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf},
}

@article{senanayake_sparse_2020,
	title = {A sparse iteration space transformation framework for sparse tensor algebra},
	volume = {4},
	url = {https://doi.org/10.1145/3428226},
	doi = {10.1145/3428226},
	abstract = {We address the problem of optimizing sparse tensor algebra in a compiler and show how to define standard loop transformations---split, collapse, and reorder---on sparse iteration spaces. The key idea is to track the transformation functions that map the original iteration space to derived iteration spaces. These functions are needed by the code generator to emit code that maps coordinates between iteration spaces at runtime, since the coordinates in the sparse data structures remain in the original iteration space. We further demonstrate that derived iteration spaces can tile both the universe of coordinates and the subset of nonzero coordinates: the former is analogous to tiling dense iteration spaces, while the latter tiles sparse iteration spaces into statically load-balanced blocks of nonzeros. Tiling the space of nonzeros lets the generated code efficiently exploit heterogeneous compute resources such as threads, vector units, and GPUs. We implement these concepts by extending the sparse iteration theory implementation in the TACO system. The associated scheduling API can be used by performance engineers or it can be the target of an automatic scheduling system. We outline one heuristic autoscheduling system, but other systems are possible. Using the scheduling API, we show how to optimize mixed sparse-dense tensor algebra expressions on CPUs and GPUs. Our results show that the sparse transformations are sufficient to generate code with competitive performance to hand-optimized implementations from the literature, while generalizing to all of the tensor algebra.},
	number = {OOPSLA},
	urldate = {2021-11-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Senanayake, Ryan and Hong, Changwan and Wang, Ziheng and Wilson, Amalee and Chou, Stephen and Kamil, Shoaib and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = nov,
	year = {2020},
	keywords = {Optimizing Transformations, Sparse Iteration Spaces, Sparse Tensor Algebra},
	pages = {158:1--158:30},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6WCKXU8K/Senanayake et al. - 2020 - A sparse iteration space transformation framework .pdf:application/pdf},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	url = {https://doi.org/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
	number = {OOPSLA},
	urldate = {2021-11-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	keywords = {Compilation, Sparse Array Programming, Sparse Arrays},
	pages = {128:1--128:29},
	file = {Full Text PDF:/Users/willow/Zotero/storage/724NU8UX/Henry et al. - 2021 - Compilation of sparse array programming models.pdf:application/pdf},
}

@phdthesis{kotlyar_relational_1999,
	type = {{PhD} {Thesis}},
	title = {Relational {Algebraic} {Techniques} for the {Synthesis} of {Sparse} {Matrix} {Programs}},
	school = {Cornell},
	author = {Kotlyar, Vladimir},
	year = {1999},
	note = {00000},
}

@techreport{kotlyar_compiling_1997,
	title = {Compiling parallel sparse code for user-defined data structures},
	institution = {Cornell},
	author = {Kotlyar, Vladimir and Pingali, Keshav and Stodghill, Paul},
	year = {1997},
	note = {00000},
}

@inproceedings{liu_csr5_2015,
	address = {New York, NY, USA},
	series = {{ICS} '15},
	title = {{CSR5}: {An} {Efficient} {Storage} {Format} for {Cross}-{Platform} {Sparse} {Matrix}-{Vector} {Multiplication}},
	isbn = {978-1-4503-3559-1},
	shorttitle = {{CSR5}},
	url = {https://doi.org/10.1145/2751205.2751209},
	doi = {10.1145/2751205.2751209},
	abstract = {Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations. We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6\%, 28.5\%, 173.0\% and 293.3\% (up to 213.3\%, 153.6\%, 405.1\% and 943.3\%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the 29th {ACM} on {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Weifeng and Vinter, Brian},
	month = jun,
	year = {2015},
	keywords = {cpu, csr, csr5, gpu, sparse matrices, spmv, storage formats, xeon phi},
	pages = {339--350},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6PDCWARD/Liu and Vinter - 2015 - CSR5 An Efficient Storage Format for Cross-Platfo.pdf:application/pdf},
}

@article{kjolstad_tensor_2017,
	title = {The {Tensor} {Algebra} {Compiler}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2019-11-20},
	journal = {Proc. ACM Program. Lang.},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {parallelism, code generation, linear algebra, tensors, performance, sparse data structures, iteration graphs, merge lattices, tensor algebra},
	pages = {77:1--77:29},
	file = {ACM Full Text PDF:/Users/willow/Zotero/storage/M3YTNCUI/Kjolstad et al. - 2017 - The Tensor Algebra Compiler.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/EZXEWXYJ/Kjolstad et al. - 2017 - The tensor algebra compiler.pdf:application/pdf;kjolstad-oopsla17-taco-preprint.pdf:/Users/willow/Zotero/storage/XUPL86I6/kjolstad-oopsla17-taco-preprint.pdf:application/pdf},
}

@inproceedings{vuduc_performance_2002,
	title = {Performance {Optimizations} and {Bounds} for {Sparse} {Matrix}-{Vector} {Multiply}},
	doi = {10.1109/SC.2002.10025},
	abstract = {We consider performance tuning, by code and data structure reorganization, of sparse matrix-vector multiply (SpM×V), one of the most important computational kernels in scientific applications. This paper addresses the fundamental questions of what limits exist on such performance tuning, and how closely tuned code approaches these limits. Specifically, we develop upper and lower bounds on the performance (Mflop/s) of SpM×V when tuned using our previously proposed register blocking optimization. These bounds are based on the non-zero pattern in the matrix and the cost of basic memory operations, such as cache hits and misses. We evaluate our tuned implementations with respect to these bounds using hardware counter data on 4 different platforms and on test set of 44 sparse matrices. We find that we can often get within 20\% of the upper bound, particularly on class of matrices from finite element modeling (FEM) problems; on non-FEM matrices, performance improvements of 2× are still possible. Lastly, we present new heuristic that selects optimal or near-optimal register block sizes (the key tuning parameters) more accurately than our previous heuristic. Using the new heuristic, we show improvements in SpM×V performance (Mflop/s) by as much as 2.5× over an untuned implementation. Collectively, our results suggest that future performance improvements, beyond those that we have already demonstrated for SpM×V, will come from two sources: (1) consideration of higher-level matrix structures (e.g. exploiting symmetry, matrix reordering, multiple register block sizes), and (2) optimizing kernels with more opportunity for data reuse (e.g. sparse matrix-multiple vector multiply, multiplication of AT A by a vector).},
	booktitle = {{SC} '02: {Proceedings} of the 2002 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Vuduc, R. and Demmel, J.W. and Yelick, K.A. and Kamil, S. and Nishtala, R. and Lee, B.},
	month = nov,
	year = {2002},
	note = {ISSN: 1063-9535},
	keywords = {Costs, Counting circuits, Data structures, Hardware, Kernel, Optimization, Registers, Sparse matrices, Testing, Upper bound},
	pages = {26--26},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/NB49QD9N/1592862.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/BLUY9Q7P/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf;Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:/Users/willow/Zotero/storage/EIBMBAFB/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf},
}

@inproceedings{ragan-kelley_halide_2013,
	address = {New York, NY, USA},
	series = {{PLDI} '13},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Halide},
	url = {http://doi.org/10.1145/2491956.2462176},
	doi = {10.1145/2491956.2462176},
	abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Frédo and Amarasinghe, Saman},
	month = jun,
	year = {2013},
	keywords = {autotuning, parallelism, gpu, optimization, vectorization, locality, compiler, image processing, domain specific language, redundant computation},
	pages = {519--530},
	file = {Full Text PDF:/Users/willow/Zotero/storage/QZF9XYFW/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@inproceedings{bik_automatic_1994,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On automatic data structure selection and code generation for sparse computations},
	isbn = {978-3-540-48308-3},
	doi = {10.1007/3-540-57659-2_4},
	abstract = {Traditionally restructuring compilers were only able to apply program transformations in order to exploit certain characteristics of the target architecture. Adaptation of data structures was limited to e.g. linearization or transposing of arrays. However, as more complex data structures are required to exploit characteristics of the data operated on, current compiler support appears to be inappropriate. In this paper we present the implementation issues of a restructuring compiler that automatically converts programs operating on dense matrices into sparse code, i.e. after a suited data structure has been selected for every dense matrix that in fact is sparse, the original code is adapted to operate on these data structures. This simplifies the task of the programmer and, in general, enables the compiler to apply more optimizations.},
	language = {en},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer},
	author = {Bik, Aart J. C. and Wijshoff, Harry A. G.},
	editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	year = {1994},
	keywords = {Restructuring Compilers, Sparse Computations, Sparse Matrices},
	pages = {57--75},
	file = {Full Text PDF:/Users/willow/Zotero/storage/N9BR5TBS/Bik and Wijshoff - 1994 - On automatic data structure selection and code gen.pdf:application/pdf},
}

@inproceedings{bik_compilation_1993,
	address = {New York, NY, USA},
	series = {{ICS} '93},
	title = {Compilation techniques for sparse matrix computations},
	isbn = {978-0-89791-600-4},
	url = {http://doi.org/10.1145/165939.166023},
	doi = {10.1145/165939.166023},
	abstract = {The problem of compiler optimization of sparse codes is well known and no satisfactory solutions have been found yet. One of the major obstacles is formed by the fact that sparse programs deal explicitly with the particular data structures selected for storing sparse matrices. This explicit data structure handling obscures the functionality of a code to such a degree that the optimization of the code is prohibited, e.g. by the introduction of indirect addressing. The method presented in this paper postpones data structure selection until the compile phase, thereby allowing the compiler to combine code optimization with explicit data structure selection. Not only enables this method the compiler to generate efficient code for sparse computations, also the task of the programmer is greatly reduced in complexity.},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 7th international conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Bik, Aart J. C. and Wijshoff, Harry A. G.},
	month = aug,
	year = {1993},
	keywords = {compilation techniques, optimization, program transformations, restructuring compilers, sparse computations, sparse matrices},
	pages = {416--424},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LZCICPAR/Bik and Wijshoff - 1993 - Compilation techniques for sparse matrix computati.pdf:application/pdf},
}

@inproceedings{kotlyar_relational_1997,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A relational approach to the compilation of sparse matrix programs},
	isbn = {978-3-540-69549-3},
	doi = {10.1007/BFb0002751},
	abstract = {We present a relational algebra based framework for compiling efficient sparse matrix code from dense DO-ANY loops and a specification of the representation of the sparse matrix. We present experimental data that demonstrates that the code generated by our compiler achieves performance competitive with that of hand-written codes for important computational kernels.},
	language = {en},
	booktitle = {Euro-{Par}'97 {Parallel} {Processing}},
	publisher = {Springer},
	author = {Kotlyar, Vladimir and Pingali, Keshav and Stodghill, Paul},
	editor = {Lengauer, Christian and Griebl, Martin and Gorlatch, Sergei},
	year = {1997},
	keywords = {Loop Nest, Relational Query, Sparse Code, Sparse Matrix, Storage Format},
	pages = {318--327},
	file = {Springer Full Text PDF:/Users/willow/Zotero/storage/3JWGLXRU/Kotlyar et al. - 1997 - A relational approach to the compilation of sparse.pdf:application/pdf;Springer Full Text PDF:/Users/willow/Zotero/storage/QAKRP5KM/Kotlyar et al. - 1997 - A relational approach to the compilation of sparse.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	volume = {17},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	language = {en},
	number = {3},
	urldate = {2021-11-19},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = mar,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biophysical chemistry;Computational biology and bioinformatics;Technology
Subject\_term\_id: biophysical-chemistry;computational-biology-and-bioinformatics;technology},
	keywords = {Biophysical chemistry, Computational biology and bioinformatics, Technology},
	pages = {261--272},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2Q9W83VV/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/V7KB63J9/s41592-019-0686-2.html:text/html},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	urldate = {2021-11-19},
	booktitle = {12th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 16)},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Q4MBXVX8/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@inproceedings{williams_optimization_2007,
	address = {New York, NY, USA},
	series = {{SC} '07},
	title = {Optimization of sparse matrix-vector multiplication on emerging multicore platforms},
	isbn = {978-1-59593-764-3},
	url = {http://doi.org/10.1145/1362622.1362674},
	doi = {10.1145/1362622.1362674},
	abstract = {We are witnessing a dramatic change in computer architecture due to the multicore paradigm shift, as every electronic device from cell phones to supercomputers confronts parallelism of unprecedented scale. To fully unleash the potential of these systems, the HPC community must develop multicore specific optimization methodologies for important scientific computations. In this work, we examine sparse matrix-vector multiply (SpMV) - one of the most heavily used kernels in scientific computing - across a broad spectrum of multicore designs. Our experimental platform includes the homogeneous AMD dual-core and Intel quad-core designs, the heterogeneous STI Cell, as well as the first scientific study of the highly multithreaded Sun Niagara2. We present several optimization strategies especially effective for the multicore environment, and demonstrate significant performance improvements compared to existing state-of-the-art serial and parallel SpMV implementations. Additionally, we present key insights into the architectural tradeoffs of leading multicore design strategies, in the context of demanding memory-bound numerical algorithms.},
	urldate = {2022-03-15},
	booktitle = {Proceedings of the 2007 {ACM}/{IEEE} conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Williams, Samuel and Oliker, Leonid and Vuduc, Richard and Shalf, John and Yelick, Katherine and Demmel, James},
	month = nov,
	year = {2007},
	keywords = {Sparse matrices, Supercomputers, Kernel, Multicore processing, Parallel processing, Computer architecture, Scientific computing, Cellular phones, Optimization methods, Sun},
	pages = {1--12},
	file = {Full Text PDF:/Users/willow/Zotero/storage/G4VQIMPC/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/P7C29HUU/5348797.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/XVTJHNHK/5348797.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/MLDMCFE7/Williams et al. - 2007 - Optimization of sparse matrix-vector multiplicatio.pdf:application/pdf},
}

@inproceedings{solomonik_cyclops_2013,
	title = {Cyclops {Tensor} {Framework}: {Reducing} {Communication} and {Eliminating} {Load} {Imbalance} in {Massively} {Parallel} {Contractions}},
	shorttitle = {Cyclops {Tensor} {Framework}},
	doi = {10.1109/IPDPS.2013.112},
	abstract = {Cyclops (cyclic-operations) Tensor Framework (CTF) 1 is a distributed library for tensor contractions. CTF aims to scale high-dimensional tensor contractions such as those required in the Coupled Cluster (CC) electronic structure method to massively-parallel supercomputers. The framework preserves tensor structure by subdividing tensors cyclically, producing a regular parallel decomposition. An internal virtualization layer provides completely general mapping support while maintaining ideal load balance. The mapping framework decides on the best mapping for each tensor contraction at run-time via explicit calculations of memory usage and communication volume. CTF employs a general redistribution kernel, which transposes tensors of any dimension between arbitrary distributed layouts, yet touches each piece of data only once. Sequential symmetric contractions are reduced to matrix multiplication calls via tensor index transpositions and partial unpacking. The user-level interface elegantly expresses arbitrary-dimensional generalized tensor contractions in the form of a domain specific language. We demonstrate performance of CC with single and double excitations on 8192 nodes of Blue Gene/Q and show that CTF outperforms NWChem on Cray XE6 supercomputers for benchmarked systems.},
	booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
	author = {Solomonik, Edgar and Matthews, Devin and Hammond, Jeff and Demmel, James},
	month = may,
	year = {2013},
	note = {ISSN: 1530-2075},
	keywords = {Chemistry, Clustering algorithms, communication-avoiding algorithms, Coupled Cluster, Cyclops, Equations, Indexes, Manganese, Program processors, Tensile stress, tensor contractions},
	pages = {813--824},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/F5CY8QZN/6569864.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/X25BVLFH/Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:application/pdf;Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:/Users/willow/Zotero/storage/9LFL6LHL/Solomonik et al. - 2013 - Cyclops Tensor Framework Reducing Communication a.pdf:application/pdf},
}

@article{zhou_enabling_2020,
	title = {Enabling {Runtime} {SpMV} {Format} {Selection} through an {Overhead} {Conscious} {Method}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2932931},
	abstract = {Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix; it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give.},
	number = {1},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhou, Weijie and Zhao, Yue and Shen, Xipeng and Chen, Wang},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Buildings, high performance computing, Kernel, Matrix converters, prediction model, Predictive models, program optimization, Runtime, Sparse matrices, sparse matrix format, SpMV, Time series analysis},
	pages = {80--93},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/6S64R2T2/8787872.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/PTUF7P8M/Zhou et al. - 2020 - Enabling Runtime SpMV Format Selection through an .pdf:application/pdf},
}

@inproceedings{kjolstad_tensor_2019,
	title = {Tensor {Algebra} {Compilation} with {Workspaces}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1109/CGO.2019.8661185},
	doi = {10.1109/CGO.2019.8661185},
	abstract = {This paper shows how to extend sparse tensor algebra compilers to introduce temporary tensors called workspaces to avoid inefficient sparse data structures accesses. We develop an intermediate representation (IR) for tensor operations called concrete index notation that specifies when sub-computations occur and where they are stored. We then describe the workspace transformation in this IR, how to programmatically invoke it, and how the IR is compiled to sparse code. Finally, we show how the transformation can be used to optimize sparse tensor kernels, including sparse matrix multiplication, sparse tensor addition, and the matricized tensor times Khatri-Rao product (MTTKRP). Our results show that the workspace transformation brings the performance of these kernels on par with hand-optimized implementations. For example, we improve the performance of MTTKRP with dense output by up to 35\%, and enable generating sparse matrix multiplication and MTTKRP with sparse output, neither of which were supported by prior tensor algebra compilers.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Kjolstad, Fredrik and Ahrens, Willow and Kamil, Shoaib and Amarasinghe, Saman},
	month = feb,
	year = {2019},
	keywords = {Algebra, Arrays, code optimization, compiler IR, concrete index notation, data structures, Indexes, intermediate representation, Kernel, matricized tensor times Khatri-Rao product, matrix decomposition, matrix multiplication, program compilers, sparse data structures, sparse matrices, Sparse matrices, sparse matrix multiplication, sparse tensor addition, sparse tensor algebra, sparse tensor algebra compilers, sparse tensor kernels, temporaries, tensors, workspaces},
	pages = {180--192},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FCYUD2HP/Kjolstad et al. - 2019 - Tensor algebra compilation with workspaces.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/SAZD6MM9/8661185.html:text/html;Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:/Users/willow/Zotero/storage/K5Z25DFE/Kjolstad et al. - 2019 - Tensor Algebra Compilation with Workspaces.pdf:application/pdf},
}

@article{fegade_cora_2021,
	title = {The {CoRa} {Tensor} {Compiler}: {Compilation} for {Ragged} {Tensors} with {Minimal} {Padding}},
	shorttitle = {The {CoRa} {Tensor} {Compiler}},
	url = {http://arxiv.org/abs/2110.10221},
	abstract = {There is often variation in the shape and size of input data used for deep learning. In many cases, such data can be represented using tensors with non-uniform shapes, or ragged tensors. Due to limited and non-portable support for efﬁcient execution on ragged tensors, current deep learning frameworks generally use techniques such as padding and masking to make the data shapes uniform and then ofﬂoad the computations to optimized kernels for dense tensor algebra. Such techniques can, however, lead to a lot of wasted computation and therefore, a loss in performance. This paper presents CORA, a tensor compiler that allows users to easily generate efﬁcient code for ragged tensor operators targeting a wide range of CPUs and GPUs. Evaluating CORA on a variety of operators on ragged tensors as well as on an encoder layer of the transformer model, we ﬁnd that CORA (i) performs competitively with hand-optimized implementations of the operators and the transformer encoder and (ii) achieves, over PyTorch, a 1.6× geomean speedup for the encoder on an Nvidia GPU and a 1.86× geomean speedup for the multi-head attention module used in transformers on an ARM CPU.},
	language = {en},
	urldate = {2021-11-26},
	journal = {arXiv:2110.10221 [cs]},
	author = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip B. and Mowry, Todd C.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.10221},
	keywords = {Computer Science - Machine Learning},
	file = {Fegade et al. - 2021 - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/6CRTC4AC/Fegade et al. - 2021 - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf},
}

@article{liu_intermediate_2023,
	title = {An {Intermediate} {Language} for {General} {Sparse} {Format} {Customization}},
	issn = {1556-6064},
	doi = {10.1109/LCA.2023.3262610},
	abstract = {The inevitable trend of hardware specialization drives an increasing use of custom data formats in processing sparse workloads, which are typically memory-bound. These formats facilitate the automated generation of target-aware data layouts to improve memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Moreover, since these frameworks adopt an attribute-based approach for format abstraction, they cannot easily be extended to support general format customization. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. More concretely, we express a sparse format as a map from dense coordinates to a layout tree using a small set of well-defined query and mutation primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with hybrid formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, and a simulated processing-in-memory (PIM) device.},
	journal = {IEEE Computer Architecture Letters},
	author = {Liu, Jie and Zhao, Zhongyuan and Ding, Zijian and Brock, Benjamin and Rong, Hongbo and Zhang, Zhiru},
	year = {2023},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Codes, compilers, Hardware, heterogeneous (hybrid) systems, Indexes, Kernel, Layout, Metadata, sparse linear algebra, specialized application languages, Tensors},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/UA4W8VQ7/10083210.html:text/html},
}

@inproceedings{ahrens_looplets_2023,
	address = {New York, NY, USA},
	series = {{CGO} 2023},
	title = {Looplets: {A} {Language} for {Structured} {Coiteration}},
	isbn = {9798400701016},
	shorttitle = {Looplets},
	doi = {10.1145/3579990.3580020},
	abstract = {Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Specializing for structure yields significant speedups. But automatically generating efficient code for structured data is challenging, especially when arrays with different structure interact. We show how to abstract over array structures so that the compiler can generate code to coiterate over any combination of them. Our technique enables new array formats (such as 1DVBL for irregular clustered sparsity), new iteration strategies (such as galloping intersections), and new operations over structured data (such as concatenation or convolution).},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Donenfeld, Daniel and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = feb,
	year = {2023},
	keywords = {Sparse, Tensor, Array, Coiteration, Compressed},
	pages = {41--54},
	file = {Full Text PDF:/Users/willow/Zotero/storage/HUBGW7N9/Ahrens et al. - 2023 - Looplets A Language for Structured Coiteration.pdf:application/pdf},
}

@article{shaikhha_functional_2022,
	title = {Functional collection programming with semi-ring dictionaries},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3527333},
	doi = {10.1145/3527333},
	abstract = {This paper introduces semi-ring dictionaries, a powerful class of compositional and purely functional collections that subsume other collection types such as sets, multisets, arrays, vectors, and matrices. We developed SDQL, a statically typed language that can express relational algebra with aggregations, linear algebra, and functional collections over data such as relations and matrices using semi-ring dictionaries. Furthermore, thanks to the algebraic structure behind these dictionaries, SDQL unifies a wide range of optimizations commonly used in databases (DB) and linear algebra (LA). As a result, SDQL enables efficient processing of hybrid DB and LA workloads, by putting together optimizations that are otherwise confined to either DB systems or LA frameworks. We show experimentally that a handful of DB and LA workloads can take advantage of the SDQL language and optimizations. SDQL can be competitive with or outperforms a host of systems that are state of the art in their own domain: in-memory DB systems Typer and Tectorwise for (flat, not nested) relational data; SciPy for LA workloads; sparse tensor compiler taco; the Trance nested relational engine; and the in-database machine learning engines LMFAO and Morpheus for hybrid DB/LA workloads over relational data.},
	number = {OOPSLA1},
	urldate = {2023-07-10},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Shaikhha, Amir and Huot, Mathieu and Smith, Jaclyn and Olteanu, Dan},
	month = apr,
	year = {2022},
	keywords = {Nested Relational Algebra, Semi-Ring Dictionary, Sparse Linear Algebra},
	pages = {89:1--89:33},
	file = {Full Text PDF:/Users/willow/Zotero/storage/Z2IY8UTN/Shaikhha et al. - 2022 - Functional collection programming with semi-ring d.pdf:application/pdf},
}

@article{psarras_linear_2022,
	title = {The {Linear} {Algebra} {Mapping} {Problem}. {Current} state of linear algebra languages and libraries},
	volume = {48},
	issn = {0098-3500, 1557-7295},
	url = {http://arxiv.org/abs/1911.09421},
	doi = {10.1145/3549935},
	abstract = {We observe a disconnect between the developers and the end users of linear algebra libraries. On the one hand, the numerical linear algebra and the high-performance communities invest significant effort in the development and optimization of highly sophisticated numerical kernels and libraries, aiming at the maximum exploitation of both the properties of the input matrices, and the architectural features of the target computing platform. On the other hand, end users are progressively less likely to go through the error-prone and time consuming process of directly using said libraries by writing their code in C or Fortran; instead, languages and libraries such as Matlab, Julia, Eigen and Armadillo, which offer a higher level of abstraction, are becoming more and more popular. Users are given the opportunity to code matrix computations with a syntax that closely resembles the mathematical description; it is then a compiler or an interpreter that internally maps the input program to lower level kernels, as provided by libraries such as BLAS and LAPACK. Unfortunately, our experience suggests that in terms of performance, this translation is typically vastly suboptimal. In this paper, we first introduce the Linear Algebra Mapping Problem, and then investigate how effectively a benchmark of test problems is solved by popular high-level programming languages. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both standard compiler optimizations such as common subexpression elimination and loop-invariant code motion, as well as linear algebra specific optimizations such as optimal parenthesization of a matrix product and kernel selection for matrices with properties. The aim of this study is to give concrete guidelines for the development of languages and libraries that support linear algebra computations.},
	number = {3},
	urldate = {2023-10-03},
	journal = {ACM Transactions on Mathematical Software},
	author = {Psarras, Christos and Barthels, Henrik and Bientinesi, Paolo},
	month = sep,
	year = {2022},
	note = {arXiv:1911.09421 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Mathematical Software},
	pages = {1--30},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/6APM5QY7/Psarras et al. - 2022 - The Linear Algebra Mapping Problem. Current state .pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/6JF2TJK5/1911.html:text/html},
}

@inproceedings{lo_roofline_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	isbn = {978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	doi = {10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	keywords = {CUDA unified memory, Memory bandwidth, Roofline},
	pages = {129--148},
	file = {Submitted Version:/Users/willow/Zotero/storage/X3YWPR6T/Lo et al. - 2015 - Roofline Model Toolkit A Practical Tool for Archi.pdf:application/pdf},
}

@inproceedings{fegade_cora_2022,
	title = {The {CoRa} {Tensor} {Compiler}: {Compilation} for {Ragged} {Tensors} with {Minimal} {Padding}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems}},
	author = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip and Mowry, Todd},
	editor = {Marculescu, D. and Chi, Y. and Wu, C.},
	year = {2022},
	pages = {721--747},
	file = {Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:/Users/willow/Zotero/storage/8PT7K7GL/Fegade et al. - The CoRa Tensor Compiler Compilation for Ragged T.pdf:application/pdf},
}

@article{mcmichen_representing_nodate,
	title = {Representing {Data} {Collections} in an {SSA} {Form}},
	abstract = {Compiler research and development has treated computation as the primary driver of performance improvements in C/C++ programs, leaving memory optimizations as a secondary consideration. Developers are currently handed the arduous task of describing both the semantics and layout of their data in memory, either manually or via libraries, prematurely lowering high-level data collections to a low-level view of memory for the compiler. Thus, the compiler can only glean conservative information about the memory in a program, e.g., alias analysis, and is further hampered by heavy memory optimizations. This paper proposes the Memory Object Intermediate Representation (MEMOIR), a language-agnostic SSA form for sequential and associative data collections, objects, and the fields contained therein. At the core of MEMOIR is a decoupling of the memory used to store data from that used to logically organize data. Through its SSA form, MEMOIR compilers can perform element-level analysis on data collections, enabling static analysis on the state of a collection or object at any given program point. To illustrate the power of this analysis, we perform dead element elimination, resulting in a 26.6\% speedup on mcf from SPECINT 2017. With the degree of freedom to mutate memory layout, our MEMOIR compiler performs field elision and dead field elimination, reducing peak memory usage of mcf by 20.8\%.},
	language = {en},
	author = {McMichen, Tommy and Greiner, Nathan and Zhong, Peter and Sossai, Federico and Patel, Atmn and Campanoni, Simone},
	file = {McMichen et al. - Representing Data Collections in an SSA Form.pdf:/Users/willow/Zotero/storage/GP9YC4IB/McMichen et al. - Representing Data Collections in an SSA Form.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018,
	address = {Eugene OR USA},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer’s push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebrabased framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-ofthe-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {ACM},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	pages = {1--11},
	file = {Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:/Users/willow/Zotero/storage/ADD2JSKL/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{yang_implementing_2018-1,
	address = {New York, NY, USA},
	series = {{ICPP} '18},
	title = {Implementing {Push}-{Pull} {Efficiently} in {GraphBLAS}},
	isbn = {978-1-4503-6510-9},
	url = {https://dl.acm.org/doi/10.1145/3225058.3225122},
	doi = {10.1145/3225058.3225122},
	abstract = {We factor Beamer's push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebra-based framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-of-the-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Carl and Buluç, Aydın and Owens, John D.},
	month = aug,
	year = {2018},
	keywords = {graph algorithms, sparse matrix multiplication, breadth-first search},
	pages = {1--11},
	file = {Full Text PDF:/Users/willow/Zotero/storage/NGHWT98P/Yang et al. - 2018 - Implementing Push-Pull Efficiently in GraphBLAS.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/Users/willow/Zotero/storage/6GTQB2I8/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{moler_history_2020,
	title = {A history of {MATLAB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3386331},
	doi = {10.1145/3386331},
	abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
	number = {HOPL},
	urldate = {2024-03-18},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Moler, Cleve and Little, Jack},
	month = jun,
	year = {2020},
	keywords = {linear algebra, MATLAB, matrix computation},
	pages = {81:1--81:67},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2HANKRI8/Moler and Little - 2020 - A history of MATLAB.pdf:application/pdf},
}

@book{abelson_structure_1996,
	title = {Structure and {Interpretation} of {Computer} {Programs}},
	isbn = {978-0-262-51087-5 978-0-262-31091-8},
	url = {https://library.oapen.org/handle/20.500.12657/26092},
	abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
	language = {English},
	urldate = {2024-03-18},
	publisher = {The MIT Press},
	author = {Abelson, Harold and Sussman, Gerald Jay},
	month = jul,
	year = {1996},
	note = {Accepted: 2019-01-17 23:55},
	keywords = {bic Book Industry Communication::U Computing \& information technology::UY Computer science},
	file = {Full Text PDF:/Users/willow/Zotero/storage/KZEF98BF/Abelson and Sussman - 1996 - Structure and Interpretation of Computer Programs.pdf:application/pdf},
}

@book{knuth_art_1997,
	title = {The {Art} of {Computer} {Programming}: {Fundamental} {Algorithms}, {Volume} 1},
	isbn = {978-0-321-63574-7},
	shorttitle = {The {Art} of {Computer} {Programming}},
	abstract = {\&\&gt;The bible of all fundamental algorithms and the work that taught many of today's software developers most of what they know about computer programming.  —Byte, September 1995   I can't begin to tell you how many pleasurable hours of study and recreation they have afforded me! I have pored over them in cars, restaurants, at work, at home... and even at a Little League game when my son wasn't in the line-up. —Charles Long   If you think you're a really good programmer... read [Knuth's] Art of Computer Programming... You should definitely send me a resume if you can read the whole thing. —Bill Gates   It's always a pleasure when a problem is hard enough that you have to get the Knuths off the shelf. I find that merely opening one has a very useful terrorizing effect on computers. —Jonathan Laventhol   This first volume in the series begins with basic programming concepts and techniques, then focuses more particularly on information structures—the representation of information inside a computer, the structural relationships between data elements and how to deal with them efficiently. Elementary applications are given to simulation, numerical methods, symbolic computing, software and system design. Dozens of simple and important algorithms and techniques have been added to those of the previous edition. The section on mathematical preliminaries has been extensively revised to match present trends in research.  Ebook (PDF version) produced by Mathematical Sciences Publishers (MSP),http://msp.org},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Knuth, Donald E.},
	month = jul,
	year = {1997},
	note = {Google-Books-ID: x9AsAwAAQBAJ},
	keywords = {Computers / Programming / General},
}

@inproceedings{dias_sparselnr_2022,
	address = {Virtual Event},
	title = {{SparseLNR}: accelerating sparse tensor computations using loop nest restructuring},
	isbn = {978-1-4503-9281-5},
	shorttitle = {{SparseLNR}},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532386},
	doi = {10.1145/3524059.3532386},
	abstract = {Sparse tensor algebra computations have become important in many real-world applications like machine learning, scientific simulations, and data mining. Hence, automated code generation and performance optimizations for tensor algebra kernels are paramount. Recent advancements such as the Tensor Algebra Compiler (TACO) greatly generalize and automate the code generation for tensor algebra expressions. However, the code generated by TACO for many important tensor computations remains suboptimal due to the absence of a scheduling directive to support transformations such as distribution/fusion.},
	language = {en},
	urldate = {2024-03-20},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Dias, Adhitha and Sundararajah, Kirshanthan and Saumya, Charitha and Kulkarni, Milind},
	month = jun,
	year = {2022},
	pages = {1--14},
	file = {Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:/Users/willow/Zotero/storage/ESTZ5XHN/Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:application/pdf},
}

@article{fisher_hypermedia_1996,
	title = {Hypermedia image processing reference},
	journal = {England: John Wiley \& Sons Ltd},
	author = {Fisher, Robert and Perkins, Simon and Walker, Ashley and Wolfart, Erik},
	year = {1996},
	pages = {118--130},
	file = {Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:/Users/willow/Zotero/storage/HT6FNWC4/Perkins and Wolfart - 1996 - JOHN WILEY & SONS LTD Chichester . New York . Bris.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	number = {6266},
	urldate = {2024-04-01},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	month = dec,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {Omniglot},
	pages = {1332--1338},
}

@book{gonzalez_digital_2006,
	address = {USA},
	title = {Digital {Image} {Processing} (3rd {Edition})},
	isbn = {978-0-13-168728-8},
	publisher = {Prentice-Hall, Inc.},
	author = {Gonzalez, Rafael C. and Woods, Richard E.},
	month = jan,
	year = {2006},
}

@article{liu_intermediate_2023-1,
	title = {An {Intermediate} {Language} for {General} {Sparse} {Format} {Customization}},
	volume = {22},
	issn = {1556-6064},
	url = {https://ieeexplore.ieee.org/document/10083210},
	doi = {10.1109/LCA.2023.3262610},
	abstract = {The inevitable trend of hardware specialization drives an increasing use of custom data formats in processing sparse workloads, which are typically memory-bound. These formats facilitate the automated generation of target-aware data layouts to improve memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Moreover, since these frameworks adopt an attribute-based approach for format abstraction, they cannot easily be extended to support general format customization. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with hybrid formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, and a simulated processing-in-memory (PIM) device.},
	number = {2},
	urldate = {2024-04-02},
	journal = {IEEE Computer Architecture Letters},
	author = {Liu, Jie and Zhao, Zhongyuan and Ding, Zijian and Brock, Benjamin and Rong, Hongbo and Zhang, Zhiru},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Codes, Compilers, Hardware, heterogeneous (hybrid) systems, Indexes, Kernel, Layout, Metadata, sparse linear algebra, specialized application languages, Tensors},
	pages = {153--156},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/S3LTD86J/10083210.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/AX7BMSJS/Liu et al. - 2023 - An Intermediate Language for General Sparse Format.pdf:application/pdf},
}

@misc{sundram_compiling_2023,
	title = {Compiling {Recurrences} over {Dense} and {Sparse} {Arrays}},
	url = {http://arxiv.org/abs/2309.04660},
	doi = {10.48550/arXiv.2309.04660},
	abstract = {Recurrence equations lie at the heart of many computational paradigms including dynamic programming, graph analysis, and linear solvers. These equations are often expensive to compute and much work has gone into optimizing them for different situations. The set of recurrence implementations is a large design space across the set of all recurrences (e.g., the Viterbi and Floyd-Warshall algorithms), the choice of data structures (e.g., dense and sparse matrices), and the set of different loop orders. Optimized library implementations do not exist for most points in this design space, and developers must therefore often manually implement and optimize recurrences. We present a general framework for compiling recurrence equations into native code corresponding to any valid point in this general design space. In this framework, users specify a system of recurrences, the type of data structures for storing the input and outputs, and a set of scheduling primitives for optimization. A greedy algorithm then takes this specification and lowers it into a native program that respects the dependencies inherent to the recurrence equation. We describe the compiler transformations necessary to lower this high-level specification into native parallel code for either sparse and dense data structures and provide an algorithm for determining whether the recurrence system is solvable with the provided scheduling primitives. We evaluate the performance and correctness of the generated code on various computational tasks from domains including dense and sparse matrix solvers, dynamic programming, graph problems, and sparse tensor algebra. We demonstrate that generated code has competitive performance to handwritten implementations in libraries.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Sundram, Shiv and Tariq, Muhammad Usman and Kjolstad, Fredrik},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04660 [cs]},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/SWQTJTR9/Sundram et al. - 2023 - Compiling Recurrences over Dense and Sparse Arrays.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/F6T9JKCX/2309.html:text/html},
}

@article{noauthor_developer_2024,
	title = {Developer {Reference} for {Intel}® {oneAPI} {Math} {Kernel} {Library} for {Fortran}},
	url = {https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-fortran/2024-0/overview.html},
	abstract = {The Intel® oneAPI Math Kernel Library (oneMKL) improves performance with math routines for software applications that solve large computational problems. oneMKL provides BLAS and LAPACK linear algebra routines, fast Fourier transforms, vectorized math functions, random number generation functions, and other functionality.},
	language = {en},
	month = apr,
	year = {2024},
	file = {Developer Reference for Intel® oneAPI Math Kernel .pdf:/Users/willow/Zotero/storage/R6N2BIWN/Developer Reference for Intel® oneAPI Math Kernel .pdf:application/pdf},
}

@article{dean_mapreduce_2008,
	title = {{MapReduce}: simplified data processing on large clusters},
	volume = {51},
	issn = {0001-0782},
	shorttitle = {{MapReduce}},
	url = {https://dl.acm.org/doi/10.1145/1327452.1327492},
	doi = {10.1145/1327452.1327492},
	abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
	number = {1},
	urldate = {2024-04-03},
	journal = {Communications of the ACM},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	month = jan,
	year = {2008},
	pages = {107--113},
	file = {Full Text PDF:/Users/willow/Zotero/storage/BFY2JL33/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:application/pdf},
}

@article{liu_verified_2022,
	title = {Verified tensor-program optimization via high-level scheduling rewrites},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3498717},
	doi = {10.1145/3498717},
	abstract = {We present a lightweight Coq framework for optimizing tensor kernels written in a pure, functional array language. Optimizations rely on user scheduling using series of verified, semantics-preserving rewrites. Unusually for compilation targeting imperative code with arrays and nested loops, all rewrites are source-to-source within a purely functional language. Our language comprises a set of core constructs for expressing high-level computation detail and a set of what we call reshape operators, which can be derived from core constructs but trigger low-level decisions about storage patterns and ordering. We demonstrate that not only is this system capable of deriving the optimizations of existing state-of-the-art languages like Halide and generating comparably performant code, it is also able to schedule a family of useful program transformations beyond what is reachable in Halide.},
	number = {POPL},
	urldate = {2024-04-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Liu, Amanda and Bernstein, Gilbert Louis and Chlipala, Adam and Ragan-Kelley, Jonathan},
	month = jan,
	year = {2022},
	keywords = {array programming, formal verification, optimization, proof assistants},
	pages = {55:1--55:28},
	file = {Full Text PDF:/Users/willow/Zotero/storage/AKFB743Y/Liu et al. - 2022 - Verified tensor-program optimization via high-leve.pdf:application/pdf},
}

@inproceedings{dias_sparselnr_2022-1,
	address = {New York, NY, USA},
	series = {{ICS} '22},
	title = {{SparseLNR}: accelerating sparse tensor computations using loop nest restructuring},
	isbn = {978-1-4503-9281-5},
	shorttitle = {{SparseLNR}},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532386},
	doi = {10.1145/3524059.3532386},
	abstract = {Sparse tensor algebra computations have become important in many real-world applications like machine learning, scientific simulations, and data mining. Hence, automated code generation and performance optimizations for tensor algebra kernels are paramount. Recent advancements such as the Tensor Algebra Compiler (TACO) greatly generalize and automate the code generation for tensor algebra expressions. However, the code generated by TACO for many important tensor computations remains suboptimal due to the absence of a scheduling directive to support transformations such as distribution/fusion. This paper extends TACO's scheduling space to support kernel distribution/loop fusion in order to reduce asymptotic time complexity and improve locality of complex tensor algebra computations. We develop an intermediate representation (IR) for tensor operations called branched iteration graph which specifies breakdown of the computation into smaller ones (kernel distribution) and then fuse (loop fusion) outermost dimensions of the loop nests, while the inner-most dimensions are distributed, to increase data locality. We describe exchanges of intermediate results between space iteration spaces, transformation in the IR, and its programmatic invocation. Finally, we show that the transformation can be used to optimize sparse tensor kernels. Our results show that this new transformation significantly improves the performance of several real-world tensor algebra computations compared to TACO-generated code.},
	urldate = {2024-04-03},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Dias, Adhitha and Sundararajah, Kirshanthan and Saumya, Charitha and Kulkarni, Milind},
	month = jun,
	year = {2022},
	keywords = {kernel distribution, loop fusion, loop transformations, sparse tensor algebra},
	pages = {1--14},
	file = {Full Text PDF:/Users/willow/Zotero/storage/KEAJLXF5/Dias et al. - 2022 - SparseLNR accelerating sparse tensor computations.pdf:application/pdf},
}

@misc{nayak_teaal_2023,
	title = {{TeAAL}: {A} {Declarative} {Framework} for {Modeling} {Sparse} {Tensor} {Accelerators}},
	shorttitle = {{TeAAL}},
	url = {http://arxiv.org/abs/2304.07931},
	abstract = {Over the past few years, the explosion in sparse tensor algebra workloads has led to a corresponding rise in domain-specific accelerators to service them. Due to the irregularity present in sparse tensors, these accelerators employ a wide variety of novel solutions to achieve good performance. At the same time, prior work on design-flexible sparse accelerator modeling does not express this full range of design features, making it difficult to understand the impact of each design choice and compare or extend the state-ofthe-art. To address this, we propose TeAAL: a language and simulator generator for the concise and precise specification and evaluation of sparse tensor algebra accelerators. We use TeAAL to represent and evaluate four disparate state-of-the-art accelerators—ExTensor, Gamma, OuterSPACE, and SIGMA—and verify that it reproduces their performance with high accuracy. Finally, we demonstrate the potential of TeAAL as a tool for designing new accelerators by showing how it can be used to speed up vertex-centric programming accelerators—achieving 1.9× on BFS and 1.2× on SSSP over GraphDynS.},
	language = {en},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Nayak, Nandeeka and Odemuyiwa, Toluwanimi O. and Ugare, Shubham and Fletcher, Christopher W. and Pellauer, Michael and Emer, Joel S.},
	month = oct,
	year = {2023},
	note = {arXiv:2304.07931 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	file = {Nayak et al. - 2023 - TeAAL A Declarative Framework for Modeling Sparse.pdf:/Users/willow/Zotero/storage/X333VC2H/Nayak et al. - 2023 - TeAAL A Declarative Framework for Modeling Sparse.pdf:application/pdf},
}

@inproceedings{yadav_spdistal_2022-1,
	address = {Dallas, Texas},
	series = {{SC} '22},
	title = {{SpDISTAL}: compiling distributed sparse tensor computations},
	shorttitle = {{SpDISTAL}},
	abstract = {We introduce SpDISTAL, a compiler for sparse tensor algebra that targets distributed systems. SpDISTAL combines separate descriptions of tensor algebra expressions, sparse data structures, data distribution, and computation distribution. Thus, it enables distributed execution of sparse tensor algebra expressions with a wide variety of sparse data structures and data distributions. SpDISTAL is implemented as a C++ library that targets a distributed task-based runtime system and can generate code for nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates distributed code that achieves performance competitive with hand-written distributed functions for specific sparse tensor algebra expressions and that outperforms general interpretation-based systems by one to two orders of magnitude.},
	urldate = {2024-04-03},
	booktitle = {Proceedings of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Press},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = nov,
	year = {2022},
	keywords = {computer science, parallel programming, programming},
	pages = {1--15},
	file = {Full Text PDF:/Users/willow/Zotero/storage/LAU7SENV/Yadav et al. - 2022 - SpDISTAL compiling distributed sparse tensor comp.pdf:application/pdf},
}
